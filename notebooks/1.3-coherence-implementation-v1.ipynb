{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dc7697a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"# Run if working locally\\n%load_ext autoreload\\n%autoreload 2\\n%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"# Run if working locally\\n%load_ext autoreload\\n%autoreload 2\\n%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run if working locally\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88ee84b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"import sqlite3\\nfrom sqlite3 import Error\\nimport pickle\\nimport os, sys\\nimport config\\n\\nconfig.root_path = os.path.abspath(os.path.join(os.getcwd(), \\\"..\\\"))\\nsys.path.insert(0, config.root_path)\\n\\nfrom src.dataset.dataset import RawData\\nfrom src.dataset.wikisection_preprocessing import (\\n    tokenize,\\n    clean_sentence,\\n    preprocess_text_segmentation,\\n    format_data_for_db_insertion,\\n)\\nfrom src.dataset.utils import truncate_by_token\\nfrom db.dbv2 import Table, AugmentedTable, TrainTestTable\\nimport pprint\\n\\nfrom src.bertkeywords.src.similarities import Embedding, Similarities\\nfrom src.bertkeywords.src.keywords import Keywords\\nfrom src.encoders.coherence_v2 import Coherence\\nfrom src.dataset.utils import flatten, dedupe_list, truncate_string\";\n",
       "                var nbb_formatted_code = \"import sqlite3\\nfrom sqlite3 import Error\\nimport pickle\\nimport os, sys\\nimport config\\n\\nconfig.root_path = os.path.abspath(os.path.join(os.getcwd(), \\\"..\\\"))\\nsys.path.insert(0, config.root_path)\\n\\nfrom src.dataset.dataset import RawData\\nfrom src.dataset.wikisection_preprocessing import (\\n    tokenize,\\n    clean_sentence,\\n    preprocess_text_segmentation,\\n    format_data_for_db_insertion,\\n)\\nfrom src.dataset.utils import truncate_by_token\\nfrom db.dbv2 import Table, AugmentedTable, TrainTestTable\\nimport pprint\\n\\nfrom src.bertkeywords.src.similarities import Embedding, Similarities\\nfrom src.bertkeywords.src.keywords import Keywords\\nfrom src.encoders.coherence_v2 import Coherence\\nfrom src.dataset.utils import flatten, dedupe_list, truncate_string\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sqlite3\n",
    "from sqlite3 import Error\n",
    "import pickle\n",
    "import os, sys\n",
    "import config\n",
    "\n",
    "config.root_path = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.insert(0, config.root_path)\n",
    "\n",
    "from src.dataset.dataset import RawData\n",
    "from src.dataset.wikisection_preprocessing import (\n",
    "    tokenize,\n",
    "    clean_sentence,\n",
    "    preprocess_text_segmentation,\n",
    "    format_data_for_db_insertion,\n",
    ")\n",
    "from src.dataset.utils import truncate_by_token\n",
    "from db.dbv2 import Table, AugmentedTable, TrainTestTable\n",
    "import pprint\n",
    "\n",
    "from src.bertkeywords.src.similarities import Embedding, Similarities\n",
    "from src.bertkeywords.src.keywords import Keywords\n",
    "from src.encoders.coherence_v2 import Coherence\n",
    "from src.dataset.utils import flatten, dedupe_list, truncate_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e4dcd8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "No sentence-transformers model found with name /Users/amitmaraj/.cache/torch/sentence_transformers/bert-base-uncased. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /Users/amitmaraj/.cache/torch/sentence_transformers/bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"# initialize the coherence library\\ncoherence = Coherence(max_words_per_step=4)\";\n",
       "                var nbb_formatted_code = \"# initialize the coherence library\\ncoherence = Coherence(max_words_per_step=4)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialize the coherence library\n",
    "coherence = Coherence(max_words_per_step=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "226021b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "No sentence-transformers model found with name /Users/amitmaraj/.cache/torch/sentence_transformers/bert-base-uncased. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /Users/amitmaraj/.cache/torch/sentence_transformers/bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"# initialize the keywords and embeddings library\\npp = pprint.PrettyPrinter(indent=4)\\nsimilarities_lib = Similarities(\\\"bert-base-uncased\\\")\\nkeywords_lib = Keywords(similarities_lib.model, similarities_lib.tokenizer)\\nembedding_lib = Embedding(similarities_lib.model, similarities_lib.tokenizer)\";\n",
       "                var nbb_formatted_code = \"# initialize the keywords and embeddings library\\npp = pprint.PrettyPrinter(indent=4)\\nsimilarities_lib = Similarities(\\\"bert-base-uncased\\\")\\nkeywords_lib = Keywords(similarities_lib.model, similarities_lib.tokenizer)\\nembedding_lib = Embedding(similarities_lib.model, similarities_lib.tokenizer)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialize the keywords and embeddings library\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "similarities_lib = Similarities(\"bert-base-uncased\")\n",
    "keywords_lib = Keywords(similarities_lib.model, similarities_lib.tokenizer)\n",
    "embedding_lib = Embedding(similarities_lib.model, similarities_lib.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bb2458b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wikisection_city\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"dataset_type = \\\"city\\\"\\ntable = Table(dataset_type)\\naugmented_table = AugmentedTable(dataset_type)\\ntrain_test_table = TrainTestTable(dataset_type)\";\n",
       "                var nbb_formatted_code = \"dataset_type = \\\"city\\\"\\ntable = Table(dataset_type)\\naugmented_table = AugmentedTable(dataset_type)\\ntrain_test_table = TrainTestTable(dataset_type)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_type = \"city\"\n",
    "table = Table(dataset_type)\n",
    "augmented_table = AugmentedTable(dataset_type)\n",
    "train_test_table = TrainTestTable(dataset_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfd59c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"data = table.get_all()\\n\\ntext_data = [x[1] for x in data]\\ntext_labels = [x[2] for x in data]\";\n",
       "                var nbb_formatted_code = \"data = table.get_all()\\n\\ntext_data = [x[1] for x in data]\\ntext_labels = [x[2] for x in data]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = table.get_all()\n",
    "\n",
    "text_data = [x[1] for x in data]\n",
    "text_labels = [x[2] for x in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcccad5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"all_segments = table.get_all_segments()\\ntext_segments = [[y[1] for y in x] for x in all_segments]\\nsegments_labels = [[1 if i == 0 else 0 for i, y in enumerate(x)] for x in all_segments]\";\n",
       "                var nbb_formatted_code = \"all_segments = table.get_all_segments()\\ntext_segments = [[y[1] for y in x] for x in all_segments]\\nsegments_labels = [[1 if i == 0 else 0 for i, y in enumerate(x)] for x in all_segments]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_segments = table.get_all_segments()\n",
    "text_segments = [[y[1] for y in x] for x in all_segments]\n",
    "segments_labels = [[1 if i == 0 else 0 for i, y in enumerate(x)] for x in all_segments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "295657fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"samples = 5\\nmax_tokens = 400\\n\\nfor i, (segment, labels) in enumerate(\\n    zip(text_segments[:samples], segments_labels[:samples])\\n):\\n    for sentence, label in zip(segment, labels):\\n        # this is the training case. During inference, we will have no idea\\n        # when segments start and when they end.\\n        pass\";\n",
       "                var nbb_formatted_code = \"samples = 5\\nmax_tokens = 400\\n\\nfor i, (segment, labels) in enumerate(\\n    zip(text_segments[:samples], segments_labels[:samples])\\n):\\n    for sentence, label in zip(segment, labels):\\n        # this is the training case. During inference, we will have no idea\\n        # when segments start and when they end.\\n        pass\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "samples = 5\n",
    "max_tokens = 400\n",
    "\n",
    "for i, (segment, labels) in enumerate(\n",
    "    zip(text_segments[:samples], segments_labels[:samples])\n",
    "):\n",
    "    for sentence, label in zip(segment, labels):\n",
    "        # this is the training case. During inference, we will have no idea\n",
    "        # when segments start and when they end.\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "467789d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"text_labels[:25]\";\n",
       "                var nbb_formatted_code = \"text_labels[:25]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_labels[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6870992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 19;\n",
       "                var nbb_unformatted_code = \"pruning = 4  # remove the lowest n important words from coherence map\\npruning_min = 10  # only prune after n words in the coherence map\\n\\n\\ndef coherence_tester(\\n    text_data, text_labels, max_tokens=400, max_str_length=30, prediction_thresh=0.3\\n):\\n    coherence_map = []\\n    predictions = []\\n    for i, (row, label) in enumerate(zip(text_data, text_labels)):\\n        # compare the current sentence to the previous one\\n        if i == 0:\\n            pass\\n        else:\\n            prev_row = text_data[i - 1]\\n\\n            row = truncate_by_token(row, max_tokens)\\n            prev_row = truncate_by_token(prev_row, max_tokens)\\n\\n            # add the keywords to the coherence map\\n            coherence_map.extend(\\n                coherence.get_coherence([row, prev_row])\\n            )\\n            print(f\\\"Coherence Map: {[x[0] for x in coherence_map]}\\\")\\n            if pruning > 0 and len(coherence_map) >= pruning_min:\\n                print(\\\"pruning...\\\", len(coherence_map))\\n                sorted_map = sorted(\\n                    coherence_map, key=lambda tup: tup[1]\\n                )  # sort asc by importance based on keybert\\n                coherence_map = sorted_map[pruning:][\\n                    ::-1\\n                ]  # get the last n - pruning values and reverse the list\\n                print(\\\"done pruning...\\\", len(coherence_map))\\n\\n            # truncate the strings for printing\\n            truncated_row = truncate_string(row, max_str_length)\\n            truncated_prev_row = truncate_string(prev_row, max_str_length)\\n\\n            # get the keywords for the current sentences\\n            keywords_current = keywords_lib.get_keywords_with_embeddings(row)\\n            keywords_prev = keywords_lib.get_keywords_with_embeddings(prev_row)\\n\\n            # compute the word comparisons between the previous (with the coherence map)\\n            # and the current (possibly the first sentence in a new segment)\\n            word_comparisons_with_coherence = embedding_lib.compare_keyword_tuples(\\n                [*coherence_map, *keywords_prev], keywords_current\\n            )\\n            word_comparisons = embedding_lib.compare_keyword_tuples(\\n                keywords_prev, keywords_current\\n            )\\n\\n            similarities_with_coherence = [\\n                comparison[2] for comparison in word_comparisons_with_coherence\\n            ]\\n            avg_similarity_with_coherence = sum(similarities_with_coherence) / len(\\n                similarities_with_coherence\\n            )\\n            similarities = [comparison[2] for comparison in word_comparisons]\\n            avg_similarity = sum(similarities) / len(similarities)\\n\\n            # if the two sentences are similar, create a cohesive prediction\\n            # otherwise, predict a new segment\\n            if avg_similarity_with_coherence[0] > prediction_thresh:\\n                print(f\\\"Label: {label}, Prediction: {0}\\\")\\n                predictions.append((avg_similarity_with_coherence[0], 0))\\n            else:\\n                # start of a new segment, empty the map\\n                coherence_map = []\\n                print(f\\\"Label: {label}, Prediction: {1}\\\")\\n                predictions.append((avg_similarity_with_coherence[0], 1))\\n\\n    #             print(\\n    #                 f\\\"{[x[0] for x in coherence_map]} <> {[x[0] for x in keywords_prev]} <> {[x[0] for x in keywords_current]}\\\"\\n    #             )\\n    #             print(f\\\"{label}, {avg_similarity}, {truncated_prev_row} <> {truncated_row}\\\")\\n    #             print(\\n    #                 f\\\"{label}, {avg_similarity_with_coherence}, {truncated_prev_row} <> {truncated_row}\\\"\\n    #             )\\n\\n    return predictions\";\n",
       "                var nbb_formatted_code = \"pruning = 4  # remove the lowest n important words from coherence map\\npruning_min = 10  # only prune after n words in the coherence map\\n\\n\\ndef coherence_tester(\\n    text_data, text_labels, max_tokens=400, max_str_length=30, prediction_thresh=0.3\\n):\\n    coherence_map = []\\n    predictions = []\\n    for i, (row, label) in enumerate(zip(text_data, text_labels)):\\n        # compare the current sentence to the previous one\\n        if i == 0:\\n            pass\\n        else:\\n            prev_row = text_data[i - 1]\\n\\n            row = truncate_by_token(row, max_tokens)\\n            prev_row = truncate_by_token(prev_row, max_tokens)\\n\\n            # add the keywords to the coherence map\\n            coherence_map.extend(coherence.get_coherence([row, prev_row]))\\n            print(f\\\"Coherence Map: {[x[0] for x in coherence_map]}\\\")\\n            if pruning > 0 and len(coherence_map) >= pruning_min:\\n                print(\\\"pruning...\\\", len(coherence_map))\\n                sorted_map = sorted(\\n                    coherence_map, key=lambda tup: tup[1]\\n                )  # sort asc by importance based on keybert\\n                coherence_map = sorted_map[pruning:][\\n                    ::-1\\n                ]  # get the last n - pruning values and reverse the list\\n                print(\\\"done pruning...\\\", len(coherence_map))\\n\\n            # truncate the strings for printing\\n            truncated_row = truncate_string(row, max_str_length)\\n            truncated_prev_row = truncate_string(prev_row, max_str_length)\\n\\n            # get the keywords for the current sentences\\n            keywords_current = keywords_lib.get_keywords_with_embeddings(row)\\n            keywords_prev = keywords_lib.get_keywords_with_embeddings(prev_row)\\n\\n            # compute the word comparisons between the previous (with the coherence map)\\n            # and the current (possibly the first sentence in a new segment)\\n            word_comparisons_with_coherence = embedding_lib.compare_keyword_tuples(\\n                [*coherence_map, *keywords_prev], keywords_current\\n            )\\n            word_comparisons = embedding_lib.compare_keyword_tuples(\\n                keywords_prev, keywords_current\\n            )\\n\\n            similarities_with_coherence = [\\n                comparison[2] for comparison in word_comparisons_with_coherence\\n            ]\\n            avg_similarity_with_coherence = sum(similarities_with_coherence) / len(\\n                similarities_with_coherence\\n            )\\n            similarities = [comparison[2] for comparison in word_comparisons]\\n            avg_similarity = sum(similarities) / len(similarities)\\n\\n            # if the two sentences are similar, create a cohesive prediction\\n            # otherwise, predict a new segment\\n            if avg_similarity_with_coherence[0] > prediction_thresh:\\n                print(f\\\"Label: {label}, Prediction: {0}\\\")\\n                predictions.append((avg_similarity_with_coherence[0], 0))\\n            else:\\n                # start of a new segment, empty the map\\n                coherence_map = []\\n                print(f\\\"Label: {label}, Prediction: {1}\\\")\\n                predictions.append((avg_similarity_with_coherence[0], 1))\\n\\n    #             print(\\n    #                 f\\\"{[x[0] for x in coherence_map]} <> {[x[0] for x in keywords_prev]} <> {[x[0] for x in keywords_current]}\\\"\\n    #             )\\n    #             print(f\\\"{label}, {avg_similarity}, {truncated_prev_row} <> {truncated_row}\\\")\\n    #             print(\\n    #                 f\\\"{label}, {avg_similarity_with_coherence}, {truncated_prev_row} <> {truncated_row}\\\"\\n    #             )\\n\\n    return predictions\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pruning = 4  # remove the lowest n important words from coherence map\n",
    "pruning_min = 10  # only prune after n words in the coherence map\n",
    "\n",
    "\n",
    "def coherence_tester(\n",
    "    text_data, text_labels, max_tokens=400, max_str_length=30, prediction_thresh=0.3\n",
    "):\n",
    "    coherence_map = []\n",
    "    predictions = []\n",
    "    for i, (row, label) in enumerate(zip(text_data, text_labels)):\n",
    "        # compare the current sentence to the previous one\n",
    "        if i == 0:\n",
    "            pass\n",
    "        else:\n",
    "            prev_row = text_data[i - 1]\n",
    "\n",
    "            row = truncate_by_token(row, max_tokens)\n",
    "            prev_row = truncate_by_token(prev_row, max_tokens)\n",
    "\n",
    "            # add the keywords to the coherence map\n",
    "            coherence_map.extend(coherence.get_coherence([row, prev_row]))\n",
    "            print(f\"Coherence Map: {[x[0] for x in coherence_map]}\")\n",
    "            if pruning > 0 and len(coherence_map) >= pruning_min:\n",
    "                print(\"pruning...\", len(coherence_map))\n",
    "                sorted_map = sorted(\n",
    "                    coherence_map, key=lambda tup: tup[1]\n",
    "                )  # sort asc by importance based on keybert\n",
    "                coherence_map = sorted_map[pruning:][\n",
    "                    ::-1\n",
    "                ]  # get the last n - pruning values and reverse the list\n",
    "                print(\"done pruning...\", len(coherence_map))\n",
    "\n",
    "            # truncate the strings for printing\n",
    "            truncated_row = truncate_string(row, max_str_length)\n",
    "            truncated_prev_row = truncate_string(prev_row, max_str_length)\n",
    "\n",
    "            # get the keywords for the current sentences\n",
    "            keywords_current = keywords_lib.get_keywords_with_embeddings(row)\n",
    "            keywords_prev = keywords_lib.get_keywords_with_embeddings(prev_row)\n",
    "\n",
    "            # compute the word comparisons between the previous (with the coherence map)\n",
    "            # and the current (possibly the first sentence in a new segment)\n",
    "            word_comparisons_with_coherence = embedding_lib.compare_keyword_tuples(\n",
    "                [*coherence_map, *keywords_prev], keywords_current\n",
    "            )\n",
    "            word_comparisons = embedding_lib.compare_keyword_tuples(\n",
    "                keywords_prev, keywords_current\n",
    "            )\n",
    "\n",
    "            similarities_with_coherence = [\n",
    "                comparison[2] for comparison in word_comparisons_with_coherence\n",
    "            ]\n",
    "            avg_similarity_with_coherence = sum(similarities_with_coherence) / len(\n",
    "                similarities_with_coherence\n",
    "            )\n",
    "            similarities = [comparison[2] for comparison in word_comparisons]\n",
    "            avg_similarity = sum(similarities) / len(similarities)\n",
    "\n",
    "            # if the two sentences are similar, create a cohesive prediction\n",
    "            # otherwise, predict a new segment\n",
    "            if avg_similarity_with_coherence[0] > prediction_thresh:\n",
    "                print(f\"Label: {label}, Prediction: {0}\")\n",
    "                predictions.append((avg_similarity_with_coherence[0], 0))\n",
    "            else:\n",
    "                # start of a new segment, empty the map\n",
    "                coherence_map = []\n",
    "                print(f\"Label: {label}, Prediction: {1}\")\n",
    "                predictions.append((avg_similarity_with_coherence[0], 1))\n",
    "\n",
    "    #             print(\n",
    "    #                 f\"{[x[0] for x in coherence_map]} <> {[x[0] for x in keywords_prev]} <> {[x[0] for x in keywords_current]}\"\n",
    "    #             )\n",
    "    #             print(f\"{label}, {avg_similarity}, {truncated_prev_row} <> {truncated_row}\")\n",
    "    #             print(\n",
    "    #                 f\"{label}, {avg_similarity_with_coherence}, {truncated_prev_row} <> {truncated_row}\"\n",
    "    #             )\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca71b4cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [20], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m max_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m400\u001b[39m  \u001b[38;5;66;03m# want to keep this under 512\u001b[39;00m\n\u001b[1;32m      3\u001b[0m max_str_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m\n\u001b[0;32m----> 5\u001b[0m coherence_tester(\n\u001b[1;32m      6\u001b[0m     text_data[:samples],\n\u001b[1;32m      7\u001b[0m     text_labels[:samples],\n\u001b[1;32m      8\u001b[0m     max_tokens\u001b[38;5;241m=\u001b[39mmax_tokens,\n\u001b[1;32m      9\u001b[0m     max_str_length\u001b[38;5;241m=\u001b[39mmax_str_length,\n\u001b[1;32m     10\u001b[0m )\n",
      "Cell \u001b[0;32mIn [19], line 22\u001b[0m, in \u001b[0;36mcoherence_tester\u001b[0;34m(text_data, text_labels, max_tokens, max_str_length, prediction_thresh)\u001b[0m\n\u001b[1;32m     18\u001b[0m prev_row \u001b[38;5;241m=\u001b[39m truncate_by_token(prev_row, max_tokens)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# add the keywords to the coherence map\u001b[39;00m\n\u001b[1;32m     21\u001b[0m coherence_map\u001b[38;5;241m.\u001b[39mextend(\n\u001b[0;32m---> 22\u001b[0m     \u001b[43mcoherence\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_coherence\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_row\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCoherence Map: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m[x[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m coherence_map]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pruning \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(coherence_map) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m pruning_min:\n",
      "File \u001b[0;32m~/Documents/PhD/HumanMachineLab/Coherence/src/encoders/coherence_v2.py:119\u001b[0m, in \u001b[0;36mCoherence.get_coherence\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m    116\u001b[0m kw_prev_sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev_sentence\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# get all the cohesive words as dictated by the coherence_threshold\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m coherent_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_similar_coherent_words\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkw_prev_sentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkw_curr_sentence\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_words_per_step]\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# add the words to the cohesion map for this run.\u001b[39;00m\n\u001b[1;32m    124\u001b[0m cohesion\u001b[38;5;241m.\u001b[39mappend(coherent_words)\n",
      "File \u001b[0;32m~/Documents/PhD/HumanMachineLab/Coherence/src/encoders/coherence_v2.py:59\u001b[0m, in \u001b[0;36mCoherence.get_similar_coherent_words\u001b[0;34m(self, kw_prev_sentence, kw_curr_sentence)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word2 \u001b[38;5;129;01min\u001b[39;00m kw_curr_sentence:\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m word1 \u001b[38;5;129;01min\u001b[39;00m kw_prev_sentence:\n\u001b[0;32m---> 59\u001b[0m         emb1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(\u001b[43mword1\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     60\u001b[0m         emb2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(word2[\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;66;03m# check to see if either word by its embedding already exists in the\u001b[39;00m\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;66;03m# coherent words so far.\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: string index out of range"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 20;\n",
       "                var nbb_unformatted_code = \"samples = 6\\nmax_tokens = 400  # want to keep this under 512\\nmax_str_length = 30\\n\\ncoherence_tester(\\n    text_data[:samples],\\n    text_labels[:samples],\\n    max_tokens=max_tokens,\\n    max_str_length=max_str_length,\\n)\";\n",
       "                var nbb_formatted_code = \"samples = 6\\nmax_tokens = 400  # want to keep this under 512\\nmax_str_length = 30\\n\\ncoherence_tester(\\n    text_data[:samples],\\n    text_labels[:samples],\\n    max_tokens=max_tokens,\\n    max_str_length=max_str_length,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "samples = 6\n",
    "max_tokens = 400  # want to keep this under 512\n",
    "max_str_length = 30\n",
    "\n",
    "coherence_tester(\n",
    "    text_data[:samples],\n",
    "    text_labels[:samples],\n",
    "    max_tokens=max_tokens,\n",
    "    max_str_length=max_str_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a93727",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a18a8471",
   "metadata": {},
   "source": [
    "## Sample Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a97f25cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 15;\n",
       "                var nbb_unformatted_code = \"segments_to_test = 10\\n\\ntext_segments_to_check = [\\n    [truncate_by_token(s, max_tokens) for s in segment]\\n    for segment in text_segments[:segments_to_test]\\n]\\ntext_labels_to_check = segments_labels[:segments_to_test]\";\n",
       "                var nbb_formatted_code = \"segments_to_test = 10\\n\\ntext_segments_to_check = [\\n    [truncate_by_token(s, max_tokens) for s in segment]\\n    for segment in text_segments[:segments_to_test]\\n]\\ntext_labels_to_check = segments_labels[:segments_to_test]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "segments_to_test = 10\n",
    "\n",
    "text_segments_to_check = [\n",
    "    [truncate_by_token(s, max_tokens) for s in segment]\n",
    "    for segment in text_segments[:segments_to_test]\n",
    "]\n",
    "text_labels_to_check = segments_labels[:segments_to_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "583ba1ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36, 36)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 16;\n",
       "                var nbb_unformatted_code = \"len(text_segments_to_check[0]), len(text_labels_to_check[0])\";\n",
       "                var nbb_formatted_code = \"len(text_segments_to_check[0]), len(text_labels_to_check[0])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(text_segments_to_check[0]), len(text_labels_to_check[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d1c6816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 17;\n",
       "                var nbb_unformatted_code = \"different_segment_1 = [\\n    text_segments_to_check[0][-3],\\n    text_segments_to_check[0][-2],\\n    text_segments_to_check[0][-1],\\n    text_segments_to_check[1][0],\\n    text_segments_to_check[1][1],\\n    text_segments_to_check[1][2],\\n]\\ndifferent_segment_2 = [\\n    text_segments_to_check[1][-3],\\n    text_segments_to_check[1][-2],\\n    text_segments_to_check[1][-1],\\n    text_segments_to_check[2][0],\\n    text_segments_to_check[2][1],\\n    text_segments_to_check[2][2],\\n]\\ndifferent_segment_3 = [\\n    text_segments_to_check[2][-3],\\n    text_segments_to_check[2][-2],\\n    text_segments_to_check[2][-1],\\n    text_segments_to_check[3][0],\\n    text_segments_to_check[3][1],\\n    text_segments_to_check[3][2],\\n]\\ndifferent_segment_4 = [\\n    text_segments_to_check[3][-3],\\n    text_segments_to_check[3][-2],\\n    text_segments_to_check[3][-1],\\n    text_segments_to_check[4][0],\\n    text_segments_to_check[4][1],\\n    text_segments_to_check[4][2],\\n]\\ndifferent_segment_5 = [\\n    text_segments_to_check[4][-3],\\n    text_segments_to_check[4][-2],\\n    text_segments_to_check[4][-1],\\n    text_segments_to_check[5][0],\\n    text_segments_to_check[5][1],\\n    text_segments_to_check[5][2],\\n]\\n\\nsame_segment_1 = [\\n    text_segments_to_check[0][0],\\n    text_segments_to_check[0][1],\\n    text_segments_to_check[0][2],\\n]\\nsame_segment_2 = [\\n    text_segments_to_check[1][0],\\n    text_segments_to_check[1][1],\\n    text_segments_to_check[1][2],\\n]\\nsame_segment_3 = [\\n    text_segments_to_check[2][0],\\n    text_segments_to_check[2][1],\\n    text_segments_to_check[2][2],\\n]\\nsame_segment_4 = [\\n    text_segments_to_check[3][0],\\n    text_segments_to_check[3][1],\\n    text_segments_to_check[3][2],\\n]\\nsame_segment_5 = [\\n    text_segments_to_check[4][0],\\n    text_segments_to_check[4][1],\\n    text_segments_to_check[4][2],\\n]\";\n",
       "                var nbb_formatted_code = \"different_segment_1 = [\\n    text_segments_to_check[0][-3],\\n    text_segments_to_check[0][-2],\\n    text_segments_to_check[0][-1],\\n    text_segments_to_check[1][0],\\n    text_segments_to_check[1][1],\\n    text_segments_to_check[1][2],\\n]\\ndifferent_segment_2 = [\\n    text_segments_to_check[1][-3],\\n    text_segments_to_check[1][-2],\\n    text_segments_to_check[1][-1],\\n    text_segments_to_check[2][0],\\n    text_segments_to_check[2][1],\\n    text_segments_to_check[2][2],\\n]\\ndifferent_segment_3 = [\\n    text_segments_to_check[2][-3],\\n    text_segments_to_check[2][-2],\\n    text_segments_to_check[2][-1],\\n    text_segments_to_check[3][0],\\n    text_segments_to_check[3][1],\\n    text_segments_to_check[3][2],\\n]\\ndifferent_segment_4 = [\\n    text_segments_to_check[3][-3],\\n    text_segments_to_check[3][-2],\\n    text_segments_to_check[3][-1],\\n    text_segments_to_check[4][0],\\n    text_segments_to_check[4][1],\\n    text_segments_to_check[4][2],\\n]\\ndifferent_segment_5 = [\\n    text_segments_to_check[4][-3],\\n    text_segments_to_check[4][-2],\\n    text_segments_to_check[4][-1],\\n    text_segments_to_check[5][0],\\n    text_segments_to_check[5][1],\\n    text_segments_to_check[5][2],\\n]\\n\\nsame_segment_1 = [\\n    text_segments_to_check[0][0],\\n    text_segments_to_check[0][1],\\n    text_segments_to_check[0][2],\\n]\\nsame_segment_2 = [\\n    text_segments_to_check[1][0],\\n    text_segments_to_check[1][1],\\n    text_segments_to_check[1][2],\\n]\\nsame_segment_3 = [\\n    text_segments_to_check[2][0],\\n    text_segments_to_check[2][1],\\n    text_segments_to_check[2][2],\\n]\\nsame_segment_4 = [\\n    text_segments_to_check[3][0],\\n    text_segments_to_check[3][1],\\n    text_segments_to_check[3][2],\\n]\\nsame_segment_5 = [\\n    text_segments_to_check[4][0],\\n    text_segments_to_check[4][1],\\n    text_segments_to_check[4][2],\\n]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "different_segment_1 = [\n",
    "    text_segments_to_check[0][-3],\n",
    "    text_segments_to_check[0][-2],\n",
    "    text_segments_to_check[0][-1],\n",
    "    text_segments_to_check[1][0],\n",
    "    text_segments_to_check[1][1],\n",
    "    text_segments_to_check[1][2],\n",
    "]\n",
    "different_segment_2 = [\n",
    "    text_segments_to_check[1][-3],\n",
    "    text_segments_to_check[1][-2],\n",
    "    text_segments_to_check[1][-1],\n",
    "    text_segments_to_check[2][0],\n",
    "    text_segments_to_check[2][1],\n",
    "    text_segments_to_check[2][2],\n",
    "]\n",
    "different_segment_3 = [\n",
    "    text_segments_to_check[2][-3],\n",
    "    text_segments_to_check[2][-2],\n",
    "    text_segments_to_check[2][-1],\n",
    "    text_segments_to_check[3][0],\n",
    "    text_segments_to_check[3][1],\n",
    "    text_segments_to_check[3][2],\n",
    "]\n",
    "different_segment_4 = [\n",
    "    text_segments_to_check[3][-3],\n",
    "    text_segments_to_check[3][-2],\n",
    "    text_segments_to_check[3][-1],\n",
    "    text_segments_to_check[4][0],\n",
    "    text_segments_to_check[4][1],\n",
    "    text_segments_to_check[4][2],\n",
    "]\n",
    "different_segment_5 = [\n",
    "    text_segments_to_check[4][-3],\n",
    "    text_segments_to_check[4][-2],\n",
    "    text_segments_to_check[4][-1],\n",
    "    text_segments_to_check[5][0],\n",
    "    text_segments_to_check[5][1],\n",
    "    text_segments_to_check[5][2],\n",
    "]\n",
    "\n",
    "same_segment_1 = [\n",
    "    text_segments_to_check[0][0],\n",
    "    text_segments_to_check[0][1],\n",
    "    text_segments_to_check[0][2],\n",
    "]\n",
    "same_segment_2 = [\n",
    "    text_segments_to_check[1][0],\n",
    "    text_segments_to_check[1][1],\n",
    "    text_segments_to_check[1][2],\n",
    "]\n",
    "same_segment_3 = [\n",
    "    text_segments_to_check[2][0],\n",
    "    text_segments_to_check[2][1],\n",
    "    text_segments_to_check[2][2],\n",
    "]\n",
    "same_segment_4 = [\n",
    "    text_segments_to_check[3][0],\n",
    "    text_segments_to_check[3][1],\n",
    "    text_segments_to_check[3][2],\n",
    "]\n",
    "same_segment_5 = [\n",
    "    text_segments_to_check[4][0],\n",
    "    text_segments_to_check[4][1],\n",
    "    text_segments_to_check[4][2],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed7cac3b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_coherence() got an unexpected keyword argument 'coherence_similarity'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m target_labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# test coherence on different segments\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m coherence_tester(\n\u001b[1;32m      6\u001b[0m     different_segment_1[:num_sentences_to_check], target_labels[:num_sentences_to_check]\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m coherence_tester(\n\u001b[1;32m      9\u001b[0m     different_segment_2[:num_sentences_to_check], target_labels[:num_sentences_to_check]\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m coherence_tester(\n\u001b[1;32m     12\u001b[0m     different_segment_3[:num_sentences_to_check], target_labels[:num_sentences_to_check]\n\u001b[1;32m     13\u001b[0m )\n",
      "Cell \u001b[0;32mIn [13], line 21\u001b[0m, in \u001b[0;36mcoherence_tester\u001b[0;34m(text_data, text_labels, max_tokens, max_str_length, prediction_thresh)\u001b[0m\n\u001b[1;32m     18\u001b[0m prev_row \u001b[38;5;241m=\u001b[39m truncate_by_token(prev_row, max_tokens)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# add the keywords to the coherence map\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m coherence_map\u001b[38;5;241m.\u001b[39mextend(\u001b[43mcoherence\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_coherence\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_row\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoherence_similarity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCoherence Map: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m[x[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m coherence_map]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pruning \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(coherence_map) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m pruning_min:\n",
      "\u001b[0;31mTypeError\u001b[0m: get_coherence() got an unexpected keyword argument 'coherence_similarity'"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 18;\n",
       "                var nbb_unformatted_code = \"num_sentences_to_check = 6  # test only the first n in the segment\\ntarget_labels = [0, 0, 0, 1, 0, 0]\\n\\n# test coherence on different segments\\ncoherence_tester(\\n    different_segment_1[:num_sentences_to_check], target_labels[:num_sentences_to_check]\\n)\\ncoherence_tester(\\n    different_segment_2[:num_sentences_to_check], target_labels[:num_sentences_to_check]\\n)\\ncoherence_tester(\\n    different_segment_3[:num_sentences_to_check], target_labels[:num_sentences_to_check]\\n)\\ncoherence_tester(\\n    different_segment_4[:num_sentences_to_check], target_labels[:num_sentences_to_check]\\n)\\ncoherence_tester(\\n    different_segment_5[:num_sentences_to_check], target_labels[:num_sentences_to_check]\\n)\";\n",
       "                var nbb_formatted_code = \"num_sentences_to_check = 6  # test only the first n in the segment\\ntarget_labels = [0, 0, 0, 1, 0, 0]\\n\\n# test coherence on different segments\\ncoherence_tester(\\n    different_segment_1[:num_sentences_to_check], target_labels[:num_sentences_to_check]\\n)\\ncoherence_tester(\\n    different_segment_2[:num_sentences_to_check], target_labels[:num_sentences_to_check]\\n)\\ncoherence_tester(\\n    different_segment_3[:num_sentences_to_check], target_labels[:num_sentences_to_check]\\n)\\ncoherence_tester(\\n    different_segment_4[:num_sentences_to_check], target_labels[:num_sentences_to_check]\\n)\\ncoherence_tester(\\n    different_segment_5[:num_sentences_to_check], target_labels[:num_sentences_to_check]\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_sentences_to_check = 6  # test only the first n in the segment\n",
    "target_labels = [0, 0, 0, 1, 0, 0]\n",
    "\n",
    "# test coherence on different segments\n",
    "coherence_tester(\n",
    "    different_segment_1[:num_sentences_to_check], target_labels[:num_sentences_to_check]\n",
    ")\n",
    "coherence_tester(\n",
    "    different_segment_2[:num_sentences_to_check], target_labels[:num_sentences_to_check]\n",
    ")\n",
    "coherence_tester(\n",
    "    different_segment_3[:num_sentences_to_check], target_labels[:num_sentences_to_check]\n",
    ")\n",
    "coherence_tester(\n",
    "    different_segment_4[:num_sentences_to_check], target_labels[:num_sentences_to_check]\n",
    ")\n",
    "coherence_tester(\n",
    "    different_segment_5[:num_sentences_to_check], target_labels[:num_sentences_to_check]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f34ae8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence: ['shoreline', 'basque', 'seashore', 'basque']\n",
      "0, tensor([0.2451]), In spite of appearances both t.. <> The city is in the north of th..\n",
      "0, tensor([0.3396]), In spite of appearances both t.. <> The city is in the north of th..\n",
      "Coherence: ['precipitation', 'shoreline', 'climate', 'shoreline']\n",
      "0, tensor([0.2880]), The city is in the north of th.. <> San Sebastin features an ocea..\n",
      "0, tensor([0.3303]), The city is in the north of th.. <> San Sebastin features an ocea..\n",
      "Coherence: ['census', 'sioux', 'area', 'sioux']\n",
      "0, tensor([0.2409]), Hospers was founded in 1872 wh.. <> Hospers is located at 43 07103..\n",
      "0, tensor([0.2987]), Hospers was founded in 1872 wh.. <> Hospers is located at 43 07103..\n",
      "Coherence: ['households', 'census', 'census', 'census']\n",
      "0, tensor([0.2941]), Hospers is located at 43 07103.. <> As of the census of 2010 there..\n",
      "0, tensor([0.3422]), Hospers is located at 43 07103.. <> As of the census of 2010 there..\n",
      "Coherence: ['1943', 'cossacks', '1942', 'cossacks']\n",
      "0, tensor([0.3146]), First mentioned in 1571 in con.. <> During World War II the Red Ar..\n",
      "0, tensor([0.3753]), First mentioned in 1571 in con.. <> During World War II the Red Ar..\n",
      "Coherence: ['separatists', '1943', 'ukrainian', '1943']\n",
      "0, tensor([0.3053]), During World War II the Red Ar.. <> The town was the site of spora..\n",
      "0, tensor([0.3648]), During World War II the Red Ar.. <> The town was the site of spora..\n",
      "Coherence: ['households', 'census', 'census', 'census']\n",
      "0, tensor([0.2690]), Wallingford is located at 43 3.. <> As of the census of 2010 there..\n",
      "0, tensor([0.3508]), Wallingford is located at 43 3.. <> As of the census of 2010 there..\n",
      "Coherence: ['households', 'households', 'median', 'households']\n",
      "0, tensor([0.5013]), As of the census of 2010 there.. <> As of the census of 2000 there..\n",
      "0, tensor([0.4821]), As of the census of 2010 there.. <> As of the census of 2000 there..\n",
      "Coherence: ['springs', 'winery', 'sources', 'winery']\n",
      "0, tensor([0.3112]), The health resort village of B.. <> The remains of an ancient vill..\n",
      "0, tensor([0.3717]), The health resort village of B.. <> The remains of an ancient vill..\n",
      "Coherence: ['boris', 'springs', 'iii', 'springs']\n",
      "0, tensor([0.3035]), The remains of an ancient vill.. <> The Banya Palace summerhouse o..\n",
      "0, tensor([0.3243]), The remains of an ancient vill.. <> The Banya Palace summerhouse o..\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 18;\n",
       "                var nbb_unformatted_code = \"# test coherence on same segments\\ncoherence_tester(same_segment_1, [0,0,0])\\ncoherence_tester(same_segment_2, [0,0,0])\\ncoherence_tester(same_segment_3, [0,0,0])\\ncoherence_tester(same_segment_4, [0,0,0])\\ncoherence_tester(same_segment_5, [0,0,0])\";\n",
       "                var nbb_formatted_code = \"# test coherence on same segments\\ncoherence_tester(same_segment_1, [0, 0, 0])\\ncoherence_tester(same_segment_2, [0, 0, 0])\\ncoherence_tester(same_segment_3, [0, 0, 0])\\ncoherence_tester(same_segment_4, [0, 0, 0])\\ncoherence_tester(same_segment_5, [0, 0, 0])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test coherence on same segments\n",
    "coherence_tester(same_segment_1, [0, 0, 0])\n",
    "coherence_tester(same_segment_2, [0, 0, 0])\n",
    "coherence_tester(same_segment_3, [0, 0, 0])\n",
    "coherence_tester(same_segment_4, [0, 0, 0])\n",
    "coherence_tester(same_segment_5, [0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "810d05f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Map: ['shoreline', 'basque', 'seashore', 'basque']\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['shoreline', 'basque', 'seashore', 'basque', 'precipitation', 'shoreline', 'climate', 'shoreline']\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['shoreline', 'basque', 'seashore', 'basque', 'precipitation', 'shoreline', 'climate', 'shoreline', 'paleolithic', 'precipitation', 'evidence', 'precipitation']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 1\n",
      "Coherence Map: ['basque', 'paleolithic', 'roman', 'paleolithic']\n",
      "Label: 0, Prediction: 1\n",
      "Coherence Map: ['castile', 'san', 'colonizers', 'san']\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['castile', 'san', 'colonizers', 'san', 'navarre', 'castile', 'iberian', 'castile']\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['castile', 'san', 'colonizers', 'san', 'navarre', 'castile', 'iberian', 'castile', 'neoclassical', 'navarre', 'bourgeois', 'navarre']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 1\n",
      "Coherence Map: ['railway', 'neoclassical', 'bridge', 'neoclassical']\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['railway', 'neoclassical', 'bridge', 'neoclassical', 'residents', 'railway', 'walls', 'railway']\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['railway', 'neoclassical', 'bridge', 'neoclassical', 'residents', 'railway', 'walls', 'railway', 'industry', 'residents', 'nucleus', 'residents']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['railway', 'railway', 'railway', 'neoclassical', 'neoclassical', 'residents', 'residents', 'residents', 'terminal', 'industry', 'centre', 'industry']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['railway', 'railway', 'railway', 'neoclassical', 'neoclassical', 'residents', 'residents', 'residents', 'district', 'terminal', 'avenida', 'terminal']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['district', 'avenida', 'railway', 'railway', 'railway', 'neoclassical', 'neoclassical', 'residents', 'district', 'district', 'beach', 'district']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['district', 'district', 'district', 'avenida', 'railway', 'railway', 'railway', 'neoclassical', 'postwar', 'district', 'dictator', 'district']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['district', 'district', 'district', 'postwar', 'avenida', 'railway', 'railway', 'railway', 'railway', 'postwar', 'former', 'postwar']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['district', 'district', 'district', 'postwar', 'postwar', 'postwar', 'avenida', 'railway', 'railway', 'railway', 'civil', 'railway']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['district', 'district', 'district', 'postwar', 'postwar', 'postwar', 'avenida', 'railway', 'inhabitants', 'railway', 'immigrants', 'railway']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['district', 'district', 'district', 'postwar', 'postwar', 'postwar', 'avenida', 'railway', 'nanotechnology', 'inhabitants', 'campus', 'inhabitants']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['district', 'district', 'district', 'postwar', 'postwar', 'postwar', 'avenida', 'railway', 'downtown', 'nanotechnology', 'axis', 'nanotechnology']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['district', 'district', 'district', 'postwar', 'postwar', 'postwar', 'avenida', 'railway', 'bridge', 'district', 'pedestrian', 'district']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['bridge', 'district', 'district', 'district', 'district', 'district', 'postwar', 'postwar', 'municipality', 'bridge', 'training', 'bridge']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['bridge', 'bridge', 'bridge', 'district', 'district', 'district', 'district', 'district', 'nursery', 'municipality', 'nurseries', 'municipality']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['bridge', 'bridge', 'bridge', 'district', 'district', 'district', 'district', 'district', 'village', 'nursery', 'reconstruction', 'nursery']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['bridge', 'bridge', 'bridge', 'district', 'district', 'district', 'district', 'district', 'festivals', 'village', 'festival', 'village']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['festivals', 'festival', 'bridge', 'bridge', 'bridge', 'district', 'district', 'district', 'celebrations', 'festivals', 'parades', 'festivals']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['festivals', 'festivals', 'festivals', 'festival', 'bridge', 'bridge', 'bridge', 'celebrations', 'fireworks', 'celebrations', 'festival', 'celebrations']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['festivals', 'festivals', 'festivals', 'festival', 'bridge', 'bridge', 'bridge', 'celebrations', 'basque', 'fireworks', 'festival', 'fireworks']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['festivals', 'festivals', 'festivals', 'festival', 'bridge', 'bridge', 'bridge', 'celebrations', 'basque', 'basque', 'traditional', 'basque']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['festivals', 'festivals', 'festivals', 'festival', 'bridge', 'bridge', 'bridge', 'celebrations', 'festival', 'basque', 'gypsy', 'basque']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['festivals', 'festivals', 'festivals', 'festival', 'bridge', 'bridge', 'bridge', 'celebrations', 'festival', 'festival', 'stalls', 'festival']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['festivals', 'festivals', 'festivals', 'festival', 'festival', 'bridge', 'bridge', 'bridge', 'basque', 'festival', 'villages', 'festival']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['festivals', 'festivals', 'festivals', 'festival', 'festival', 'festival', 'festival', 'bridge', 'san', 'basque', 'tourism', 'basque']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['festivals', 'festivals', 'festivals', 'festival', 'festival', 'festival', 'festival', 'bridge', 'michelin', 'san', 'gastronomy', 'san']\n",
      "pruning... 12\n",
      "done pruning... 8\n",
      "Label: 0, Prediction: 1\n",
      "Coherence Map: ['universidad', 'michelin', 'universities', 'michelin']\n",
      "Label: 0, Prediction: 0\n",
      "Coherence Map: ['universidad', 'michelin', 'universities', 'michelin', 'uci', 'universidad', 'sociedad', 'universidad']\n",
      "Label: 0, Prediction: 0\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 48;\n",
       "                var nbb_unformatted_code = \"# test on a full segment\\npredictions = coherence_tester(text_segments_to_check[0], text_labels_to_check[0])\";\n",
       "                var nbb_formatted_code = \"# test on a full segment\\npredictions = coherence_tester(text_segments_to_check[0], text_labels_to_check[0])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test on a full segment\n",
    "predictions = coherence_tester(text_segments_to_check[0], text_labels_to_check[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91a3613",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00458200",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
