{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dc7697a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"# Run if working locally\\n%load_ext autoreload\\n%autoreload 2\\n%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"# Run if working locally\\n%load_ext autoreload\\n%autoreload 2\\n%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run if working locally\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88ee84b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 12;\n",
       "                var nbb_unformatted_code = \"import sqlite3\\nfrom sqlite3 import Error\\nimport pickle\\nimport os, sys\\nimport config\\n\\nconfig.root_path = os.path.abspath(os.path.join(os.getcwd(), \\\"..\\\"))\\nsys.path.insert(0, config.root_path)\\n\\nfrom src.dataset.dataset import RawData\\nfrom src.dataset.wikisection_preprocessing import (\\n    tokenize,\\n    clean_sentence,\\n    preprocess_text_segmentation,\\n    format_data_for_db_insertion,\\n)\\nfrom src.dataset.utils import truncate_by_token\\nfrom db.dbv2 import Table, AugmentedTable, TrainTestTable\\nimport pprint\\n\\nfrom utils.metrics import windowdiff, pk\\n\\nfrom src.bertkeywords.src.similarities import Embedding, Similarities\\nfrom src.bertkeywords.src.keywords import Keywords\\nfrom src.encoders.coherence import Coherence\\nfrom src.dataset.utils import flatten, dedupe_list, truncate_string\";\n",
       "                var nbb_formatted_code = \"import sqlite3\\nfrom sqlite3 import Error\\nimport pickle\\nimport os, sys\\nimport config\\n\\nconfig.root_path = os.path.abspath(os.path.join(os.getcwd(), \\\"..\\\"))\\nsys.path.insert(0, config.root_path)\\n\\nfrom src.dataset.dataset import RawData\\nfrom src.dataset.wikisection_preprocessing import (\\n    tokenize,\\n    clean_sentence,\\n    preprocess_text_segmentation,\\n    format_data_for_db_insertion,\\n)\\nfrom src.dataset.utils import truncate_by_token\\nfrom db.dbv2 import Table, AugmentedTable, TrainTestTable\\nimport pprint\\n\\nfrom utils.metrics import windowdiff, pk\\n\\nfrom src.bertkeywords.src.similarities import Embedding, Similarities\\nfrom src.bertkeywords.src.keywords import Keywords\\nfrom src.encoders.coherence import Coherence\\nfrom src.dataset.utils import flatten, dedupe_list, truncate_string\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sqlite3\n",
    "from sqlite3 import Error\n",
    "import pickle\n",
    "import os, sys\n",
    "import config\n",
    "\n",
    "config.root_path = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.insert(0, config.root_path)\n",
    "\n",
    "from src.dataset.dataset import RawData\n",
    "from src.dataset.wikisection_preprocessing import (\n",
    "    tokenize,\n",
    "    clean_sentence,\n",
    "    preprocess_text_segmentation,\n",
    "    format_data_for_db_insertion,\n",
    ")\n",
    "from src.dataset.utils import truncate_by_token\n",
    "from db.dbv2 import Table, AugmentedTable, TrainTestTable\n",
    "import pprint\n",
    "\n",
    "from utils.metrics import windowdiff, pk\n",
    "\n",
    "from src.bertkeywords.src.similarities import Embedding, Similarities\n",
    "from src.bertkeywords.src.keywords import Keywords\n",
    "from src.encoders.coherence import Coherence\n",
    "from src.dataset.utils import flatten, dedupe_list, truncate_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "id": "6e4dcd8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f2d9f19d09443a498789587b01f370c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/804 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "944ba04839db4031b6da9b53929bc84a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/1.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "182dfeaa48594a11982dce7d56cfec0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/411 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04daa003ac7d4042b41cf3a82ea62d6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/5.22M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16681be6fb6e4bddbe6b3197b8a712ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/9.62M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30d1e7735c0e4d3497653356b9d7bae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-14 13:21:24.166444: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "179e4ea1a7494cf9b2bb2fc27a73e2ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)be010/.gitattributes:   0%|          | 0.00/391 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c99c5a8830a54263aa2971752592b701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6761368ed53c4832994d04872ecea9b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/2_Dense/config.json:   0%|          | 0.00/114 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "443263fceaa74d4c9394783b146f13bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/2.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22fc653d8e504ead80083e1deb8bd08c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)168ebbe010/README.md:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c60382aed514b45bb62191f0b9afae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)8ebbe010/config.json:   0%|          | 0.00/804 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a52ed76f14714a9b893ee9237b552e1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ce_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15786bed7ecf478c9d396660313db398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/1.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faf23e0f616f44edbda8f8bad9daef50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3084ee69926740e786f409274769031c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c911b31383d4a17b223e6b8332467a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)be010/tokenizer.json:   0%|          | 0.00/9.62M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88264497b07644c9947f452fbb28ec26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/411 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2c14ff72e6044aebba5927d6cfdf5f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)168ebbe010/vocab.txt:   0%|          | 0.00/5.22M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52c0cc6101a14dc6acf3052f2ee1f804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ebbe010/modules.json:   0%|          | 0.00/461 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-14 13:22:05.241162: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 759;\n",
       "                var nbb_unformatted_code = \"# initialize the coherence library\\nmax_words_per_step = 3\\ncoherence = Coherence(max_words_per_step=max_words_per_step)\";\n",
       "                var nbb_formatted_code = \"# initialize the coherence library\\nmax_words_per_step = 3\\ncoherence = Coherence(max_words_per_step=max_words_per_step)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialize the coherence library\n",
    "max_words_per_step = 3\n",
    "coherence = Coherence(max_words_per_step=max_words_per_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "id": "226021b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2023-04-14 11:15:17.082057: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /Users/amitmaraj/.cache/torch/sentence_transformers/bert-base-uncased. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /Users/amitmaraj/.cache/torch/sentence_transformers/bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2023-04-14 11:15:19.961632: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 605;\n",
       "                var nbb_unformatted_code = \"# initialize the keywords and embeddings library\\npp = pprint.PrettyPrinter(indent=4)\\nsimilarities_lib = Similarities(\\\"bert-base-uncased\\\")\\nkeywords_lib = Keywords(similarities_lib.model, similarities_lib.tokenizer)\\nembedding_lib = Embedding(similarities_lib.model, similarities_lib.tokenizer)\";\n",
       "                var nbb_formatted_code = \"# initialize the keywords and embeddings library\\npp = pprint.PrettyPrinter(indent=4)\\nsimilarities_lib = Similarities(\\\"bert-base-uncased\\\")\\nkeywords_lib = Keywords(similarities_lib.model, similarities_lib.tokenizer)\\nembedding_lib = Embedding(similarities_lib.model, similarities_lib.tokenizer)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialize the keywords and embeddings library\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "similarities_lib = Similarities(\"bert-base-uncased\")\n",
    "keywords_lib = Keywords(similarities_lib.model, similarities_lib.tokenizer)\n",
    "embedding_lib = Embedding(similarities_lib.model, similarities_lib.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bb2458b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"dataset_type = \\\"city\\\"\\ntable = Table(dataset_type)\\naugmented_table = AugmentedTable(dataset_type)\\ntrain_test_table = TrainTestTable(dataset_type)\";\n",
       "                var nbb_formatted_code = \"dataset_type = \\\"city\\\"\\ntable = Table(dataset_type)\\naugmented_table = AugmentedTable(dataset_type)\\ntrain_test_table = TrainTestTable(dataset_type)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_type = \"city\"\n",
    "table = Table(dataset_type)\n",
    "augmented_table = AugmentedTable(dataset_type)\n",
    "train_test_table = TrainTestTable(dataset_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfd59c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"data = table.get_all()\\n\\ntext_data = [x[1] for x in data]\\ntext_labels = [x[2] for x in data]\";\n",
       "                var nbb_formatted_code = \"data = table.get_all()\\n\\ntext_data = [x[1] for x in data]\\ntext_labels = [x[2] for x in data]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = table.get_all()\n",
    "\n",
    "text_data = [x[1] for x in data]\n",
    "text_labels = [x[2] for x in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcccad5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"all_segments = table.get_all_segments()\\ntext_segments = [[y[1] for y in x] for x in all_segments]\\nsegments_labels = [[1 if i == 0 else 0 for i, y in enumerate(x)] for x in all_segments]\";\n",
       "                var nbb_formatted_code = \"all_segments = table.get_all_segments()\\ntext_segments = [[y[1] for y in x] for x in all_segments]\\nsegments_labels = [[1 if i == 0 else 0 for i, y in enumerate(x)] for x in all_segments]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_segments = table.get_all_segments()\n",
    "text_segments = [[y[1] for y in x] for x in all_segments]\n",
    "segments_labels = [[1 if i == 0 else 0 for i, y in enumerate(x)] for x in all_segments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "295657fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"samples = 5\\nmax_tokens = 400\\n\\nfor i, (segment, labels) in enumerate(\\n    zip(text_segments[:samples], segments_labels[:samples])\\n):\\n    for sentence, label in zip(segment, labels):\\n        # this is the training case. During inference, we will have no idea\\n        # when segments start and when they end.\\n        pass\";\n",
       "                var nbb_formatted_code = \"samples = 5\\nmax_tokens = 400\\n\\nfor i, (segment, labels) in enumerate(\\n    zip(text_segments[:samples], segments_labels[:samples])\\n):\\n    for sentence, label in zip(segment, labels):\\n        # this is the training case. During inference, we will have no idea\\n        # when segments start and when they end.\\n        pass\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "samples = 5\n",
    "max_tokens = 400\n",
    "\n",
    "for i, (segment, labels) in enumerate(\n",
    "    zip(text_segments[:samples], segments_labels[:samples])\n",
    "):\n",
    "    for sentence, label in zip(segment, labels):\n",
    "        # this is the training case. During inference, we will have no idea\n",
    "        # when segments start and when they end.\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "467789d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"text_labels[:25]\";\n",
       "                var nbb_formatted_code = \"text_labels[:25]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_labels[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "id": "f6870992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 769;\n",
       "                var nbb_unformatted_code = \"pruning = 0  # remove the lowest n important words from coherence map\\npruning_min = 10  # only prune after n words in the coherence map\\n\\n\\ndef get_weighted_average(weighted_similarities, weights):\\n    return sum(weighted_similarities) / sum(weights)\\n\\n\\n# importance testing\\ndef compare_coherent_words(coherence_map, keywords_current, suppress_errors=False):\\n    word_comparisons = []\\n    weights = []\\n    for i, keywords in enumerate(coherence_map[::-1]):\\n        for word_tuple in keywords:\\n            word = word_tuple[0]\\n            for second_word_tuple in keywords_current:\\n                second_word = second_word_tuple[0]\\n\\n                try:\\n                    word_one_emb = word_tuple[2]\\n                    word_two_emb = second_word_tuple[2]\\n\\n                    # this weight is a recipricol function that will grow smaller the further the keywords are away\\n                    # we want to put more importance on the current words, so we apply twice as much weight.\\n                    if i == 0:\\n                        weight = 2 / (i + 1)\\n                    else:\\n                        weight = 1 / (i + 1)\\n\\n                    word_comparisons.append(\\n                        (\\n                            word,\\n                            second_word,\\n                            weight\\n                            * embedding_lib.get_similarity(word_one_emb, word_two_emb),\\n                        )\\n                    )\\n                    weights.append(weight)\\n                except AssertionError as e:\\n                    if not suppress_errors:\\n                        print(e, word, second_word)\\n\\n    return word_comparisons, weights\\n\\n\\n# TODO: add weighted average: https://www.google.com/search?q=weighted+average&rlz=1C5CHFA_enCA1019CA1024&sxsrf=APwXEdcb6dhJ5L_mvWvrWr4AxQcxOFB01g:1681098698316&tbm=isch&source=iu&ictx=1&vet=1&fir=V-LTDKtCElo89M%252C2WVwd1NrPkHFOM%252C_%253BVGk_lj0HALhXQM%252C2WVwd1NrPkHFOM%252C_%253ByzfbB4i3SpPTFM%252C5e7an03wLAdfhM%252C_%253B47HYmoDH6WlThM%252CsRXbJWfpyOLEOM%252C_%253BOsB4jtfzenfuyM%252CHKcmLkpfJ3xWqM%252C_&usg=AI4_-kRmBXgUWAm_nR3vDsLT17TqM5AvSQ&sa=X&ved=2ahUKEwi6hvvVtJ7-AhXJkIkEHe4JCX4Q_h16BAgoEAE#imgrc=V-LTDKtCElo89M\\ndef coherence_tester(\\n    text_data, text_labels, max_tokens=400, max_str_length=30, prediction_thresh=0.35\\n):\\n    coherence_map = []\\n    predictions = []\\n    for i, (row, label) in enumerate(zip(text_data, text_labels)):\\n        # compare the current sentence to the previous one\\n        if i == 0:\\n            predictions.append((0, 0))\\n        else:\\n            prev_row = text_data[i - 1]\\n\\n            row = truncate_by_token(row, max_tokens)\\n            prev_row = truncate_by_token(prev_row, max_tokens)\\n\\n            cohesion, keywords_prev, keywords_current = coherence.get_coherence(\\n                [row, prev_row], coherence_threshold=0.25\\n            )\\n\\n            # add the keywords to the coherence map\\n            coherence_map.append(cohesion)\\n            if pruning > 0 and len(coherence_map) >= pruning_min:\\n                print(\\\"pruning...\\\", len(coherence_map))\\n                sorted_map = sorted(\\n                    coherence_map, key=lambda tup: tup[1]\\n                )  # sort asc by importance based on keybert\\n                coherence_map = sorted_map[pruning:][\\n                    ::-1\\n                ]  # get the last n - pruning values and reverse the list\\n                print(\\\"done pruning...\\\", len(coherence_map))\\n\\n            # truncate the strings for printing\\n            truncated_row = truncate_string(row, max_str_length)\\n            truncated_prev_row = truncate_string(prev_row, max_str_length)\\n            print(\\n                f\\\"Coherence Map: {[[x[0] for x in c] for c in coherence_map]}, KW Curr: {[x[0] for x in keywords_current]}\\\"\\n            )\\n\\n            # compute the word comparisons between the previous (with the coherence map)\\n            # and the current (possibly the first sentence in a new segment)\\n            word_comparisons_with_coherence, weights = compare_coherent_words(\\n                [*coherence_map, keywords_prev], keywords_current\\n            )\\n\\n            similarities_with_coherence = [\\n                comparison[2] for comparison in word_comparisons_with_coherence\\n            ]\\n            avg_similarity_with_coherence = sum(similarities_with_coherence) / (\\n                len(similarities_with_coherence) or 1\\n            )\\n            weighted_avg_similarity_with_coherence = get_weighted_average(\\n                similarities_with_coherence, weights\\n            )\\n            print(f\\\"weighted: {weighted_avg_similarity_with_coherence}\\\")\\n\\n            # if the two sentences are similar, create a cohesive prediction\\n            # otherwise, predict a new segment\\n            if weighted_avg_similarity_with_coherence > prediction_thresh:\\n                print(\\n                    f\\\"Label: {label}, Prediction: {0}, logit: {weighted_avg_similarity_with_coherence}\\\"\\n                )\\n                predictions.append((weighted_avg_similarity_with_coherence, 0))\\n            else:\\n                # start of a new segment, empty the map\\n                coherence_map = []\\n                print(\\n                    f\\\"Label: {label}, Prediction: {1}, logit: {weighted_avg_similarity_with_coherence}\\\"\\n                )\\n                predictions.append((weighted_avg_similarity_with_coherence, 1))\\n\\n            print(\\\"===============================================\\\")\\n\\n    return predictions\";\n",
       "                var nbb_formatted_code = \"pruning = 0  # remove the lowest n important words from coherence map\\npruning_min = 10  # only prune after n words in the coherence map\\n\\n\\ndef get_weighted_average(weighted_similarities, weights):\\n    return sum(weighted_similarities) / sum(weights)\\n\\n\\n# importance testing\\ndef compare_coherent_words(coherence_map, keywords_current, suppress_errors=False):\\n    word_comparisons = []\\n    weights = []\\n    for i, keywords in enumerate(coherence_map[::-1]):\\n        for word_tuple in keywords:\\n            word = word_tuple[0]\\n            for second_word_tuple in keywords_current:\\n                second_word = second_word_tuple[0]\\n\\n                try:\\n                    word_one_emb = word_tuple[2]\\n                    word_two_emb = second_word_tuple[2]\\n\\n                    # this weight is a recipricol function that will grow smaller the further the keywords are away\\n                    # we want to put more importance on the current words, so we apply twice as much weight.\\n                    if i == 0:\\n                        weight = 2 / (i + 1)\\n                    else:\\n                        weight = 1 / (i + 1)\\n\\n                    word_comparisons.append(\\n                        (\\n                            word,\\n                            second_word,\\n                            weight\\n                            * embedding_lib.get_similarity(word_one_emb, word_two_emb),\\n                        )\\n                    )\\n                    weights.append(weight)\\n                except AssertionError as e:\\n                    if not suppress_errors:\\n                        print(e, word, second_word)\\n\\n    return word_comparisons, weights\\n\\n\\n# TODO: add weighted average: https://www.google.com/search?q=weighted+average&rlz=1C5CHFA_enCA1019CA1024&sxsrf=APwXEdcb6dhJ5L_mvWvrWr4AxQcxOFB01g:1681098698316&tbm=isch&source=iu&ictx=1&vet=1&fir=V-LTDKtCElo89M%252C2WVwd1NrPkHFOM%252C_%253BVGk_lj0HALhXQM%252C2WVwd1NrPkHFOM%252C_%253ByzfbB4i3SpPTFM%252C5e7an03wLAdfhM%252C_%253B47HYmoDH6WlThM%252CsRXbJWfpyOLEOM%252C_%253BOsB4jtfzenfuyM%252CHKcmLkpfJ3xWqM%252C_&usg=AI4_-kRmBXgUWAm_nR3vDsLT17TqM5AvSQ&sa=X&ved=2ahUKEwi6hvvVtJ7-AhXJkIkEHe4JCX4Q_h16BAgoEAE#imgrc=V-LTDKtCElo89M\\ndef coherence_tester(\\n    text_data, text_labels, max_tokens=400, max_str_length=30, prediction_thresh=0.35\\n):\\n    coherence_map = []\\n    predictions = []\\n    for i, (row, label) in enumerate(zip(text_data, text_labels)):\\n        # compare the current sentence to the previous one\\n        if i == 0:\\n            predictions.append((0, 0))\\n        else:\\n            prev_row = text_data[i - 1]\\n\\n            row = truncate_by_token(row, max_tokens)\\n            prev_row = truncate_by_token(prev_row, max_tokens)\\n\\n            cohesion, keywords_prev, keywords_current = coherence.get_coherence(\\n                [row, prev_row], coherence_threshold=0.25\\n            )\\n\\n            # add the keywords to the coherence map\\n            coherence_map.append(cohesion)\\n            if pruning > 0 and len(coherence_map) >= pruning_min:\\n                print(\\\"pruning...\\\", len(coherence_map))\\n                sorted_map = sorted(\\n                    coherence_map, key=lambda tup: tup[1]\\n                )  # sort asc by importance based on keybert\\n                coherence_map = sorted_map[pruning:][\\n                    ::-1\\n                ]  # get the last n - pruning values and reverse the list\\n                print(\\\"done pruning...\\\", len(coherence_map))\\n\\n            # truncate the strings for printing\\n            truncated_row = truncate_string(row, max_str_length)\\n            truncated_prev_row = truncate_string(prev_row, max_str_length)\\n            print(\\n                f\\\"Coherence Map: {[[x[0] for x in c] for c in coherence_map]}, KW Curr: {[x[0] for x in keywords_current]}\\\"\\n            )\\n\\n            # compute the word comparisons between the previous (with the coherence map)\\n            # and the current (possibly the first sentence in a new segment)\\n            word_comparisons_with_coherence, weights = compare_coherent_words(\\n                [*coherence_map, keywords_prev], keywords_current\\n            )\\n\\n            similarities_with_coherence = [\\n                comparison[2] for comparison in word_comparisons_with_coherence\\n            ]\\n            avg_similarity_with_coherence = sum(similarities_with_coherence) / (\\n                len(similarities_with_coherence) or 1\\n            )\\n            weighted_avg_similarity_with_coherence = get_weighted_average(\\n                similarities_with_coherence, weights\\n            )\\n            print(f\\\"weighted: {weighted_avg_similarity_with_coherence}\\\")\\n\\n            # if the two sentences are similar, create a cohesive prediction\\n            # otherwise, predict a new segment\\n            if weighted_avg_similarity_with_coherence > prediction_thresh:\\n                print(\\n                    f\\\"Label: {label}, Prediction: {0}, logit: {weighted_avg_similarity_with_coherence}\\\"\\n                )\\n                predictions.append((weighted_avg_similarity_with_coherence, 0))\\n            else:\\n                # start of a new segment, empty the map\\n                coherence_map = []\\n                print(\\n                    f\\\"Label: {label}, Prediction: {1}, logit: {weighted_avg_similarity_with_coherence}\\\"\\n                )\\n                predictions.append((weighted_avg_similarity_with_coherence, 1))\\n\\n            print(\\\"===============================================\\\")\\n\\n    return predictions\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pruning = 0  # remove the lowest n important words from coherence map\n",
    "pruning_min = 10  # only prune after n words in the coherence map\n",
    "\n",
    "\n",
    "def get_weighted_average(weighted_similarities, weights):\n",
    "    return sum(weighted_similarities) / sum(weights)\n",
    "\n",
    "\n",
    "# importance testing\n",
    "def compare_coherent_words(coherence_map, keywords_current, suppress_errors=False):\n",
    "    word_comparisons = []\n",
    "    weights = []\n",
    "    for i, keywords in enumerate(coherence_map[::-1]):\n",
    "        for word_tuple in keywords:\n",
    "            word = word_tuple[0]\n",
    "            for second_word_tuple in keywords_current:\n",
    "                second_word = second_word_tuple[0]\n",
    "\n",
    "                try:\n",
    "                    word_one_emb = word_tuple[2]\n",
    "                    word_two_emb = second_word_tuple[2]\n",
    "\n",
    "                    # this weight is a recipricol function that will grow smaller the further the keywords are away\n",
    "                    # we want to put more importance on the current words, so we apply twice as much weight.\n",
    "                    if i == 0:\n",
    "                        weight = 2 / (i + 1)\n",
    "                    else:\n",
    "                        weight = 1 / (i + 1)\n",
    "\n",
    "                    word_comparisons.append(\n",
    "                        (\n",
    "                            word,\n",
    "                            second_word,\n",
    "                            weight\n",
    "                            * embedding_lib.get_similarity(word_one_emb, word_two_emb),\n",
    "                        )\n",
    "                    )\n",
    "                    weights.append(weight)\n",
    "                except AssertionError as e:\n",
    "                    if not suppress_errors:\n",
    "                        print(e, word, second_word)\n",
    "\n",
    "    return word_comparisons, weights\n",
    "\n",
    "\n",
    "# TODO: add weighted average: https://www.google.com/search?q=weighted+average&rlz=1C5CHFA_enCA1019CA1024&sxsrf=APwXEdcb6dhJ5L_mvWvrWr4AxQcxOFB01g:1681098698316&tbm=isch&source=iu&ictx=1&vet=1&fir=V-LTDKtCElo89M%252C2WVwd1NrPkHFOM%252C_%253BVGk_lj0HALhXQM%252C2WVwd1NrPkHFOM%252C_%253ByzfbB4i3SpPTFM%252C5e7an03wLAdfhM%252C_%253B47HYmoDH6WlThM%252CsRXbJWfpyOLEOM%252C_%253BOsB4jtfzenfuyM%252CHKcmLkpfJ3xWqM%252C_&usg=AI4_-kRmBXgUWAm_nR3vDsLT17TqM5AvSQ&sa=X&ved=2ahUKEwi6hvvVtJ7-AhXJkIkEHe4JCX4Q_h16BAgoEAE#imgrc=V-LTDKtCElo89M\n",
    "def coherence_tester(\n",
    "    text_data, text_labels, max_tokens=400, max_str_length=30, prediction_thresh=0.35\n",
    "):\n",
    "    coherence_map = []\n",
    "    predictions = []\n",
    "    for i, (row, label) in enumerate(zip(text_data, text_labels)):\n",
    "        # compare the current sentence to the previous one\n",
    "        if i == 0:\n",
    "            predictions.append((0, 0))\n",
    "        else:\n",
    "            prev_row = text_data[i - 1]\n",
    "\n",
    "            row = truncate_by_token(row, max_tokens)\n",
    "            prev_row = truncate_by_token(prev_row, max_tokens)\n",
    "\n",
    "            cohesion, keywords_prev, keywords_current = coherence.get_coherence(\n",
    "                [row, prev_row], coherence_threshold=0.2\n",
    "            )\n",
    "\n",
    "            # add the keywords to the coherence map\n",
    "            coherence_map.append(cohesion)\n",
    "            if pruning > 0 and len(coherence_map) >= pruning_min:\n",
    "                print(\"pruning...\", len(coherence_map))\n",
    "                sorted_map = sorted(\n",
    "                    coherence_map, key=lambda tup: tup[1]\n",
    "                )  # sort asc by importance based on keybert\n",
    "                coherence_map = sorted_map[pruning:][\n",
    "                    ::-1\n",
    "                ]  # get the last n - pruning values and reverse the list\n",
    "                print(\"done pruning...\", len(coherence_map))\n",
    "\n",
    "            # truncate the strings for printing\n",
    "            truncated_row = truncate_string(row, max_str_length)\n",
    "            truncated_prev_row = truncate_string(prev_row, max_str_length)\n",
    "            print(\n",
    "                f\"Coherence Map: {[[x[0] for x in c] for c in coherence_map]}, KW Curr: {[x[0] for x in keywords_current]}\"\n",
    "            )\n",
    "\n",
    "            # compute the word comparisons between the previous (with the coherence map)\n",
    "            # and the current (possibly the first sentence in a new segment)\n",
    "            word_comparisons_with_coherence, weights = compare_coherent_words(\n",
    "                [*coherence_map, keywords_prev], keywords_current\n",
    "            )\n",
    "\n",
    "            similarities_with_coherence = [\n",
    "                comparison[2] for comparison in word_comparisons_with_coherence\n",
    "            ]\n",
    "            avg_similarity_with_coherence = sum(similarities_with_coherence) / (\n",
    "                len(similarities_with_coherence) or 1\n",
    "            )\n",
    "            weighted_avg_similarity_with_coherence = get_weighted_average(\n",
    "                similarities_with_coherence, weights\n",
    "            )\n",
    "            print(f\"weighted: {weighted_avg_similarity_with_coherence}\")\n",
    "\n",
    "            # if the two sentences are similar, create a cohesive prediction\n",
    "            # otherwise, predict a new segment\n",
    "            if weighted_avg_similarity_with_coherence > prediction_thresh:\n",
    "                print(\n",
    "                    f\"Label: {label}, Prediction: {0}, logit: {weighted_avg_similarity_with_coherence}\"\n",
    "                )\n",
    "                predictions.append((weighted_avg_similarity_with_coherence, 0))\n",
    "            else:\n",
    "                # start of a new segment, empty the map\n",
    "                coherence_map = []\n",
    "                print(\n",
    "                    f\"Label: {label}, Prediction: {1}, logit: {weighted_avg_similarity_with_coherence}\"\n",
    "                )\n",
    "                predictions.append((weighted_avg_similarity_with_coherence, 1))\n",
    "\n",
    "            print(\"===============================================\")\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "id": "ca71b4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1963', 'suva', 'sports']\n",
      "['fiji', 'broadcasting', 'television']\n",
      "Got the keywords in 0.6738 seconds\n",
      "Got the embeddings and comparisons in 0.0006 seconds\n",
      "Coherence Map: [['fiji', 'suva', 'broadcasting']], KW Curr: ['1963', 'suva', 'sports']\n",
      "weighted: tensor([0.3807])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3807])\n",
      "===============================================\n",
      "['fiji', 'broadcasting', 'television']\n",
      "['suva', 'tv', 'shows']\n",
      "Got the keywords in 1.0819 seconds\n",
      "Got the embeddings and comparisons in 0.0006 seconds\n",
      "Coherence Map: [['fiji', 'suva', 'broadcasting'], ['suva', 'fiji', 'tv']], KW Curr: ['fiji', 'broadcasting', 'television']\n",
      "weighted: tensor([0.3865])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3865])\n",
      "===============================================\n",
      "['suva', 'tv', 'shows']\n",
      "['nausori', 'ships', 'nadi']\n",
      "Got the keywords in 1.0789 seconds\n",
      "Got the embeddings and comparisons in 0.0007 seconds\n",
      "Coherence Map: [['fiji', 'suva', 'broadcasting'], ['suva', 'fiji', 'tv'], ['nausori', 'suva', 'ships']], KW Curr: ['suva', 'tv', 'shows']\n",
      "weighted: tensor([0.5235])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.5235])\n",
      "===============================================\n",
      "['nausori', 'ships', 'nadi']\n",
      "['civoniceva', 'suva', 'sivivatu']\n",
      "Got the keywords in 1.0108 seconds\n",
      "Got the embeddings and comparisons in 0.0006 seconds\n",
      "Coherence Map: [['fiji', 'suva', 'broadcasting'], ['suva', 'fiji', 'tv'], ['nausori', 'suva', 'ships'], ['civoniceva', 'nausori', 'suva']], KW Curr: ['nausori', 'ships', 'nadi']\n",
      "weighted: tensor([0.3893])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3893])\n",
      "===============================================\n",
      "['civoniceva', 'suva', 'sivivatu']\n",
      "['kashiwara', 'yamato', 'river']\n",
      "Got the keywords in 0.8522 seconds\n",
      "Got the embeddings and comparisons in 0.0008 seconds\n",
      "Coherence Map: [['fiji', 'suva', 'broadcasting'], ['suva', 'fiji', 'tv'], ['nausori', 'suva', 'ships'], ['civoniceva', 'nausori', 'suva'], ['kashiwara', 'civoniceva', 'yamato']], KW Curr: ['civoniceva', 'suva', 'sivivatu']\n",
      "weighted: tensor([0.4134])\n",
      "Label: 1, Prediction: 0, logit: tensor([0.4134])\n",
      "===============================================\n",
      "['kashiwara', 'yamato', 'river']\n",
      "['kashiwara', 'kawachi', '1956']\n",
      "Got the keywords in 0.6678 seconds\n",
      "Got the embeddings and comparisons in 0.0003 seconds\n",
      "Coherence Map: [['fiji', 'suva', 'broadcasting'], ['suva', 'fiji', 'tv'], ['nausori', 'suva', 'ships'], ['civoniceva', 'nausori', 'suva'], ['kashiwara', 'civoniceva', 'yamato'], ['kashiwara', 'kashiwara', 'kashiwara']], KW Curr: ['kashiwara', 'yamato', 'river']\n",
      "weighted: tensor([0.3509])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3509])\n",
      "===============================================\n",
      "['kashiwara', 'kawachi', '1956']\n",
      "['havana', '529064', '111021']\n",
      "Got the keywords in 0.6369 seconds\n",
      "Got the embeddings and comparisons in 0.0002 seconds\n",
      "Coherence Map: [['fiji', 'suva', 'broadcasting'], ['suva', 'fiji', 'tv'], ['nausori', 'suva', 'ships'], ['civoniceva', 'nausori', 'suva'], ['kashiwara', 'civoniceva', 'yamato'], ['kashiwara', 'kashiwara', 'kashiwara'], ['havana', 'kashiwara', 'havana']], KW Curr: ['kashiwara', 'kawachi', '1956']\n",
      "weighted: tensor([0.4216])\n",
      "Label: 1, Prediction: 0, logit: tensor([0.4216])\n",
      "===============================================\n",
      "['havana', '529064', '111021']\n",
      "['marvinville', '1903', 'greenville']\n",
      "Got the keywords in 0.2614 seconds\n",
      "Got the embeddings and comparisons in 0.0004 seconds\n",
      "Coherence Map: [['fiji', 'suva', 'broadcasting'], ['suva', 'fiji', 'tv'], ['nausori', 'suva', 'ships'], ['civoniceva', 'nausori', 'suva'], ['kashiwara', 'civoniceva', 'yamato'], ['kashiwara', 'kashiwara', 'kashiwara'], ['havana', 'kashiwara', 'havana'], ['marvinville', 'havana', 'greenville']], KW Curr: ['havana', '529064', '111021']\n",
      "weighted: tensor([0.3766])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3766])\n",
      "===============================================\n",
      "['marvinville', '1903', 'greenville']\n",
      "['population', 'residing', 'householder']\n",
      "Got the keywords in 0.8029 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['fiji', 'suva', 'broadcasting'], ['suva', 'fiji', 'tv'], ['nausori', 'suva', 'ships'], ['civoniceva', 'nausori', 'suva'], ['kashiwara', 'civoniceva', 'yamato'], ['kashiwara', 'kashiwara', 'kashiwara'], ['havana', 'kashiwara', 'havana'], ['marvinville', 'havana', 'greenville'], ['population', 'marvinville', 'residing']], KW Curr: ['marvinville', '1903', 'greenville']\n",
      "weighted: tensor([0.3917])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3917])\n",
      "===============================================\n",
      "['population', 'residing', 'householder']\n",
      "['1905', 'alvarado', 'called']\n",
      "Got the keywords in 0.7730 seconds\n",
      "Got the embeddings and comparisons in 0.0003 seconds\n",
      "Coherence Map: [['fiji', 'suva', 'broadcasting'], ['suva', 'fiji', 'tv'], ['nausori', 'suva', 'ships'], ['civoniceva', 'nausori', 'suva'], ['kashiwara', 'civoniceva', 'yamato'], ['kashiwara', 'kashiwara', 'kashiwara'], ['havana', 'kashiwara', 'havana'], ['marvinville', 'havana', 'greenville'], ['population', 'marvinville', 'residing'], ['alvarado', 'population', 'called']], KW Curr: ['population', 'residing', 'householder']\n",
      "weighted: tensor([0.4070])\n",
      "Label: 1, Prediction: 0, logit: tensor([0.4070])\n",
      "===============================================\n",
      "['1905', 'alvarado', 'called']\n",
      "['total', 'census', 'city']\n",
      "Got the keywords in 0.1375 seconds\n",
      "Got the embeddings and comparisons in 0.0001 seconds\n",
      "Coherence Map: [['fiji', 'suva', 'broadcasting'], ['suva', 'fiji', 'tv'], ['nausori', 'suva', 'ships'], ['civoniceva', 'nausori', 'suva'], ['kashiwara', 'civoniceva', 'yamato'], ['kashiwara', 'kashiwara', 'kashiwara'], ['havana', 'kashiwara', 'havana'], ['marvinville', 'havana', 'greenville'], ['population', 'marvinville', 'residing'], ['alvarado', 'population', 'called'], ['total', 'alvarado', 'census']], KW Curr: ['1905', 'alvarado', 'called']\n",
      "weighted: tensor([0.4141])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.4141])\n",
      "===============================================\n",
      "['total', 'census', 'city']\n",
      "['population', 'residents', 'racial']\n",
      "Got the keywords in 0.3298 seconds\n",
      "Got the embeddings and comparisons in 0.0006 seconds\n",
      "Coherence Map: [['fiji', 'suva', 'broadcasting'], ['suva', 'fiji', 'tv'], ['nausori', 'suva', 'ships'], ['civoniceva', 'nausori', 'suva'], ['kashiwara', 'civoniceva', 'yamato'], ['kashiwara', 'kashiwara', 'kashiwara'], ['havana', 'kashiwara', 'havana'], ['marvinville', 'havana', 'greenville'], ['population', 'marvinville', 'residing'], ['alvarado', 'population', 'called'], ['total', 'alvarado', 'census'], ['population', 'total', 'residents']], KW Curr: ['total', 'census', 'city']\n",
      "weighted: tensor([0.3595])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3595])\n",
      "===============================================\n",
      "['population', 'residents', 'racial']\n",
      "['population', 'households', 'householder']\n",
      "Got the keywords in 0.9629 seconds\n",
      "Got the embeddings and comparisons in 0.0006 seconds\n",
      "Coherence Map: [['fiji', 'suva', 'broadcasting'], ['suva', 'fiji', 'tv'], ['nausori', 'suva', 'ships'], ['civoniceva', 'nausori', 'suva'], ['kashiwara', 'civoniceva', 'yamato'], ['kashiwara', 'kashiwara', 'kashiwara'], ['havana', 'kashiwara', 'havana'], ['marvinville', 'havana', 'greenville'], ['population', 'marvinville', 'residing'], ['alvarado', 'population', 'called'], ['total', 'alvarado', 'census'], ['population', 'total', 'residents'], ['population', 'population', 'households']], KW Curr: ['population', 'residents', 'racial']\n",
      "weighted: tensor([0.4835])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.4835])\n",
      "===============================================\n",
      "['population', 'households', 'householder']\n",
      "['1890', '1901', '1899']\n",
      "Got the keywords in 0.7841 seconds\n",
      "Got the embeddings and comparisons in 0.0000 seconds\n",
      "Coherence Map: [['fiji', 'suva', 'broadcasting'], ['suva', 'fiji', 'tv'], ['nausori', 'suva', 'ships'], ['civoniceva', 'nausori', 'suva'], ['kashiwara', 'civoniceva', 'yamato'], ['kashiwara', 'kashiwara', 'kashiwara'], ['havana', 'kashiwara', 'havana'], ['marvinville', 'havana', 'greenville'], ['population', 'marvinville', 'residing'], ['alvarado', 'population', 'called'], ['total', 'alvarado', 'census'], ['population', 'total', 'residents'], ['population', 'population', 'households'], []], KW Curr: ['population', 'households', 'householder']\n",
      "weighted: tensor([0.4085])\n",
      "Label: 1, Prediction: 0, logit: tensor([0.4085])\n",
      "===============================================\n",
      "['1890', '1901', '1899']\n",
      "['total', 'census', 'city']\n",
      "Got the keywords in 0.1344 seconds\n",
      "Got the embeddings and comparisons in 0.0000 seconds\n",
      "Coherence Map: [['fiji', 'suva', 'broadcasting'], ['suva', 'fiji', 'tv'], ['nausori', 'suva', 'ships'], ['civoniceva', 'nausori', 'suva'], ['kashiwara', 'civoniceva', 'yamato'], ['kashiwara', 'kashiwara', 'kashiwara'], ['havana', 'kashiwara', 'havana'], ['marvinville', 'havana', 'greenville'], ['population', 'marvinville', 'residing'], ['alvarado', 'population', 'called'], ['total', 'alvarado', 'census'], ['population', 'total', 'residents'], ['population', 'population', 'households'], [], []], KW Curr: ['1890', '1901', '1899']\n",
      "weighted: tensor([0.3618])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3618])\n",
      "===============================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['total', 'census', 'city']\n",
      "['population', 'residents', 'racial']\n",
      "Got the keywords in 0.3228 seconds\n",
      "Got the embeddings and comparisons in 0.0006 seconds\n",
      "Coherence Map: [['fiji', 'suva', 'broadcasting'], ['suva', 'fiji', 'tv'], ['nausori', 'suva', 'ships'], ['civoniceva', 'nausori', 'suva'], ['kashiwara', 'civoniceva', 'yamato'], ['kashiwara', 'kashiwara', 'kashiwara'], ['havana', 'kashiwara', 'havana'], ['marvinville', 'havana', 'greenville'], ['population', 'marvinville', 'residing'], ['alvarado', 'population', 'called'], ['total', 'alvarado', 'census'], ['population', 'total', 'residents'], ['population', 'population', 'households'], [], [], ['population', 'total', 'residents']], KW Curr: ['total', 'census', 'city']\n",
      "weighted: tensor([0.3561])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3561])\n",
      "===============================================\n",
      "['population', 'residents', 'racial']\n",
      "['population', 'householder', 'households']\n",
      "Got the keywords in 0.9618 seconds\n",
      "Got the embeddings and comparisons in 0.0006 seconds\n",
      "Coherence Map: [['fiji', 'suva', 'broadcasting'], ['suva', 'fiji', 'tv'], ['nausori', 'suva', 'ships'], ['civoniceva', 'nausori', 'suva'], ['kashiwara', 'civoniceva', 'yamato'], ['kashiwara', 'kashiwara', 'kashiwara'], ['havana', 'kashiwara', 'havana'], ['marvinville', 'havana', 'greenville'], ['population', 'marvinville', 'residing'], ['alvarado', 'population', 'called'], ['total', 'alvarado', 'census'], ['population', 'total', 'residents'], ['population', 'population', 'households'], [], [], ['population', 'total', 'residents'], ['population', 'population', 'householder']], KW Curr: ['population', 'residents', 'racial']\n",
      "weighted: tensor([0.5624])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.5624])\n",
      "===============================================\n",
      "['population', 'householder', 'households']\n",
      "['washta', '575896', 'river']\n",
      "Got the keywords in 0.7679 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['fiji', 'suva', 'broadcasting'], ['suva', 'fiji', 'tv'], ['nausori', 'suva', 'ships'], ['civoniceva', 'nausori', 'suva'], ['kashiwara', 'civoniceva', 'yamato'], ['kashiwara', 'kashiwara', 'kashiwara'], ['havana', 'kashiwara', 'havana'], ['marvinville', 'havana', 'greenville'], ['population', 'marvinville', 'residing'], ['alvarado', 'population', 'called'], ['total', 'alvarado', 'census'], ['population', 'total', 'residents'], ['population', 'population', 'households'], [], [], ['population', 'total', 'residents'], ['population', 'population', 'householder'], ['washta', 'population', 'river']], KW Curr: ['population', 'householder', 'households']\n",
      "weighted: tensor([0.4156])\n",
      "Label: 1, Prediction: 0, logit: tensor([0.4156])\n",
      "===============================================\n",
      "['washta', '575896', 'river']\n",
      "['population', 'residents', 'living']\n",
      "Got the keywords in 0.7542 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['fiji', 'suva', 'broadcasting'], ['suva', 'fiji', 'tv'], ['nausori', 'suva', 'ships'], ['civoniceva', 'nausori', 'suva'], ['kashiwara', 'civoniceva', 'yamato'], ['kashiwara', 'kashiwara', 'kashiwara'], ['havana', 'kashiwara', 'havana'], ['marvinville', 'havana', 'greenville'], ['population', 'marvinville', 'residing'], ['alvarado', 'population', 'called'], ['total', 'alvarado', 'census'], ['population', 'total', 'residents'], ['population', 'population', 'households'], [], [], ['population', 'total', 'residents'], ['population', 'population', 'householder'], ['washta', 'population', 'river'], ['population', 'washta', 'residents']], KW Curr: ['washta', '575896', 'river']\n",
      "weighted: tensor([0.3375])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.3375])\n",
      "===============================================\n",
      "['population', 'residents', 'living']\n",
      "['householder', 'households', 'household']\n",
      "Got the keywords in 0.9504 seconds\n",
      "Got the embeddings and comparisons in 0.0006 seconds\n",
      "Coherence Map: [['householder', 'population', 'households']], KW Curr: ['population', 'residents', 'living']\n",
      "weighted: tensor([0.4874])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.4874])\n",
      "===============================================\n",
      "['householder', 'households', 'household']\n",
      "['1889', '1891', '1909']\n",
      "Got the keywords in 1.0756 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['householder', 'population', 'households'], []], KW Curr: ['householder', 'households', 'household']\n",
      "weighted: tensor([0.4245])\n",
      "Label: 1, Prediction: 0, logit: tensor([0.4245])\n",
      "===============================================\n",
      "['1889', '1891', '1909']\n",
      "['historic', 'register', 'district']\n",
      "Got the keywords in 0.8979 seconds\n",
      "Got the embeddings and comparisons in 0.0000 seconds\n",
      "Coherence Map: [['householder', 'population', 'households'], [], []], KW Curr: ['1889', '1891', '1909']\n",
      "weighted: tensor([0.3100])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.3100])\n",
      "===============================================\n",
      "['historic', 'register', 'district']\n",
      "['oklahoma', 'dallas', 'texas']\n",
      "Got the keywords in 0.6088 seconds\n",
      "Got the embeddings and comparisons in 0.0006 seconds\n",
      "Coherence Map: [['oklahoma', 'historic', 'dallas']], KW Curr: ['historic', 'register', 'district']\n",
      "weighted: tensor([0.4512])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.4512])\n",
      "===============================================\n",
      "['oklahoma', 'dallas', 'texas']\n",
      "['racial', 'population', 'residents']\n",
      "Got the keywords in 0.8882 seconds\n",
      "Got the embeddings and comparisons in 0.0006 seconds\n",
      "Coherence Map: [['oklahoma', 'historic', 'dallas'], ['racial', 'oklahoma', 'population']], KW Curr: ['oklahoma', 'dallas', 'texas']\n",
      "weighted: tensor([0.4351])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.4351])\n",
      "===============================================\n",
      "['racial', 'population', 'residents']\n",
      "['1975', 'city', 'town']\n",
      "Got the keywords in 1.0982 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['oklahoma', 'historic', 'dallas'], ['racial', 'oklahoma', 'population'], ['city', 'racial', 'town']], KW Curr: ['racial', 'population', 'residents']\n",
      "weighted: tensor([0.4372])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.4372])\n",
      "===============================================\n",
      "['1975', 'city', 'town']\n",
      "['1909', 'university', 'institution']\n",
      "Got the keywords in 0.9702 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['oklahoma', 'historic', 'dallas'], ['racial', 'oklahoma', 'population'], ['city', 'racial', 'town'], ['university', 'city', 'institution']], KW Curr: ['1975', 'city', 'town']\n",
      "weighted: tensor([0.4309])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.4309])\n",
      "===============================================\n",
      "['1909', 'university', 'institution']\n",
      "['schools', 'school', 'glenwood']\n",
      "Got the keywords in 0.6702 seconds\n",
      "Got the embeddings and comparisons in 0.0003 seconds\n",
      "Coherence Map: [['oklahoma', 'historic', 'dallas'], ['racial', 'oklahoma', 'population'], ['city', 'racial', 'town'], ['university', 'city', 'institution'], ['schools', 'university', 'school']], KW Curr: ['1909', 'university', 'institution']\n",
      "weighted: tensor([0.3806])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3806])\n",
      "===============================================\n",
      "['schools', 'school', 'glenwood']\n",
      "['pontotoc', 'tech', 'technology']\n",
      "Got the keywords in 0.1916 seconds\n",
      "Got the embeddings and comparisons in 0.0002 seconds\n",
      "Coherence Map: [['oklahoma', 'historic', 'dallas'], ['racial', 'oklahoma', 'population'], ['city', 'racial', 'town'], ['university', 'city', 'institution'], ['schools', 'university', 'school'], ['pontotoc', 'schools', 'tech']], KW Curr: ['schools', 'school', 'glenwood']\n",
      "weighted: tensor([0.5368])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.5368])\n",
      "===============================================\n",
      "['pontotoc', 'tech', 'technology']\n",
      "['2006', 'crime', 'guilt']\n",
      "Got the keywords in 0.4300 seconds\n",
      "Got the embeddings and comparisons in 0.0037 seconds\n",
      "Coherence Map: [['oklahoma', 'historic', 'dallas'], ['racial', 'oklahoma', 'population'], ['city', 'racial', 'town'], ['university', 'city', 'institution'], ['schools', 'university', 'school'], ['pontotoc', 'schools', 'tech'], ['crime', 'pontotoc', 'guilt']], KW Curr: ['pontotoc', 'tech', 'technology']\n",
      "weighted: tensor([0.4320])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.4320])\n",
      "===============================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2006', 'crime', 'guilt']\n",
      "['1885', '1889', 'water']\n",
      "Got the keywords in 1.1601 seconds\n",
      "Got the embeddings and comparisons in 0.0011 seconds\n",
      "Coherence Map: [['oklahoma', 'historic', 'dallas'], ['racial', 'oklahoma', 'population'], ['city', 'racial', 'town'], ['university', 'city', 'institution'], ['schools', 'university', 'school'], ['pontotoc', 'schools', 'tech'], ['crime', 'pontotoc', 'guilt'], ['water', 'crime', 'water']], KW Curr: ['2006', 'crime', 'guilt']\n",
      "weighted: tensor([0.4488])\n",
      "Label: 1, Prediction: 0, logit: tensor([0.4488])\n",
      "===============================================\n",
      "['1885', '1889', 'water']\n",
      "['1907', '1903', '1922']\n",
      "Got the keywords in 1.2390 seconds\n",
      "Got the embeddings and comparisons in 0.0000 seconds\n",
      "Coherence Map: [['oklahoma', 'historic', 'dallas'], ['racial', 'oklahoma', 'population'], ['city', 'racial', 'town'], ['university', 'city', 'institution'], ['schools', 'university', 'school'], ['pontotoc', 'schools', 'tech'], ['crime', 'pontotoc', 'guilt'], ['water', 'crime', 'water'], []], KW Curr: ['1885', '1889', 'water']\n",
      "weighted: tensor([0.4816])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.4816])\n",
      "===============================================\n",
      "['1907', '1903', '1922']\n",
      "['winter', 'cold', 'comodoro']\n",
      "Got the keywords in 0.9769 seconds\n",
      "Got the embeddings and comparisons in 0.0000 seconds\n",
      "Coherence Map: [['oklahoma', 'historic', 'dallas'], ['racial', 'oklahoma', 'population'], ['city', 'racial', 'town'], ['university', 'city', 'institution'], ['schools', 'university', 'school'], ['pontotoc', 'schools', 'tech'], ['crime', 'pontotoc', 'guilt'], ['water', 'crime', 'water'], [], []], KW Curr: ['1907', '1903', '1922']\n",
      "weighted: tensor([0.3860])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3860])\n",
      "===============================================\n",
      "['winter', 'cold', 'comodoro']\n",
      "['rivadavia', 'population', '1991']\n",
      "Got the keywords in 0.7998 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['oklahoma', 'historic', 'dallas'], ['racial', 'oklahoma', 'population'], ['city', 'racial', 'town'], ['university', 'city', 'institution'], ['schools', 'university', 'school'], ['pontotoc', 'schools', 'tech'], ['crime', 'pontotoc', 'guilt'], ['water', 'crime', 'water'], [], [], ['rivadavia', 'winter', 'population']], KW Curr: ['winter', 'cold', 'comodoro']\n",
      "weighted: tensor([0.3812])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3812])\n",
      "===============================================\n",
      "['rivadavia', 'population', '1991']\n",
      "['industrial', 'northern', 'downtown']\n",
      "Got the keywords in 0.9470 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['oklahoma', 'historic', 'dallas'], ['racial', 'oklahoma', 'population'], ['city', 'racial', 'town'], ['university', 'city', 'institution'], ['schools', 'university', 'school'], ['pontotoc', 'schools', 'tech'], ['crime', 'pontotoc', 'guilt'], ['water', 'crime', 'water'], [], [], ['rivadavia', 'winter', 'population'], ['industrial', 'rivadavia', 'northern']], KW Curr: ['rivadavia', 'population', '1991']\n",
      "weighted: tensor([0.5200])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.5200])\n",
      "===============================================\n",
      "['industrial', 'northern', 'downtown']\n",
      "['industrial', 'production', 'city']\n",
      "Got the keywords in 0.8847 seconds\n",
      "Got the embeddings and comparisons in 0.0006 seconds\n",
      "Coherence Map: [['oklahoma', 'historic', 'dallas'], ['racial', 'oklahoma', 'population'], ['city', 'racial', 'town'], ['university', 'city', 'institution'], ['schools', 'university', 'school'], ['pontotoc', 'schools', 'tech'], ['crime', 'pontotoc', 'guilt'], ['water', 'crime', 'water'], [], [], ['rivadavia', 'winter', 'population'], ['industrial', 'rivadavia', 'northern'], ['industrial', 'industrial', 'production']], KW Curr: ['industrial', 'northern', 'downtown']\n",
      "weighted: tensor([0.3697])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3697])\n",
      "===============================================\n",
      "['industrial', 'production', 'city']\n",
      "['1907', '1986', '1933']\n",
      "Got the keywords in 0.8687 seconds\n",
      "Got the embeddings and comparisons in 0.0000 seconds\n",
      "Coherence Map: [['oklahoma', 'historic', 'dallas'], ['racial', 'oklahoma', 'population'], ['city', 'racial', 'town'], ['university', 'city', 'institution'], ['schools', 'university', 'school'], ['pontotoc', 'schools', 'tech'], ['crime', 'pontotoc', 'guilt'], ['water', 'crime', 'water'], [], [], ['rivadavia', 'winter', 'population'], ['industrial', 'rivadavia', 'northern'], ['industrial', 'industrial', 'production'], []], KW Curr: ['industrial', 'production', 'city']\n",
      "weighted: tensor([0.3066])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.3066])\n",
      "===============================================\n",
      "['1907', '1986', '1933']\n",
      "['rivadavia', 'create', 'argentina']\n",
      "Got the keywords in 0.8759 seconds\n",
      "Got the embeddings and comparisons in 0.0000 seconds\n",
      "Coherence Map: [[]], KW Curr: ['1907', '1986', '1933']\n",
      "weighted: tensor([0.3688])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3688])\n",
      "===============================================\n",
      "['rivadavia', 'create', 'argentina']\n",
      "['cartography', 'geographic', 'córdova']\n",
      "Got the keywords in 0.6848 seconds\n",
      "Got the embeddings and comparisons in 0.0002 seconds\n",
      "Coherence Map: [[], ['cartography', 'rivadavia', 'geographic']], KW Curr: ['rivadavia', 'create', 'argentina']\n",
      "weighted: tensor([0.3891])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3891])\n",
      "===============================================\n",
      "['cartography', 'geographic', 'córdova']\n",
      "['1926', '1908', '1928']\n",
      "Got the keywords in 0.6903 seconds\n",
      "Got the embeddings and comparisons in 0.0000 seconds\n",
      "Coherence Map: [[], ['cartography', 'rivadavia', 'geographic'], []], KW Curr: ['cartography', 'geographic', 'córdova']\n",
      "weighted: tensor([0.4549])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.4549])\n",
      "===============================================\n",
      "['1926', '1908', '1928']\n",
      "['ships', 'cordova', 'shipyard']\n",
      "Got the keywords in 0.7968 seconds\n",
      "Got the embeddings and comparisons in 0.0000 seconds\n",
      "Coherence Map: [[], ['cartography', 'rivadavia', 'geographic'], [], []], KW Curr: ['1926', '1908', '1928']\n",
      "weighted: tensor([0.3854])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3854])\n",
      "===============================================\n",
      "['ships', 'cordova', 'shipyard']\n",
      "['concrete', '228', 'bca']\n",
      "Got the keywords in 0.8335 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [[], ['cartography', 'rivadavia', 'geographic'], [], [], ['concrete', 'ships', 'bca']], KW Curr: ['ships', 'cordova', 'shipyard']\n",
      "weighted: tensor([0.5134])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.5134])\n",
      "===============================================\n",
      "['concrete', '228', 'bca']\n",
      "['rivadavia', 'capacity', 'generators']\n",
      "Got the keywords in 0.6067 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [[], ['cartography', 'rivadavia', 'geographic'], [], [], ['concrete', 'ships', 'bca'], ['rivadavia', 'concrete', 'capacity']], KW Curr: ['concrete', '228', 'bca']\n",
      "weighted: tensor([0.4604])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.4604])\n",
      "===============================================\n",
      "['rivadavia', 'capacity', 'generators']\n",
      "['deportiva', 'rivadavia', 'sports']\n",
      "Got the keywords in 0.7265 seconds\n",
      "Got the embeddings and comparisons in 0.0006 seconds\n",
      "Coherence Map: [[], ['cartography', 'rivadavia', 'geographic'], [], [], ['concrete', 'ships', 'bca'], ['rivadavia', 'concrete', 'capacity'], ['deportiva', 'rivadavia', 'rivadavia']], KW Curr: ['rivadavia', 'capacity', 'generators']\n",
      "weighted: tensor([0.4088])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.4088])\n",
      "===============================================\n",
      "['deportiva', 'rivadavia', 'sports']\n",
      "['1939', '1941', '1943']\n",
      "Got the keywords in 0.8790 seconds\n",
      "Got the embeddings and comparisons in 0.0000 seconds\n",
      "Coherence Map: [[], ['cartography', 'rivadavia', 'geographic'], [], [], ['concrete', 'ships', 'bca'], ['rivadavia', 'concrete', 'capacity'], ['deportiva', 'rivadavia', 'rivadavia'], []], KW Curr: ['deportiva', 'rivadavia', 'sports']\n",
      "weighted: tensor([0.3422])\n",
      "Label: 1, Prediction: 1, logit: tensor([0.3422])\n",
      "===============================================\n",
      "['1939', '1941', '1943']\n",
      "['1878', '1894', 'nashville']\n",
      "Got the keywords in 0.7222 seconds\n",
      "Got the embeddings and comparisons in 0.0000 seconds\n",
      "Coherence Map: [[]], KW Curr: ['1939', '1941', '1943']\n",
      "weighted: tensor([0.3599])\n",
      "Label: 1, Prediction: 0, logit: tensor([0.3599])\n",
      "===============================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1878', '1894', 'nashville']\n",
      "['hohenwald', '5520', '5479']\n",
      "Got the keywords in 0.5714 seconds\n",
      "Got the embeddings and comparisons in 0.0002 seconds\n",
      "Coherence Map: [[], []], KW Curr: ['1878', '1894', 'nashville']\n",
      "weighted: tensor([0.3154])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.3154])\n",
      "===============================================\n",
      "['hohenwald', '5520', '5479']\n",
      "['population', 'racial', 'households']\n",
      "Got the keywords in 0.4993 seconds\n",
      "Got the embeddings and comparisons in 0.0101 seconds\n",
      "Coherence Map: [['population', 'hohenwald', 'racial']], KW Curr: ['hohenwald', '5520', '5479']\n",
      "weighted: tensor([0.3703])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3703])\n",
      "===============================================\n",
      "['population', 'racial', 'households']\n",
      "['279574', 'water', 'states']\n",
      "Got the keywords in 0.8482 seconds\n",
      "Got the embeddings and comparisons in 0.0008 seconds\n",
      "Coherence Map: [['population', 'hohenwald', 'racial'], ['water', 'population', 'states']], KW Curr: ['population', 'racial', 'households']\n",
      "weighted: tensor([0.4863])\n",
      "Label: 1, Prediction: 0, logit: tensor([0.4863])\n",
      "===============================================\n",
      "['279574', 'water', 'states']\n",
      "['population', 'households', 'householder']\n",
      "Got the keywords in 0.8571 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['population', 'hohenwald', 'racial'], ['water', 'population', 'states'], ['population', 'water', 'households']], KW Curr: ['279574', 'water', 'states']\n",
      "weighted: tensor([0.5557])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.5557])\n",
      "===============================================\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 770;\n",
       "                var nbb_unformatted_code = \"start = 75\\nnum_samples = 50\\nmax_tokens = 256  # want to keep this under 512\\nmax_str_length = 30\\n\\ntrue_labels = text_labels[start : start + num_samples]\\n\\npredictions = coherence_tester(\\n    text_data[start : start + num_samples],\\n    true_labels,\\n    max_tokens=max_tokens,\\n    max_str_length=max_str_length,\\n)\";\n",
       "                var nbb_formatted_code = \"start = 75\\nnum_samples = 50\\nmax_tokens = 256  # want to keep this under 512\\nmax_str_length = 30\\n\\ntrue_labels = text_labels[start : start + num_samples]\\n\\npredictions = coherence_tester(\\n    text_data[start : start + num_samples],\\n    true_labels,\\n    max_tokens=max_tokens,\\n    max_str_length=max_str_length,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start = 75\n",
    "num_samples = 50\n",
    "max_tokens = 256  # want to keep this under 512\n",
    "max_str_length = 30\n",
    "\n",
    "true_labels = text_labels[start : start + num_samples]\n",
    "\n",
    "predictions = coherence_tester(\n",
    "    text_data[start : start + num_samples],\n",
    "    true_labels,\n",
    "    max_tokens=max_tokens,\n",
    "    max_str_length=max_str_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "id": "d3a93727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0]\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 765;\n",
       "                var nbb_unformatted_code = \"print([x[1] for x in predictions])\\nprint(true_labels)\";\n",
       "                var nbb_formatted_code = \"print([x[1] for x in predictions])\\nprint(true_labels)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print([x[1] for x in predictions])\n",
    "print(true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "id": "61e7863f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 766;\n",
       "                var nbb_unformatted_code = \"pred_string = \\\"\\\".join(str([x[1] for x in predictions]))\\ntrue_string = \\\"\\\".join(str(true_labels))\";\n",
       "                var nbb_formatted_code = \"pred_string = \\\"\\\".join(str([x[1] for x in predictions]))\\ntrue_string = \\\"\\\".join(str(true_labels))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_string = \"\".join(str([x[1] for x in predictions]))\n",
    "true_string = \"\".join(str(true_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "id": "19af16ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 767;\n",
       "                var nbb_unformatted_code = \"avg_k = len(true_labels) // (true_labels.count(1) + 1)  # get avg segment size\";\n",
       "                var nbb_formatted_code = \"avg_k = len(true_labels) // (true_labels.count(1) + 1)  # get avg segment size\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "avg_k = len(true_labels) // (true_labels.count(1) + 1)  # get avg segment size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "id": "db43c420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 4\n",
      "wd = 0.3741496598639456\n",
      "pk = 0.3673469387755102\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 768;\n",
       "                var nbb_unformatted_code = \"wd_score = windowdiff(pred_string, true_string, avg_k)\\npk_score = pk(pred_string, true_string, avg_k)\\n\\nprint(f\\\"k = {avg_k}\\\")\\nprint(f\\\"wd = {wd_score}\\\")\\nprint(f\\\"pk = {pk_score}\\\")\";\n",
       "                var nbb_formatted_code = \"wd_score = windowdiff(pred_string, true_string, avg_k)\\npk_score = pk(pred_string, true_string, avg_k)\\n\\nprint(f\\\"k = {avg_k}\\\")\\nprint(f\\\"wd = {wd_score}\\\")\\nprint(f\\\"pk = {pk_score}\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wd_score = windowdiff(pred_string, true_string, avg_k)\n",
    "pk_score = pk(pred_string, true_string, avg_k)\n",
    "\n",
    "print(f\"k = {avg_k}\")\n",
    "print(f\"wd = {wd_score}\")\n",
    "print(f\"pk = {pk_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb16194",
   "metadata": {},
   "source": [
    "## Prediction Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "id": "af208503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 755;\n",
       "                var nbb_unformatted_code = \"pred_thresh = 0.33\";\n",
       "                var nbb_formatted_code = \"pred_thresh = 0.33\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_thresh = 0.33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "id": "e9cebcd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 756;\n",
       "                var nbb_unformatted_code = \"modified_predictions = [\\n    1 if x < pred_thresh else 0 for x in [x[0] for x in predictions]\\n]\\n\\npred_string = \\\"\\\".join(str(modified_predictions))\\ntrue_string = \\\"\\\".join(str(true_labels))\\n\\navg_k = len(true_labels) // (true_labels.count(1) + 1)  # get avg segment size\";\n",
       "                var nbb_formatted_code = \"modified_predictions = [\\n    1 if x < pred_thresh else 0 for x in [x[0] for x in predictions]\\n]\\n\\npred_string = \\\"\\\".join(str(modified_predictions))\\ntrue_string = \\\"\\\".join(str(true_labels))\\n\\navg_k = len(true_labels) // (true_labels.count(1) + 1)  # get avg segment size\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "modified_predictions = [\n",
    "    1 if x < pred_thresh else 0 for x in [x[0] for x in predictions]\n",
    "]\n",
    "\n",
    "pred_string = \"\".join(str(modified_predictions))\n",
    "true_string = \"\".join(str(true_labels))\n",
    "\n",
    "avg_k = len(true_labels) // (true_labels.count(1) + 1)  # get avg segment size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "id": "6a2f65af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0]\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 757;\n",
       "                var nbb_unformatted_code = \"print(pred_string)\\nprint(true_string)\";\n",
       "                var nbb_formatted_code = \"print(pred_string)\\nprint(true_string)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(pred_string)\n",
    "print(true_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "id": "b9dbb752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 4\n",
      "wd = 0.36054421768707484\n",
      "pk = 0.35374149659863946\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 758;\n",
       "                var nbb_unformatted_code = \"wd_score = windowdiff(pred_string, true_string, avg_k)\\npk_score = pk(pred_string, true_string, avg_k)\\n\\nprint(f\\\"k = {avg_k}\\\")\\nprint(f\\\"wd = {wd_score}\\\")\\nprint(f\\\"pk = {pk_score}\\\")\";\n",
       "                var nbb_formatted_code = \"wd_score = windowdiff(pred_string, true_string, avg_k)\\npk_score = pk(pred_string, true_string, avg_k)\\n\\nprint(f\\\"k = {avg_k}\\\")\\nprint(f\\\"wd = {wd_score}\\\")\\nprint(f\\\"pk = {pk_score}\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wd_score = windowdiff(pred_string, true_string, avg_k)\n",
    "pk_score = pk(pred_string, true_string, avg_k)\n",
    "\n",
    "print(f\"k = {avg_k}\")\n",
    "print(f\"wd = {wd_score}\")\n",
    "print(f\"pk = {pk_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34585627",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "462a8434",
   "metadata": {},
   "source": [
    "## KeyBERT Embedding Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "d15e7648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 172;\n",
       "                var nbb_unformatted_code = \"curr = 230\\nprev = curr - 1\";\n",
       "                var nbb_formatted_code = \"curr = 230\\nprev = curr - 1\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "curr = 230\n",
    "prev = curr - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "8c6434ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got the keywords in 0.6567 seconds\n",
      "Got the embeddings and comparisons in 0.0007 seconds\n",
      "['cantonese', 'languages', 'vietnamese', 'communes']\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 205;\n",
       "                var nbb_unformatted_code = \"cohesion = coherence.get_coherence(\\n    [text_data[curr], text_data[prev]], coherence_threshold=0.25\\n)\\nprint([k[0] for k in cohesion])\";\n",
       "                var nbb_formatted_code = \"cohesion = coherence.get_coherence(\\n    [text_data[curr], text_data[prev]], coherence_threshold=0.25\\n)\\nprint([k[0] for k in cohesion])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cohesion = coherence.get_coherence(\n",
    "    [text_data[curr], text_data[prev]], coherence_threshold=0.25\n",
    ")\n",
    "print([k[0] for k in cohesion])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "357c0021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 206;\n",
       "                var nbb_unformatted_code = \"# get the keywords for the current sentences\\nkeywords_current = keywords_lib.get_keywords_with_kb_embeddings(text_data[curr])\\nkeywords_prev = keywords_lib.get_keywords_with_kb_embeddings(text_data[prev])\\n\\n# compute the word comparisons between the previous (with the coherence map)\\n# and the current (possibly the first sentence in a new segment)\\nword_comparisons_with_coherence, weights = compare_coherent_words(\\n    [keywords_prev], keywords_current\\n)\";\n",
       "                var nbb_formatted_code = \"# get the keywords for the current sentences\\nkeywords_current = keywords_lib.get_keywords_with_kb_embeddings(text_data[curr])\\nkeywords_prev = keywords_lib.get_keywords_with_kb_embeddings(text_data[prev])\\n\\n# compute the word comparisons between the previous (with the coherence map)\\n# and the current (possibly the first sentence in a new segment)\\nword_comparisons_with_coherence, weights = compare_coherent_words(\\n    [keywords_prev], keywords_current\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get the keywords for the current sentences\n",
    "keywords_current = keywords_lib.get_keywords_with_kb_embeddings(text_data[curr])\n",
    "keywords_prev = keywords_lib.get_keywords_with_kb_embeddings(text_data[prev])\n",
    "\n",
    "# compute the word comparisons between the previous (with the coherence map)\n",
    "# and the current (possibly the first sentence in a new segment)\n",
    "word_comparisons_with_coherence, weights = compare_coherent_words(\n",
    "    [keywords_prev], keywords_current\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "dd52c9d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('township', 0.2304),\n",
       "  ('communes', 0.1857),\n",
       "  ('hải', 0.1399),\n",
       "  ('wards', 0.1397),\n",
       "  ('đông', 0.1224)],\n",
       " [('cantonese', 0.5038),\n",
       "  ('mandarin', 0.464),\n",
       "  ('languages', 0.3483),\n",
       "  ('language', 0.343),\n",
       "  ('vietnamese', 0.3184)])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 207;\n",
       "                var nbb_unformatted_code = \"[(x[0], x[1]) for x in keywords_current], [(x[0], x[1]) for x in keywords_prev]\";\n",
       "                var nbb_formatted_code = \"[(x[0], x[1]) for x in keywords_current], [(x[0], x[1]) for x in keywords_prev]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "[(x[0], x[1]) for x in keywords_current], [(x[0], x[1]) for x in keywords_prev]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5121f953",
   "metadata": {},
   "source": [
    "# KeyBERT Embedding Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "id": "559ab602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 679;\n",
       "                var nbb_unformatted_code = \"docs = [\\n        \\\"Hi my name is Devarsh\\\",\\n        \\\"Devarsh likes to play Basketball.\\\",\\n    \\\"I love to watch Cricket.\\\",\\n        \\\"I am a strong programmer. And my name is Devarsh\\\",\\n]\";\n",
       "                var nbb_formatted_code = \"docs = [\\n    \\\"Hi my name is Devarsh\\\",\\n    \\\"Devarsh likes to play Basketball.\\\",\\n    \\\"I love to watch Cricket.\\\",\\n    \\\"I am a strong programmer. And my name is Devarsh\\\",\\n]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "docs = [\n",
    "    \"Hi my name is Devarsh\",\n",
    "    \"Devarsh likes to play Basketball.\",\n",
    "    \"I love to watch Cricket.\",\n",
    "    \"I am a strong programmer. And my name is Devarsh\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "id": "00458200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 680;\n",
       "                var nbb_unformatted_code = \"from keybert import KeyBERT\\n\\nkw_model = KeyBERT()\\ndoc_embeddings, word_embeddings = kw_model.extract_embeddings(\\n    docs, min_df=1, stop_words=\\\"english\\\"\\n)\\nkeywords = kw_model.extract_keywords(\\n    docs,\\n    min_df=1,\\n    stop_words=\\\"english\\\",\\n    doc_embeddings=doc_embeddings,\\n    word_embeddings=word_embeddings,\\n)\";\n",
       "                var nbb_formatted_code = \"from keybert import KeyBERT\\n\\nkw_model = KeyBERT()\\ndoc_embeddings, word_embeddings = kw_model.extract_embeddings(\\n    docs, min_df=1, stop_words=\\\"english\\\"\\n)\\nkeywords = kw_model.extract_keywords(\\n    docs,\\n    min_df=1,\\n    stop_words=\\\"english\\\",\\n    doc_embeddings=doc_embeddings,\\n    word_embeddings=word_embeddings,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keybert import KeyBERT\n",
    "\n",
    "kw_model = KeyBERT()\n",
    "doc_embeddings, word_embeddings = kw_model.extract_embeddings(\n",
    "    docs, min_df=1, stop_words=\"english\"\n",
    ")\n",
    "keywords = kw_model.extract_keywords(\n",
    "    docs,\n",
    "    min_df=1,\n",
    "    stop_words=\"english\",\n",
    "    doc_embeddings=doc_embeddings,\n",
    "    word_embeddings=word_embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "id": "7d30bae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 681,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 681;\n",
       "                var nbb_unformatted_code = \"len(doc_embeddings)\";\n",
       "                var nbb_formatted_code = \"len(doc_embeddings)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(doc_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "id": "018ee52e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 682,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 682;\n",
       "                var nbb_unformatted_code = \"len(word_embeddings)\";\n",
       "                var nbb_formatted_code = \"len(word_embeddings)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "id": "80cbdc77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('devarsh', 0.6267), ('hi', 0.5216)],\n",
       " [('devarsh', 0.6549),\n",
       "  ('basketball', 0.5558),\n",
       "  ('play', 0.3787),\n",
       "  ('likes', 0.2284)],\n",
       " [('cricket', 0.7118), ('watch', 0.3656), ('love', 0.307)],\n",
       " [('programmer', 0.5942), ('devarsh', 0.5528), ('strong', 0.3452)]]"
      ]
     },
     "execution_count": 683,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 683;\n",
       "                var nbb_unformatted_code = \"keywords\";\n",
       "                var nbb_formatted_code = \"keywords\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "id": "fd1ac50f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 701;\n",
       "                var nbb_unformatted_code = \"kw_model = KeyBERT()\\nimport torch\\n\\n\\ndef get_keywords_with_embeddings_test(\\n    data,\\n) -> list[tuple[str, float, torch.Tensor]]:\\n    doc_embeddings, word_embeddings = kw_model.extract_embeddings(data)\\n\\n    keywords = kw_model.extract_keywords(\\n        data, doc_embeddings=doc_embeddings, word_embeddings=word_embeddings\\n    )\\n\\n    keywords_with_embeddings = []\\n    count = 0\\n    print(len(word_embeddings))\\n    for i, (kw, we) in enumerate(zip(keywords, word_embeddings)):\\n        for j, words in enumerate(kw):\\n            keywords_with_embeddings.append((words[0], words[1], torch.tensor(we)))\\n            count += 1\\n\\n    return keywords_with_embeddings\";\n",
       "                var nbb_formatted_code = \"kw_model = KeyBERT()\\nimport torch\\n\\n\\ndef get_keywords_with_embeddings_test(\\n    data,\\n) -> list[tuple[str, float, torch.Tensor]]:\\n    doc_embeddings, word_embeddings = kw_model.extract_embeddings(data)\\n\\n    keywords = kw_model.extract_keywords(\\n        data, doc_embeddings=doc_embeddings, word_embeddings=word_embeddings\\n    )\\n\\n    keywords_with_embeddings = []\\n    count = 0\\n    print(len(word_embeddings))\\n    for i, (kw, we) in enumerate(zip(keywords, word_embeddings)):\\n        for j, words in enumerate(kw):\\n            keywords_with_embeddings.append((words[0], words[1], torch.tensor(we)))\\n            count += 1\\n\\n    return keywords_with_embeddings\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kw_model = KeyBERT()\n",
    "import torch\n",
    "\n",
    "\n",
    "def get_keywords_with_embeddings_test(\n",
    "    data,\n",
    ") -> list[tuple[str, float, torch.Tensor]]:\n",
    "    doc_embeddings, word_embeddings = kw_model.extract_embeddings(data)\n",
    "\n",
    "    keywords = kw_model.extract_keywords(\n",
    "        data, doc_embeddings=doc_embeddings, word_embeddings=word_embeddings\n",
    "    )\n",
    "\n",
    "    keywords_with_embeddings = []\n",
    "    count = 0\n",
    "    print(len(word_embeddings))\n",
    "    for i, (kw, we) in enumerate(zip(keywords, word_embeddings)):\n",
    "        for j, words in enumerate(kw):\n",
    "            keywords_with_embeddings.append((words[0], words[1], torch.tensor(we)))\n",
    "            count += 1\n",
    "\n",
    "    return keywords_with_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "id": "d1bbf3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 702;\n",
       "                var nbb_unformatted_code = \"embeddings = get_keywords_with_embeddings_test(docs)\";\n",
       "                var nbb_formatted_code = \"embeddings = get_keywords_with_embeddings_test(docs)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings = get_keywords_with_embeddings_test(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "id": "f1ea7b6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 703,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 703;\n",
       "                var nbb_unformatted_code = \"len(embeddings)\";\n",
       "                var nbb_formatted_code = \"len(embeddings)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9883ef15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
