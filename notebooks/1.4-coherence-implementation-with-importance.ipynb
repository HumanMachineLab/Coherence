{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dc7697a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"# Run if working locally\\n%load_ext autoreload\\n%autoreload 2\\n%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"# Run if working locally\\n%load_ext autoreload\\n%autoreload 2\\n%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run if working locally\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88ee84b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"import sqlite3\\nfrom sqlite3 import Error\\nimport pickle\\nimport os, sys\\nimport config\\n\\nconfig.root_path = os.path.abspath(os.path.join(os.getcwd(), \\\"..\\\"))\\nsys.path.insert(0, config.root_path)\\n\\nfrom src.dataset.dataset import RawData\\nfrom src.dataset.wikisection_preprocessing import (\\n    tokenize,\\n    clean_sentence,\\n    preprocess_text_segmentation,\\n    format_data_for_db_insertion,\\n)\\nfrom src.dataset.utils import truncate_by_token\\nfrom db.dbv2 import Table, AugmentedTable, TrainTestTable\\nimport pprint\\n\\nfrom utils.metrics import windowdiff, pk\\n\\nfrom src.bertkeywords.src.similarities import Embedding, Similarities\\nfrom src.bertkeywords.src.keywords import Keywords\\nfrom src.encoders.coherence_v2 import Coherence\\nfrom src.dataset.utils import flatten, dedupe_list, truncate_string\";\n",
       "                var nbb_formatted_code = \"import sqlite3\\nfrom sqlite3 import Error\\nimport pickle\\nimport os, sys\\nimport config\\n\\nconfig.root_path = os.path.abspath(os.path.join(os.getcwd(), \\\"..\\\"))\\nsys.path.insert(0, config.root_path)\\n\\nfrom src.dataset.dataset import RawData\\nfrom src.dataset.wikisection_preprocessing import (\\n    tokenize,\\n    clean_sentence,\\n    preprocess_text_segmentation,\\n    format_data_for_db_insertion,\\n)\\nfrom src.dataset.utils import truncate_by_token\\nfrom db.dbv2 import Table, AugmentedTable, TrainTestTable\\nimport pprint\\n\\nfrom utils.metrics import windowdiff, pk\\n\\nfrom src.bertkeywords.src.similarities import Embedding, Similarities\\nfrom src.bertkeywords.src.keywords import Keywords\\nfrom src.encoders.coherence_v2 import Coherence\\nfrom src.dataset.utils import flatten, dedupe_list, truncate_string\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sqlite3\n",
    "from sqlite3 import Error\n",
    "import pickle\n",
    "import os, sys\n",
    "import config\n",
    "\n",
    "config.root_path = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.insert(0, config.root_path)\n",
    "\n",
    "from src.dataset.dataset import RawData\n",
    "from src.dataset.wikisection_preprocessing import (\n",
    "    tokenize,\n",
    "    clean_sentence,\n",
    "    preprocess_text_segmentation,\n",
    "    format_data_for_db_insertion,\n",
    ")\n",
    "from src.dataset.utils import truncate_by_token\n",
    "from db.dbv2 import Table, AugmentedTable, TrainTestTable\n",
    "import pprint\n",
    "\n",
    "from utils.metrics import windowdiff, pk\n",
    "\n",
    "from src.bertkeywords.src.similarities import Embedding, Similarities\n",
    "from src.bertkeywords.src.keywords import Keywords\n",
    "from src.encoders.coherence_v2 import Coherence\n",
    "from src.dataset.utils import flatten, dedupe_list, truncate_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e4dcd8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "No sentence-transformers model found with name /Users/amitmaraj/.cache/torch/sentence_transformers/bert-base-uncased. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /Users/amitmaraj/.cache/torch/sentence_transformers/bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"# initialize the coherence library\\nmax_words_per_step = 3\\ncoherence = Coherence(max_words_per_step=max_words_per_step)\";\n",
       "                var nbb_formatted_code = \"# initialize the coherence library\\nmax_words_per_step = 3\\ncoherence = Coherence(max_words_per_step=max_words_per_step)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialize the coherence library\n",
    "max_words_per_step = 3\n",
    "coherence = Coherence(max_words_per_step=max_words_per_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "226021b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "No sentence-transformers model found with name /Users/amitmaraj/.cache/torch/sentence_transformers/bert-base-uncased. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /Users/amitmaraj/.cache/torch/sentence_transformers/bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"# initialize the keywords and embeddings library\\npp = pprint.PrettyPrinter(indent=4)\\nsimilarities_lib = Similarities(\\\"bert-base-uncased\\\")\\nkeywords_lib = Keywords(similarities_lib.model, similarities_lib.tokenizer)\\nembedding_lib = Embedding(similarities_lib.model, similarities_lib.tokenizer)\";\n",
       "                var nbb_formatted_code = \"# initialize the keywords and embeddings library\\npp = pprint.PrettyPrinter(indent=4)\\nsimilarities_lib = Similarities(\\\"bert-base-uncased\\\")\\nkeywords_lib = Keywords(similarities_lib.model, similarities_lib.tokenizer)\\nembedding_lib = Embedding(similarities_lib.model, similarities_lib.tokenizer)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialize the keywords and embeddings library\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "similarities_lib = Similarities(\"bert-base-uncased\")\n",
    "keywords_lib = Keywords(similarities_lib.model, similarities_lib.tokenizer)\n",
    "embedding_lib = Embedding(similarities_lib.model, similarities_lib.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bb2458b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"dataset_type = \\\"city\\\"\\ntable = Table(dataset_type)\\naugmented_table = AugmentedTable(dataset_type)\\ntrain_test_table = TrainTestTable(dataset_type)\";\n",
       "                var nbb_formatted_code = \"dataset_type = \\\"city\\\"\\ntable = Table(dataset_type)\\naugmented_table = AugmentedTable(dataset_type)\\ntrain_test_table = TrainTestTable(dataset_type)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_type = \"city\"\n",
    "table = Table(dataset_type)\n",
    "augmented_table = AugmentedTable(dataset_type)\n",
    "train_test_table = TrainTestTable(dataset_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfd59c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"data = table.get_all()\\n\\ntext_data = [x[1] for x in data]\\ntext_labels = [x[2] for x in data]\";\n",
       "                var nbb_formatted_code = \"data = table.get_all()\\n\\ntext_data = [x[1] for x in data]\\ntext_labels = [x[2] for x in data]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = table.get_all()\n",
    "\n",
    "text_data = [x[1] for x in data]\n",
    "text_labels = [x[2] for x in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8d649bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " ['The remains of an ancient village prove that the town was used for hydrotherapy in ancient times. There are nine hydrothermal sources. There are plenty of mineral springs.\\n',\n",
       "  'The Banya Palace summerhouse of Boris III with its picturesque yard-garden, called by the locals “The Palace,” is in the town of Banya. In 1927 Tzar Boris III took a cure for rheumatism in the country house of the manufacturer I. Bagarov. Pleased at his stay, he decided to build up an estate. It was in a courtyard with luxurious verdure and was finished in 1929.\\n',\n",
       "  'Harvard was founded in 1871 when the railroad was extended to that point. It was named after Harvard University, in Massachusetts.\\n',\n",
       "  'Harvard is located at (40.620276, -98.096554).\\nAccording to the United States Census Bureau, the city has a total area of , all of it land.\\n',\n",
       "  'As of the census of 2010, there were 1,013 people, 372 households, and 248 families residing in the city. The population density was . There were 453 housing units at an average density of . The racial makeup of the city was 78.9% White, 0.2% African American, 0.6% Native American, 0.3% Asian, 15.8% from other races, and 4.2% from two or more races. Hispanic or Latino of any race were 23.7% of the population.\\nThere were 372 households of which 37.4% had children under the age of 18 living with them, 49.2% were married couples living together, 10.8% had a female householder with no husband present, 6.7% had a male householder with no wife present, and 33.3% were non-families. 29.8% of all households were made up of individuals and 13.7% had someone living alone who was 65 years of age or older. The average household size was 2.63 and the average family size was 3.23.\\nThe median age in the city was 36.9 years. 29.3% of residents were under the age of 18; 7.9% were between the ages of 18 and 24; 20.5% were from 25 to 44; 27.4% were from 45 to 64; and 14.7% were 65 years of age or older. The gender makeup of the city was 49.3% male and 50.7% female.\\n',\n",
       "  'As of the census of 2000, there were 998 people, 385 households, and 259 families residing in the city. The population density was 1,558.9 people per square mile (602.1/km²). There were 450 housing units at an average density of 702.9 per square mile (271.5/km²). The racial makeup of the city was 95.19% White, 0.10% African American, 0.70% Native American, 0.10% Asian, 3.21% from other races, and 0.70% from two or more races. Hispanic or Latino of any race were 12.32% of the population.\\nThere were 385 households out of which 31.7% had children under the age of 18 living with them, 56.6% were married couples living together, 7.8% had a female householder with no husband present, and 32.7% were non-families. 28.6% of all households were made up of individuals and 13.2% had someone living alone who was 65 years of age or older. The average household size was 2.51 and the average family size was 3.08.\\nIn the city, the population was spread out with 28.3% under the age of 18, 7.3% from 18 to 24, 23.8% from 25 to 44, 22.3% from 45 to 64, and 18.2% who were 65 years of age or older. The median age was 39 years. For every 100 females there were 90.5 males. For every 100 females age 18 and over, there were 84.5 males.\\nThe median income for a household in the city was $29,350, and the median income for a family was $32,031. Males had a median income of $26,667 versus $17,159 for females. The per capita income for the city was $13,077. About 12.5% of families and 15.2% of the population were below the poverty line, including 16.4% of those under age 18 and 10.7% of those age 65 or over.\\n',\n",
       "  \"In return for a promise to pay off debts owed to the United States by the Bauan chieftain, Seru Epenisa Cakobau, the Australian-based Polynesia Company was granted 5000 km² of land, 575 km² of it near what was then the village of Suva, in 1868. The original intention was to develop a cotton farming industry, but the land and climate proved unsuitable. \\nFollowing the cessation of the Fiji Islands by the United Kingdom in 1874, the colonial authorities decided to move the capital to Suva from Levuka, Ovalau, Lomaiviti in 1877, as Levuka's location, between a steep mountain and the sea, made any expansion of the town impractical. The transfer was made official in 1882. Colonel F.E. Pratt of the Royal Engineers was appointed Surveyor-General in 1875 and designed the new capital, assisted by W. Stephens and Colonel R.W. Stewart. \\nFollowing the promulgation of the Municipal Constitution Ordinance of 1909, Suva acquired municipal status in 1910. The town initially comprised one square mile; these boundaries remained intact until 1952 when the Muanikau and Samabula wards were annexed, expanding its territory to 13 square kilometres. In October that year, Suva was proclaimed a city – Fiji's first. Tamavua was subsequently annexed; the most recent extension of the city boundaries has been to incorporate the Cunningham area to the north of the city. Urban sprawl has resulted in a number of suburbs that remain outside of the city limits; together with the city itself, they form a metropolitan area known as the Greater Suva Area. .\\nThe city hosted the 2003 South Pacific Games, being the third time in the event's 40-year history that they had been held in Suva. As part of the hosting of the event, a new gymnasium and indoor sports centre, swimming pool, and stadium; field hockey pitch; and stands were built in the area around Suva, funded by the government and a $16 million People's Republic of China aid package.\\n\",\n",
       "  \"Suva is the capital of Fiji and is a harbour city built on a peninsula reaching out into the sea. It has a mix of modern buildings and traditional colonial architecture.\\nThe city is perched on a hilly peninsula between Laucala Bay and Suva Harbour in the southeast corner of Viti Levu. The mountains north and west catch the southeast trade winds, producing moist conditions year round.\\nSuva is the commercial and political centre of Fiji, though not necessarily the cultural centre. It is Fiji's main port city.\\nAlthough Suva is on a peninsula, and almost surrounded by sea, the nearest beach is 40 kilometres (25 mi) away at Pacific Harbour and the nearby coast is lined by mangroves. A significant part of the city centre, including the old Parliament buildings, is built on reclaimed mangrove swamp.\\n\",\n",
       "  \"The Central Business District encompasses an area known as the Central Ward; one of Suva's six wards, Central occupies close to the whole south-western side of the Suva Peninsula.\\n\",\n",
       "  \"The city's six wards beginning from the city centre, then north, then clockwise rotation.\\n- Central: city centre, CBD, nucleus of the city.\\n- Tamavua: residential and urban area.\\n- Cunningham: semi-urban and residential area.\\n- Nabua: military base, Southern Division Police Headquarters, urban, residential, separate town centre, and industrial zone.\\n- Samabula: urban, residential, separate town centre, university, and large industrial zones.\\n- Muanikau: residential, urban, large sporting venues, university, and recreational areas.\\n\",\n",
       "  'Suva sits in the middle of an urban conurbation that stretches from Lami, to the immediate west of the city, along the Queens Highway and Nasinu town, on its eastern border all the way to the Rewa River, along the Kings Highway. This conurbation, sometimes known as the Suva Urban Complex, continues till Nausori, over the Rewa River. The north of the city to its northeast contains the rainforest park areas of Colo-i-Suva and Sawani, along the Princes Road, connecting at the Rewa River Bridge. This entire conurbation, is generally referred to by locals as Suva, although it contains four local government areas. In formal reference, this complex has come to be known as the Suva–Nausori Corridor (where Lami is generally excluded) and is the most populous area in Fiji, with over 330,000 people.\\n',\n",
       "  'Suva features a tropical rainforest climate under the Köppen climate classification. The city sees a copious amount of precipitation during the course of the year, with no true dry season due to no month having an average rainfall below . Suva averages of precipitation annually with its driest month, July averaging . In fact, during all 12 months of the year, Suva receives substantial precipitation, such that the term \"fine weather\" in a weather report simply means \"not actually raining\". Like many other cities with a tropical rainforest climate, temperatures are relatively constant throughout the year, with an average high of about and an average low of about .\\nSuva has a markedly higher rainfall than Nadi and the western side of Viti Levu, which is known to Suva citizens as \"the burning west\". The second Governor of Fiji, Sir Arthur Gordon, allegedly remarked that it rained in Suva like he had seen nowhere else before and that there was hardly a day without rain. The most copious rainfall is observed from November to May, while the slightly cooler months from June to October are considerably drier.\\n',\n",
       "  'Suva is a multiracial and multicultural city. Indigenous Fijians and Indo-Fijians, the two principal ethnic groups of Fiji, comprise the bulk of Suva\\'s population, and the city is home to the majority of Fiji\\'s ethnic minority populations, which include Rotumans, Lauans, Rambians, Caucasians (Europeans or Kaivalagi), part-Europeans (of European and Fijian descent), or Kailoma\" and Chinese, amongst others. The most widely spoken language is English, but Fijian, Hindustani, and other languages are also spoken by their respective communities. \\nSuva boasts having representation of all major indigenous Pacific groups, and is sometimes referred to as the “New York of the Pacific”. The city’s reputation as a major economic centre in the region, and also its hosting of the University of the South Pacific’s main campus has led to an influx of Pacific migrants who study, work and live in the city and its boroughs. \\n',\n",
       "  \"Suva has municipal status and is supposed to be governed by a Lord Mayor and a 20-member city council. The Suva City Council is the municipal law-making body of the city of Suva, Fiji's capital. It consists of 20 Councillors, elected for three-year terms from four multi-member constituencies called wards. Councillors, who are elected by residents, landowners, and representatives of corporations owning or occupying rateable property in Suva, elect a Lord Mayor and Deputy Lord Mayor from among their own members; they serve one-year terms and are eligible for re-election.\\nIn 2009, the Military-backed interim government dismissed all municipal governments throughout Fiji and appointed special administrators to run the urban areas. As of 2015, elected municipal government has not been restored. The special administrator of Suva, along with nearby Nasinu, is Chandu Umaria, a former Lord Mayor of Suva.\\n\",\n",
       "  \"A well-known landmark is the Suva City Library or the Carnegie Library, built in 1909 as well as many other colonial buildings.\\nThe government buildings complex sits on what was once the flowing waters of a creek. It was drained in 1935 and over five kilometres of reinforced concrete pilings were driven into the creek bed to support the massive buildings to be erected. After the foundation stone was laid in 1937, the building was completed in 1939; a new wing was completed in 1967. Parliament, however, was moved to a new complex on Ratu Sukuna Road in 1992.\\nGovernment House was formerly the residence of Fiji's colonial Governors and, following independence in 1970, Governors-General. It is now the official residence of Fiji's President. Originally erected in 1882, it had to be rebuilt in 1928, following its destruction by lightning in 1921.\\nThe Suva campus of the University of the South Pacific (USP) occupies what was once a New Zealand military base. It is the largest of the many USP campuses dotted throughout the South Pacific and the largest university in the Pacific Islands outside Hawaii. It offers courses which are internationally recognised and endorsed.\\nThe Fiji Museum, in Thurston Gardens, was founded in 1904 and originally occupied the old town hall, but moved to its present location in 1954. The museum houses the most extensive collection of Fijian artefacts in the world, and is also a research and educational institution, specialising in archaeology, the preservation of Fiji's oral tradition, and the publication of material on Fiji's language and culture.\\nSuva has around 78 parks, these include the new Takashi Suzuki Garden, Apted Park at Suva Point which is a popular spot for viewing sunrise and sunset, Thurston Gardens which was opened in 1913 and has flora from throughout the South Pacific.\\nSuva has many shopping and retail areas, notably Cumming street, which has since colonial times been a vibrant and colourful shopping area. Features of these streets include the original colonial buildings and narrow roads. More modern shopping malls, such as the Suva Central Shopping Mall, Mid-City Mall as well as MHCC are all part of the developments to give the city a modern and sophisticated look.\\nIn December 2009, there was an addition to Suva's skyline with the opening of TappooCity valued at USD25.7 million (FJD50 million) a joint venture six-storey low-rise building project by FNPF & Tappoo Group of Companies as Fiji's (and South Pacific's) largest department mall at present outside Australia & New Zealand.\\nConstruction work began in January 2011 for a FJD30 million mini-mall complex at Grantham Road behind the Sports-City Complex and close proximity to University of the South Pacific, which will house restaurants, retail outlets and cinemas. Although construction was scheduled for end 2011, this complex will now be ready mid-2012.\\n\",\n",
       "  \"Unlike most cities and towns in Fiji and indeed many around the world, Suva did not grow around one industry but has gradually developed as a hub and the largest and most sophisticated city in the Pacific Islands. Fijians of Indian descent have largely shaped the economy of Fiji, contributing immensely to the growth of Suva and its status as the economic and political capital of Fiji. Suva is the commercial center of Fiji with most banks having their Pacific headquarters here, including ANZ and the Westpac Bank. Most national financial institutions, non-governmental organisations and government ministries and departments are headquartered here. At one point both Air Pacific (now called Fiji Airways and Air Fiji were headquartered in Suva.\\nA large part of Fiji's international shipping is conducted at Suva's Kings Wharf as well as docking of international cruise ships, which has led to a growth in Suva's tourism industry. Many services are provided in Suva and is the basis of Suva's industrial and commercial activity.\\nThere are large industrial areas, the largest being at Walu Bay, where there is a saturation of factories and warehouses and import and export companies. This area contains many shipyards for ship building and repairs as well as container yards. There is a brewery and many printeries. Other notable industrial zones are located in Vatuwaqa, Raiwaqa and Laucala Beach.\\nThere is a large commercial and shopping scene in Suva with streets such as Cumming Street and Victoria Parade being popular. There are many shopping complexes to visit and many markets.\\n\",\n",
       "  \"Suva is host to more international and regional intergovernmental agencies and NGOs than any other Pacific Island capital. Some of the bodies with a presence in Suva are:\\n- The TRAFFIC Oceania South Pacific Programme – funded by the UK Foreign and Commonwealth Office, is in Suva, in the offices of the WWF South Pacific Programme. The programme assists in the implementation of CITES and strengthens collaboration with the World Wide Fund for Nature.\\n- The Fiji School of Medicine – which is now classed as a regional agency and a member of the Council of Regional Organisations in the Pacific.\\n- The University of Fiji.\\n- The Fiji School of Nursing.\\n- The University of the South Pacific which operates a campus in Suva as well as at other South Pacific locations.\\n- The Fiji National University which is a major polytechnic in Fiji and caters students from many small Pacific Island nations. It has centres in other Fiji towns of Nadi, Ba and Labasa.\\n- The Fiji College of Advanced Learning.\\n- TPAF (The Training and Productivity Authority of Fiji).\\n- The Secretariat of the Pacific Community (SPC).\\n- The Pacific Islands Forum Secretariat.\\n- The South Pacific Applied Geoscience Commission (SOPAC).\\n- St John's Theological College, Suva.\\n- The Pacific Regional Seminary (PRS).\\n- The Pacific Theological College (PTC).\\n- Femmus School of Hospitality.\\n- Alliance Française.\\n- Greenpeace Pacific.\\n- UNDP Headquarters (Papua New Guinea, Vanuatu, Fiji, Tonga, Samoa, Cook Islands, Palau, Micronesia, Marshall Islands, Tuvalu, Kiribati, Niue, Nauru).\\n- Asian Development Bank Headquarters Pacific\\n- World Bank Headquarters\\n\",\n",
       "  'Suva is the cultural and entertainment capital of Oceania, and is host to many regional, national and local events. The city has very developed and advanced entertainment and event infrastructure, and hosts a busy calendar of events every year.\\n',\n",
       "  'Suva has many multipurpose venues, the main ones being:\\n- The Vodafone Arena – Seats 4000 – 5000 people depending on configuration.\\n- The ANZ National Stadium – Seats 30,000 people\\n- The FMF National Gymnasium Suva – Seats 2000\\n- The Civic Auditorium – Seats 1000\\n- Albert Park – Grandstand Seating cum Stand-up concert and sporting venue\\n',\n",
       "  \"Suva has a number of parks and a few gardens. Albert Park, in the City centre, is famous as the stage for many national-historical events such as the Independence of Fiji, the landing by Kingsford Smith on the Southern Cross and many parades and carnivals. Sukuna Park, also in the CBD is a popular recreational park and has many performances and events on a weekly basis. Thurston Gardens (named for Governor of Fiji John Bates Thurston) is the city's main botanical garden and the location of the Fiji Museum. Queen Elizabeth Drive is popular as a scenic walk along Suva's foreshore. Many city residents go to the Colo-i-Suva Forest Reserve, a short drive from the city centre, to swim under the waterfalls.\\n\"])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 12;\n",
       "                var nbb_unformatted_code = \"text_labels[50:70], text_data[50:70]\";\n",
       "                var nbb_formatted_code = \"text_labels[50:70], text_data[50:70]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_labels[50:70], text_data[50:70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcccad5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"all_segments = table.get_all_segments()\\ntext_segments = [[y[1] for y in x] for x in all_segments]\\nsegments_labels = [[1 if i == 0 else 0 for i, y in enumerate(x)] for x in all_segments]\";\n",
       "                var nbb_formatted_code = \"all_segments = table.get_all_segments()\\ntext_segments = [[y[1] for y in x] for x in all_segments]\\nsegments_labels = [[1 if i == 0 else 0 for i, y in enumerate(x)] for x in all_segments]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_segments = table.get_all_segments()\n",
    "text_segments = [[y[1] for y in x] for x in all_segments]\n",
    "segments_labels = [[1 if i == 0 else 0 for i, y in enumerate(x)] for x in all_segments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "295657fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"samples = 5\\nmax_tokens = 400\\n\\nfor i, (segment, labels) in enumerate(\\n    zip(text_segments[:samples], segments_labels[:samples])\\n):\\n    for sentence, label in zip(segment, labels):\\n        # this is the training case. During inference, we will have no idea\\n        # when segments start and when they end.\\n        pass\";\n",
       "                var nbb_formatted_code = \"samples = 5\\nmax_tokens = 400\\n\\nfor i, (segment, labels) in enumerate(\\n    zip(text_segments[:samples], segments_labels[:samples])\\n):\\n    for sentence, label in zip(segment, labels):\\n        # this is the training case. During inference, we will have no idea\\n        # when segments start and when they end.\\n        pass\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "samples = 5\n",
    "max_tokens = 400\n",
    "\n",
    "for i, (segment, labels) in enumerate(\n",
    "    zip(text_segments[:samples], segments_labels[:samples])\n",
    "):\n",
    "    for sentence, label in zip(segment, labels):\n",
    "        # this is the training case. During inference, we will have no idea\n",
    "        # when segments start and when they end.\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "467789d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"text_labels[:25]\";\n",
       "                var nbb_formatted_code = \"text_labels[:25]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_labels[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "id": "f6870992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 769;\n",
       "                var nbb_unformatted_code = \"pruning = 0  # remove the lowest n important words from coherence map\\npruning_min = 10  # only prune after n words in the coherence map\\n\\n\\ndef get_weighted_average(weighted_similarities, weights):\\n    return sum(weighted_similarities) / sum(weights)\\n\\n\\n# importance testing\\ndef compare_coherent_words(coherence_map, keywords_current, suppress_errors=False):\\n    word_comparisons = []\\n    weights = []\\n    for i, keywords in enumerate(coherence_map[::-1]):\\n        for word_tuple in keywords:\\n            word = word_tuple[0]\\n            for second_word_tuple in keywords_current:\\n                second_word = second_word_tuple[0]\\n\\n                try:\\n                    word_one_emb = word_tuple[2]\\n                    word_two_emb = second_word_tuple[2]\\n\\n                    # this weight is a recipricol function that will grow smaller the further the keywords are away\\n                    # we want to put more importance on the current words, so we apply twice as much weight.\\n                    if i == 0:\\n                        weight = 2 / (i + 1)\\n                    else:\\n                        weight = 1 / (i + 1)\\n\\n                    word_comparisons.append(\\n                        (\\n                            word,\\n                            second_word,\\n                            weight\\n                            * embedding_lib.get_similarity(word_one_emb, word_two_emb),\\n                        )\\n                    )\\n                    weights.append(weight)\\n                except AssertionError as e:\\n                    if not suppress_errors:\\n                        print(e, word, second_word)\\n\\n    return word_comparisons, weights\\n\\n\\n# TODO: add weighted average: https://www.google.com/search?q=weighted+average&rlz=1C5CHFA_enCA1019CA1024&sxsrf=APwXEdcb6dhJ5L_mvWvrWr4AxQcxOFB01g:1681098698316&tbm=isch&source=iu&ictx=1&vet=1&fir=V-LTDKtCElo89M%252C2WVwd1NrPkHFOM%252C_%253BVGk_lj0HALhXQM%252C2WVwd1NrPkHFOM%252C_%253ByzfbB4i3SpPTFM%252C5e7an03wLAdfhM%252C_%253B47HYmoDH6WlThM%252CsRXbJWfpyOLEOM%252C_%253BOsB4jtfzenfuyM%252CHKcmLkpfJ3xWqM%252C_&usg=AI4_-kRmBXgUWAm_nR3vDsLT17TqM5AvSQ&sa=X&ved=2ahUKEwi6hvvVtJ7-AhXJkIkEHe4JCX4Q_h16BAgoEAE#imgrc=V-LTDKtCElo89M\\ndef coherence_tester(\\n    text_data, text_labels, max_tokens=400, max_str_length=30, prediction_thresh=0.35\\n):\\n    coherence_map = []\\n    predictions = []\\n    for i, (row, label) in enumerate(zip(text_data, text_labels)):\\n        # compare the current sentence to the previous one\\n        if i == 0:\\n            predictions.append((0, 0))\\n        else:\\n            prev_row = text_data[i - 1]\\n\\n            row = truncate_by_token(row, max_tokens)\\n            prev_row = truncate_by_token(prev_row, max_tokens)\\n\\n            cohesion, keywords_prev, keywords_current = coherence.get_coherence(\\n                [row, prev_row], coherence_threshold=0.25\\n            )\\n\\n            # add the keywords to the coherence map\\n            coherence_map.append(cohesion)\\n            if pruning > 0 and len(coherence_map) >= pruning_min:\\n                print(\\\"pruning...\\\", len(coherence_map))\\n                sorted_map = sorted(\\n                    coherence_map, key=lambda tup: tup[1]\\n                )  # sort asc by importance based on keybert\\n                coherence_map = sorted_map[pruning:][\\n                    ::-1\\n                ]  # get the last n - pruning values and reverse the list\\n                print(\\\"done pruning...\\\", len(coherence_map))\\n\\n            # truncate the strings for printing\\n            truncated_row = truncate_string(row, max_str_length)\\n            truncated_prev_row = truncate_string(prev_row, max_str_length)\\n            print(\\n                f\\\"Coherence Map: {[[x[0] for x in c] for c in coherence_map]}, KW Curr: {[x[0] for x in keywords_current]}\\\"\\n            )\\n\\n            # compute the word comparisons between the previous (with the coherence map)\\n            # and the current (possibly the first sentence in a new segment)\\n            word_comparisons_with_coherence, weights = compare_coherent_words(\\n                [*coherence_map, keywords_prev], keywords_current\\n            )\\n\\n            similarities_with_coherence = [\\n                comparison[2] for comparison in word_comparisons_with_coherence\\n            ]\\n            avg_similarity_with_coherence = sum(similarities_with_coherence) / (\\n                len(similarities_with_coherence) or 1\\n            )\\n            weighted_avg_similarity_with_coherence = get_weighted_average(\\n                similarities_with_coherence, weights\\n            )\\n            print(f\\\"weighted: {weighted_avg_similarity_with_coherence}\\\")\\n\\n            # if the two sentences are similar, create a cohesive prediction\\n            # otherwise, predict a new segment\\n            if weighted_avg_similarity_with_coherence > prediction_thresh:\\n                print(\\n                    f\\\"Label: {label}, Prediction: {0}, logit: {weighted_avg_similarity_with_coherence}\\\"\\n                )\\n                predictions.append((weighted_avg_similarity_with_coherence, 0))\\n            else:\\n                # start of a new segment, empty the map\\n                coherence_map = []\\n                print(\\n                    f\\\"Label: {label}, Prediction: {1}, logit: {weighted_avg_similarity_with_coherence}\\\"\\n                )\\n                predictions.append((weighted_avg_similarity_with_coherence, 1))\\n\\n            print(\\\"===============================================\\\")\\n\\n    return predictions\";\n",
       "                var nbb_formatted_code = \"pruning = 0  # remove the lowest n important words from coherence map\\npruning_min = 10  # only prune after n words in the coherence map\\n\\n\\ndef get_weighted_average(weighted_similarities, weights):\\n    return sum(weighted_similarities) / sum(weights)\\n\\n\\n# importance testing\\ndef compare_coherent_words(coherence_map, keywords_current, suppress_errors=False):\\n    word_comparisons = []\\n    weights = []\\n    for i, keywords in enumerate(coherence_map[::-1]):\\n        for word_tuple in keywords:\\n            word = word_tuple[0]\\n            for second_word_tuple in keywords_current:\\n                second_word = second_word_tuple[0]\\n\\n                try:\\n                    word_one_emb = word_tuple[2]\\n                    word_two_emb = second_word_tuple[2]\\n\\n                    # this weight is a recipricol function that will grow smaller the further the keywords are away\\n                    # we want to put more importance on the current words, so we apply twice as much weight.\\n                    if i == 0:\\n                        weight = 2 / (i + 1)\\n                    else:\\n                        weight = 1 / (i + 1)\\n\\n                    word_comparisons.append(\\n                        (\\n                            word,\\n                            second_word,\\n                            weight\\n                            * embedding_lib.get_similarity(word_one_emb, word_two_emb),\\n                        )\\n                    )\\n                    weights.append(weight)\\n                except AssertionError as e:\\n                    if not suppress_errors:\\n                        print(e, word, second_word)\\n\\n    return word_comparisons, weights\\n\\n\\n# TODO: add weighted average: https://www.google.com/search?q=weighted+average&rlz=1C5CHFA_enCA1019CA1024&sxsrf=APwXEdcb6dhJ5L_mvWvrWr4AxQcxOFB01g:1681098698316&tbm=isch&source=iu&ictx=1&vet=1&fir=V-LTDKtCElo89M%252C2WVwd1NrPkHFOM%252C_%253BVGk_lj0HALhXQM%252C2WVwd1NrPkHFOM%252C_%253ByzfbB4i3SpPTFM%252C5e7an03wLAdfhM%252C_%253B47HYmoDH6WlThM%252CsRXbJWfpyOLEOM%252C_%253BOsB4jtfzenfuyM%252CHKcmLkpfJ3xWqM%252C_&usg=AI4_-kRmBXgUWAm_nR3vDsLT17TqM5AvSQ&sa=X&ved=2ahUKEwi6hvvVtJ7-AhXJkIkEHe4JCX4Q_h16BAgoEAE#imgrc=V-LTDKtCElo89M\\ndef coherence_tester(\\n    text_data, text_labels, max_tokens=400, max_str_length=30, prediction_thresh=0.35\\n):\\n    coherence_map = []\\n    predictions = []\\n    for i, (row, label) in enumerate(zip(text_data, text_labels)):\\n        # compare the current sentence to the previous one\\n        if i == 0:\\n            predictions.append((0, 0))\\n        else:\\n            prev_row = text_data[i - 1]\\n\\n            row = truncate_by_token(row, max_tokens)\\n            prev_row = truncate_by_token(prev_row, max_tokens)\\n\\n            cohesion, keywords_prev, keywords_current = coherence.get_coherence(\\n                [row, prev_row], coherence_threshold=0.25\\n            )\\n\\n            # add the keywords to the coherence map\\n            coherence_map.append(cohesion)\\n            if pruning > 0 and len(coherence_map) >= pruning_min:\\n                print(\\\"pruning...\\\", len(coherence_map))\\n                sorted_map = sorted(\\n                    coherence_map, key=lambda tup: tup[1]\\n                )  # sort asc by importance based on keybert\\n                coherence_map = sorted_map[pruning:][\\n                    ::-1\\n                ]  # get the last n - pruning values and reverse the list\\n                print(\\\"done pruning...\\\", len(coherence_map))\\n\\n            # truncate the strings for printing\\n            truncated_row = truncate_string(row, max_str_length)\\n            truncated_prev_row = truncate_string(prev_row, max_str_length)\\n            print(\\n                f\\\"Coherence Map: {[[x[0] for x in c] for c in coherence_map]}, KW Curr: {[x[0] for x in keywords_current]}\\\"\\n            )\\n\\n            # compute the word comparisons between the previous (with the coherence map)\\n            # and the current (possibly the first sentence in a new segment)\\n            word_comparisons_with_coherence, weights = compare_coherent_words(\\n                [*coherence_map, keywords_prev], keywords_current\\n            )\\n\\n            similarities_with_coherence = [\\n                comparison[2] for comparison in word_comparisons_with_coherence\\n            ]\\n            avg_similarity_with_coherence = sum(similarities_with_coherence) / (\\n                len(similarities_with_coherence) or 1\\n            )\\n            weighted_avg_similarity_with_coherence = get_weighted_average(\\n                similarities_with_coherence, weights\\n            )\\n            print(f\\\"weighted: {weighted_avg_similarity_with_coherence}\\\")\\n\\n            # if the two sentences are similar, create a cohesive prediction\\n            # otherwise, predict a new segment\\n            if weighted_avg_similarity_with_coherence > prediction_thresh:\\n                print(\\n                    f\\\"Label: {label}, Prediction: {0}, logit: {weighted_avg_similarity_with_coherence}\\\"\\n                )\\n                predictions.append((weighted_avg_similarity_with_coherence, 0))\\n            else:\\n                # start of a new segment, empty the map\\n                coherence_map = []\\n                print(\\n                    f\\\"Label: {label}, Prediction: {1}, logit: {weighted_avg_similarity_with_coherence}\\\"\\n                )\\n                predictions.append((weighted_avg_similarity_with_coherence, 1))\\n\\n            print(\\\"===============================================\\\")\\n\\n    return predictions\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pruning = 0  # remove the lowest n important words from coherence map\n",
    "pruning_min = 10  # only prune after n words in the coherence map\n",
    "\n",
    "\n",
    "def get_weighted_average(weighted_similarities, weights):\n",
    "    return sum(weighted_similarities) / sum(weights)\n",
    "\n",
    "\n",
    "# importance testing\n",
    "def compare_coherent_words(coherence_map, keywords_current, suppress_errors=False):\n",
    "    word_comparisons = []\n",
    "    weights = []\n",
    "    for i, keywords in enumerate(coherence_map[::-1]):\n",
    "        for word_tuple in keywords:\n",
    "            word = word_tuple[0]\n",
    "            for second_word_tuple in keywords_current:\n",
    "                second_word = second_word_tuple[0]\n",
    "\n",
    "                try:\n",
    "                    word_one_emb = word_tuple[2]\n",
    "                    word_two_emb = second_word_tuple[2]\n",
    "\n",
    "                    # this weight is a recipricol function that will grow smaller the further the keywords are away\n",
    "                    # we want to put more importance on the current words, so we apply twice as much weight.\n",
    "                    if i == 0:\n",
    "                        weight = 2 / (i + 1)\n",
    "                    else:\n",
    "                        weight = 1 / (i + 1)\n",
    "\n",
    "                    word_comparisons.append(\n",
    "                        (\n",
    "                            word,\n",
    "                            second_word,\n",
    "                            weight\n",
    "                            * embedding_lib.get_similarity(word_one_emb, word_two_emb),\n",
    "                        )\n",
    "                    )\n",
    "                    weights.append(weight)\n",
    "                except AssertionError as e:\n",
    "                    if not suppress_errors:\n",
    "                        print(e, word, second_word)\n",
    "\n",
    "    return word_comparisons, weights\n",
    "\n",
    "\n",
    "# TODO: add weighted average: https://www.google.com/search?q=weighted+average&rlz=1C5CHFA_enCA1019CA1024&sxsrf=APwXEdcb6dhJ5L_mvWvrWr4AxQcxOFB01g:1681098698316&tbm=isch&source=iu&ictx=1&vet=1&fir=V-LTDKtCElo89M%252C2WVwd1NrPkHFOM%252C_%253BVGk_lj0HALhXQM%252C2WVwd1NrPkHFOM%252C_%253ByzfbB4i3SpPTFM%252C5e7an03wLAdfhM%252C_%253B47HYmoDH6WlThM%252CsRXbJWfpyOLEOM%252C_%253BOsB4jtfzenfuyM%252CHKcmLkpfJ3xWqM%252C_&usg=AI4_-kRmBXgUWAm_nR3vDsLT17TqM5AvSQ&sa=X&ved=2ahUKEwi6hvvVtJ7-AhXJkIkEHe4JCX4Q_h16BAgoEAE#imgrc=V-LTDKtCElo89M\n",
    "def coherence_tester(\n",
    "    text_data, text_labels, max_tokens=400, max_str_length=30, prediction_thresh=0.35\n",
    "):\n",
    "    coherence_map = []\n",
    "    predictions = []\n",
    "    for i, (row, label) in enumerate(zip(text_data, text_labels)):\n",
    "        # compare the current sentence to the previous one\n",
    "        if i == 0:\n",
    "            predictions.append((0, 0))\n",
    "        else:\n",
    "            prev_row = text_data[i - 1]\n",
    "\n",
    "            row = truncate_by_token(row, max_tokens)\n",
    "            prev_row = truncate_by_token(prev_row, max_tokens)\n",
    "\n",
    "            cohesion, keywords_prev, keywords_current = coherence.get_coherence(\n",
    "                [row, prev_row], coherence_threshold=0.2\n",
    "            )\n",
    "\n",
    "            # add the keywords to the coherence map\n",
    "            coherence_map.append(cohesion)\n",
    "            if pruning > 0 and len(coherence_map) >= pruning_min:\n",
    "                print(\"pruning...\", len(coherence_map))\n",
    "                sorted_map = sorted(\n",
    "                    coherence_map, key=lambda tup: tup[1]\n",
    "                )  # sort asc by importance based on keybert\n",
    "                coherence_map = sorted_map[pruning:][\n",
    "                    ::-1\n",
    "                ]  # get the last n - pruning values and reverse the list\n",
    "                print(\"done pruning...\", len(coherence_map))\n",
    "\n",
    "            # truncate the strings for printing\n",
    "            truncated_row = truncate_string(row, max_str_length)\n",
    "            truncated_prev_row = truncate_string(prev_row, max_str_length)\n",
    "            print(\n",
    "                f\"Coherence Map: {[[x[0] for x in c] for c in coherence_map]}, KW Curr: {[x[0] for x in keywords_current]}\"\n",
    "            )\n",
    "\n",
    "            # compute the word comparisons between the previous (with the coherence map)\n",
    "            # and the current (possibly the first sentence in a new segment)\n",
    "            word_comparisons_with_coherence, weights = compare_coherent_words(\n",
    "                [*coherence_map, keywords_prev], keywords_current\n",
    "            )\n",
    "\n",
    "            similarities_with_coherence = [\n",
    "                comparison[2] for comparison in word_comparisons_with_coherence\n",
    "            ]\n",
    "            avg_similarity_with_coherence = sum(similarities_with_coherence) / (\n",
    "                len(similarities_with_coherence) or 1\n",
    "            )\n",
    "            weighted_avg_similarity_with_coherence = get_weighted_average(\n",
    "                similarities_with_coherence, weights\n",
    "            )\n",
    "            print(f\"weighted: {weighted_avg_similarity_with_coherence}\")\n",
    "\n",
    "            # if the two sentences are similar, create a cohesive prediction\n",
    "            # otherwise, predict a new segment\n",
    "            if weighted_avg_similarity_with_coherence > prediction_thresh:\n",
    "                print(\n",
    "                    f\"Label: {label}, Prediction: {0}, logit: {weighted_avg_similarity_with_coherence}\"\n",
    "                )\n",
    "                predictions.append((weighted_avg_similarity_with_coherence, 0))\n",
    "            else:\n",
    "                # start of a new segment, empty the map\n",
    "                coherence_map = []\n",
    "                print(\n",
    "                    f\"Label: {label}, Prediction: {1}, logit: {weighted_avg_similarity_with_coherence}\"\n",
    "                )\n",
    "                predictions.append((weighted_avg_similarity_with_coherence, 1))\n",
    "\n",
    "            print(\"===============================================\")\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "id": "ca71b4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1963', 'suva', 'sports']\n",
      "['fiji', 'broadcasting', 'television']\n",
      "Got the keywords in 0.6738 seconds\n",
      "Got the embeddings and comparisons in 0.0006 seconds\n",
      "Coherence Map: [['fiji', 'suva', 'broadcasting']], KW Curr: ['1963', 'suva', 'sports']\n",
      "weighted: tensor([0.3807])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3807])\n",
      "===============================================\n",
      "['fiji', 'broadcasting', 'television']\n",
      "['suva', 'tv', 'shows']\n",
      "Got the keywords in 1.0819 seconds\n",
      "Got the embeddings and comparisons in 0.0006 seconds\n",
      "Coherence Map: [['fiji', 'suva', 'broadcasting'], ['suva', 'fiji', 'tv']], KW Curr: ['fiji', 'broadcasting', 'television']\n",
      "weighted: tensor([0.3865])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3865])\n",
      "===============================================\n",
      "['suva', 'tv', 'shows']\n",
      "['nausori', 'ships', 'nadi']\n",
      "Got the keywords in 1.0789 seconds\n",
      "Got the embeddings and comparisons in 0.0007 seconds\n",
      "Coherence Map: [['fiji', 'suva', 'broadcasting'], ['suva', 'fiji', 'tv'], ['nausori', 'suva', 'ships']], KW Curr: ['suva', 'tv', 'shows']\n",
      "weighted: tensor([0.5235])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.5235])\n",
      "===============================================\n",
      "['nausori', 'ships', 'nadi']\n",
      "['civoniceva', 'suva', 'sivivatu']\n",
      "Got the keywords in 1.0108 seconds\n",
      "Got the embeddings and comparisons in 0.0006 seconds\n",
      "Coherence Map: [['fiji', 'suva', 'broadcasting'], ['suva', 'fiji', 'tv'], ['nausori', 'suva', 'ships'], ['civoniceva', 'nausori', 'suva']], KW Curr: ['nausori', 'ships', 'nadi']\n",
      "weighted: tensor([0.3893])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3893])\n",
      "===============================================\n",
      "['civoniceva', 'suva', 'sivivatu']\n",
      "['kashiwara', 'yamato', 'river']\n",
      "Got the keywords in 0.8522 seconds\n",
      "Got the embeddings and comparisons in 0.0008 seconds\n",
      "Coherence Map: [['fiji', 'suva', 'broadcasting'], ['suva', 'fiji', 'tv'], ['nausori', 'suva', 'ships'], ['civoniceva', 'nausori', 'suva'], ['kashiwara', 'civoniceva', 'yamato']], KW Curr: ['civoniceva', 'suva', 'sivivatu']\n",
      "weighted: tensor([0.4134])\n",
      "Label: 1, Prediction: 0, logit: tensor([0.4134])\n",
      "===============================================\n",
      "['kashiwara', 'yamato', 'river']\n",
      "['kashiwara', 'kawachi', '1956']\n",
      "Got the keywords in 0.6678 seconds\n",
      "Got the embeddings and comparisons in 0.0003 seconds\n",
      "Coherence Map: [['fiji', 'suva', 'broadcasting'], ['suva', 'fiji', 'tv'], ['nausori', 'suva', 'ships'], ['civoniceva', 'nausori', 'suva'], ['kashiwara', 'civoniceva', 'yamato'], ['kashiwara', 'kashiwara', 'kashiwara']], KW Curr: ['kashiwara', 'yamato', 'river']\n",
      "weighted: tensor([0.3509])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3509])\n",
      "===============================================\n",
      "['kashiwara', 'kawachi', '1956']\n",
      "['havana', '529064', '111021']\n",
      "Got the keywords in 0.6369 seconds\n",
      "Got the embeddings and comparisons in 0.0002 seconds\n",
      "Coherence Map: [['fiji', 'suva', 'broadcasting'], ['suva', 'fiji', 'tv'], ['nausori', 'suva', 'ships'], ['civoniceva', 'nausori', 'suva'], ['kashiwara', 'civoniceva', 'yamato'], ['kashiwara', 'kashiwara', 'kashiwara'], ['havana', 'kashiwara', 'havana']], KW Curr: ['kashiwara', 'kawachi', '1956']\n",
      "weighted: tensor([0.4216])\n",
      "Label: 1, Prediction: 0, logit: tensor([0.4216])\n",
      "===============================================\n",
      "['havana', '529064', '111021']\n",
      "['marvinville', '1903', 'greenville']\n",
      "Got the keywords in 0.2614 seconds\n",
      "Got the embeddings and comparisons in 0.0004 seconds\n",
      "Coherence Map: [['fiji', 'suva', 'broadcasting'], ['suva', 'fiji', 'tv'], ['nausori', 'suva', 'ships'], ['civoniceva', 'nausori', 'suva'], ['kashiwara', 'civoniceva', 'yamato'], ['kashiwara', 'kashiwara', 'kashiwara'], ['havana', 'kashiwara', 'havana'], ['marvinville', 'havana', 'greenville']], KW Curr: ['havana', '529064', '111021']\n",
      "weighted: tensor([0.3766])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3766])\n",
      "===============================================\n",
      "['marvinville', '1903', 'greenville']\n",
      "['population', 'residing', 'householder']\n",
      "Got the keywords in 0.8029 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['fiji', 'suva', 'broadcasting'], ['suva', 'fiji', 'tv'], ['nausori', 'suva', 'ships'], ['civoniceva', 'nausori', 'suva'], ['kashiwara', 'civoniceva', 'yamato'], ['kashiwara', 'kashiwara', 'kashiwara'], ['havana', 'kashiwara', 'havana'], ['marvinville', 'havana', 'greenville'], ['population', 'marvinville', 'residing']], KW Curr: ['marvinville', '1903', 'greenville']\n",
      "weighted: tensor([0.3917])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3917])\n",
      "===============================================\n",
      "['population', 'residing', 'householder']\n",
      "['1905', 'alvarado', 'called']\n",
      "Got the keywords in 0.7730 seconds\n",
      "Got the embeddings and comparisons in 0.0003 seconds\n",
      "Coherence Map: [['fiji', 'suva', 'broadcasting'], ['suva', 'fiji', 'tv'], ['nausori', 'suva', 'ships'], ['civoniceva', 'nausori', 'suva'], ['kashiwara', 'civoniceva', 'yamato'], ['kashiwara', 'kashiwara', 'kashiwara'], ['havana', 'kashiwara', 'havana'], ['marvinville', 'havana', 'greenville'], ['population', 'marvinville', 'residing'], ['alvarado', 'population', 'called']], KW Curr: ['population', 'residing', 'householder']\n",
      "weighted: tensor([0.4070])\n",
      "Label: 1, Prediction: 0, logit: tensor([0.4070])\n",
      "===============================================\n",
      "['1905', 'alvarado', 'called']\n",
      "['total', 'census', 'city']\n",
      "Got the keywords in 0.1375 seconds\n",
      "Got the embeddings and comparisons in 0.0001 seconds\n",
      "Coherence Map: [['fiji', 'suva', 'broadcasting'], ['suva', 'fiji', 'tv'], ['nausori', 'suva', 'ships'], ['civoniceva', 'nausori', 'suva'], ['kashiwara', 'civoniceva', 'yamato'], ['kashiwara', 'kashiwara', 'kashiwara'], ['havana', 'kashiwara', 'havana'], ['marvinville', 'havana', 'greenville'], ['population', 'marvinville', 'residing'], ['alvarado', 'population', 'called'], ['total', 'alvarado', 'census']], KW Curr: ['1905', 'alvarado', 'called']\n",
      "weighted: tensor([0.4141])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.4141])\n",
      "===============================================\n",
      "['total', 'census', 'city']\n",
      "['population', 'residents', 'racial']\n",
      "Got the keywords in 0.3298 seconds\n",
      "Got the embeddings and comparisons in 0.0006 seconds\n",
      "Coherence Map: [['fiji', 'suva', 'broadcasting'], ['suva', 'fiji', 'tv'], ['nausori', 'suva', 'ships'], ['civoniceva', 'nausori', 'suva'], ['kashiwara', 'civoniceva', 'yamato'], ['kashiwara', 'kashiwara', 'kashiwara'], ['havana', 'kashiwara', 'havana'], ['marvinville', 'havana', 'greenville'], ['population', 'marvinville', 'residing'], ['alvarado', 'population', 'called'], ['total', 'alvarado', 'census'], ['population', 'total', 'residents']], KW Curr: ['total', 'census', 'city']\n",
      "weighted: tensor([0.3595])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3595])\n",
      "===============================================\n",
      "['population', 'residents', 'racial']\n",
      "['population', 'households', 'householder']\n",
      "Got the keywords in 0.9629 seconds\n",
      "Got the embeddings and comparisons in 0.0006 seconds\n",
      "Coherence Map: [['fiji', 'suva', 'broadcasting'], ['suva', 'fiji', 'tv'], ['nausori', 'suva', 'ships'], ['civoniceva', 'nausori', 'suva'], ['kashiwara', 'civoniceva', 'yamato'], ['kashiwara', 'kashiwara', 'kashiwara'], ['havana', 'kashiwara', 'havana'], ['marvinville', 'havana', 'greenville'], ['population', 'marvinville', 'residing'], ['alvarado', 'population', 'called'], ['total', 'alvarado', 'census'], ['population', 'total', 'residents'], ['population', 'population', 'households']], KW Curr: ['population', 'residents', 'racial']\n",
      "weighted: tensor([0.4835])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.4835])\n",
      "===============================================\n",
      "['population', 'households', 'householder']\n",
      "['1890', '1901', '1899']\n",
      "Got the keywords in 0.7841 seconds\n",
      "Got the embeddings and comparisons in 0.0000 seconds\n",
      "Coherence Map: [['fiji', 'suva', 'broadcasting'], ['suva', 'fiji', 'tv'], ['nausori', 'suva', 'ships'], ['civoniceva', 'nausori', 'suva'], ['kashiwara', 'civoniceva', 'yamato'], ['kashiwara', 'kashiwara', 'kashiwara'], ['havana', 'kashiwara', 'havana'], ['marvinville', 'havana', 'greenville'], ['population', 'marvinville', 'residing'], ['alvarado', 'population', 'called'], ['total', 'alvarado', 'census'], ['population', 'total', 'residents'], ['population', 'population', 'households'], []], KW Curr: ['population', 'households', 'householder']\n",
      "weighted: tensor([0.4085])\n",
      "Label: 1, Prediction: 0, logit: tensor([0.4085])\n",
      "===============================================\n",
      "['1890', '1901', '1899']\n",
      "['total', 'census', 'city']\n",
      "Got the keywords in 0.1344 seconds\n",
      "Got the embeddings and comparisons in 0.0000 seconds\n",
      "Coherence Map: [['fiji', 'suva', 'broadcasting'], ['suva', 'fiji', 'tv'], ['nausori', 'suva', 'ships'], ['civoniceva', 'nausori', 'suva'], ['kashiwara', 'civoniceva', 'yamato'], ['kashiwara', 'kashiwara', 'kashiwara'], ['havana', 'kashiwara', 'havana'], ['marvinville', 'havana', 'greenville'], ['population', 'marvinville', 'residing'], ['alvarado', 'population', 'called'], ['total', 'alvarado', 'census'], ['population', 'total', 'residents'], ['population', 'population', 'households'], [], []], KW Curr: ['1890', '1901', '1899']\n",
      "weighted: tensor([0.3618])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3618])\n",
      "===============================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['total', 'census', 'city']\n",
      "['population', 'residents', 'racial']\n",
      "Got the keywords in 0.3228 seconds\n",
      "Got the embeddings and comparisons in 0.0006 seconds\n",
      "Coherence Map: [['fiji', 'suva', 'broadcasting'], ['suva', 'fiji', 'tv'], ['nausori', 'suva', 'ships'], ['civoniceva', 'nausori', 'suva'], ['kashiwara', 'civoniceva', 'yamato'], ['kashiwara', 'kashiwara', 'kashiwara'], ['havana', 'kashiwara', 'havana'], ['marvinville', 'havana', 'greenville'], ['population', 'marvinville', 'residing'], ['alvarado', 'population', 'called'], ['total', 'alvarado', 'census'], ['population', 'total', 'residents'], ['population', 'population', 'households'], [], [], ['population', 'total', 'residents']], KW Curr: ['total', 'census', 'city']\n",
      "weighted: tensor([0.3561])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3561])\n",
      "===============================================\n",
      "['population', 'residents', 'racial']\n",
      "['population', 'householder', 'households']\n",
      "Got the keywords in 0.9618 seconds\n",
      "Got the embeddings and comparisons in 0.0006 seconds\n",
      "Coherence Map: [['fiji', 'suva', 'broadcasting'], ['suva', 'fiji', 'tv'], ['nausori', 'suva', 'ships'], ['civoniceva', 'nausori', 'suva'], ['kashiwara', 'civoniceva', 'yamato'], ['kashiwara', 'kashiwara', 'kashiwara'], ['havana', 'kashiwara', 'havana'], ['marvinville', 'havana', 'greenville'], ['population', 'marvinville', 'residing'], ['alvarado', 'population', 'called'], ['total', 'alvarado', 'census'], ['population', 'total', 'residents'], ['population', 'population', 'households'], [], [], ['population', 'total', 'residents'], ['population', 'population', 'householder']], KW Curr: ['population', 'residents', 'racial']\n",
      "weighted: tensor([0.5624])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.5624])\n",
      "===============================================\n",
      "['population', 'householder', 'households']\n",
      "['washta', '575896', 'river']\n",
      "Got the keywords in 0.7679 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['fiji', 'suva', 'broadcasting'], ['suva', 'fiji', 'tv'], ['nausori', 'suva', 'ships'], ['civoniceva', 'nausori', 'suva'], ['kashiwara', 'civoniceva', 'yamato'], ['kashiwara', 'kashiwara', 'kashiwara'], ['havana', 'kashiwara', 'havana'], ['marvinville', 'havana', 'greenville'], ['population', 'marvinville', 'residing'], ['alvarado', 'population', 'called'], ['total', 'alvarado', 'census'], ['population', 'total', 'residents'], ['population', 'population', 'households'], [], [], ['population', 'total', 'residents'], ['population', 'population', 'householder'], ['washta', 'population', 'river']], KW Curr: ['population', 'householder', 'households']\n",
      "weighted: tensor([0.4156])\n",
      "Label: 1, Prediction: 0, logit: tensor([0.4156])\n",
      "===============================================\n",
      "['washta', '575896', 'river']\n",
      "['population', 'residents', 'living']\n",
      "Got the keywords in 0.7542 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['fiji', 'suva', 'broadcasting'], ['suva', 'fiji', 'tv'], ['nausori', 'suva', 'ships'], ['civoniceva', 'nausori', 'suva'], ['kashiwara', 'civoniceva', 'yamato'], ['kashiwara', 'kashiwara', 'kashiwara'], ['havana', 'kashiwara', 'havana'], ['marvinville', 'havana', 'greenville'], ['population', 'marvinville', 'residing'], ['alvarado', 'population', 'called'], ['total', 'alvarado', 'census'], ['population', 'total', 'residents'], ['population', 'population', 'households'], [], [], ['population', 'total', 'residents'], ['population', 'population', 'householder'], ['washta', 'population', 'river'], ['population', 'washta', 'residents']], KW Curr: ['washta', '575896', 'river']\n",
      "weighted: tensor([0.3375])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.3375])\n",
      "===============================================\n",
      "['population', 'residents', 'living']\n",
      "['householder', 'households', 'household']\n",
      "Got the keywords in 0.9504 seconds\n",
      "Got the embeddings and comparisons in 0.0006 seconds\n",
      "Coherence Map: [['householder', 'population', 'households']], KW Curr: ['population', 'residents', 'living']\n",
      "weighted: tensor([0.4874])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.4874])\n",
      "===============================================\n",
      "['householder', 'households', 'household']\n",
      "['1889', '1891', '1909']\n",
      "Got the keywords in 1.0756 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['householder', 'population', 'households'], []], KW Curr: ['householder', 'households', 'household']\n",
      "weighted: tensor([0.4245])\n",
      "Label: 1, Prediction: 0, logit: tensor([0.4245])\n",
      "===============================================\n",
      "['1889', '1891', '1909']\n",
      "['historic', 'register', 'district']\n",
      "Got the keywords in 0.8979 seconds\n",
      "Got the embeddings and comparisons in 0.0000 seconds\n",
      "Coherence Map: [['householder', 'population', 'households'], [], []], KW Curr: ['1889', '1891', '1909']\n",
      "weighted: tensor([0.3100])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.3100])\n",
      "===============================================\n",
      "['historic', 'register', 'district']\n",
      "['oklahoma', 'dallas', 'texas']\n",
      "Got the keywords in 0.6088 seconds\n",
      "Got the embeddings and comparisons in 0.0006 seconds\n",
      "Coherence Map: [['oklahoma', 'historic', 'dallas']], KW Curr: ['historic', 'register', 'district']\n",
      "weighted: tensor([0.4512])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.4512])\n",
      "===============================================\n",
      "['oklahoma', 'dallas', 'texas']\n",
      "['racial', 'population', 'residents']\n",
      "Got the keywords in 0.8882 seconds\n",
      "Got the embeddings and comparisons in 0.0006 seconds\n",
      "Coherence Map: [['oklahoma', 'historic', 'dallas'], ['racial', 'oklahoma', 'population']], KW Curr: ['oklahoma', 'dallas', 'texas']\n",
      "weighted: tensor([0.4351])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.4351])\n",
      "===============================================\n",
      "['racial', 'population', 'residents']\n",
      "['1975', 'city', 'town']\n",
      "Got the keywords in 1.0982 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['oklahoma', 'historic', 'dallas'], ['racial', 'oklahoma', 'population'], ['city', 'racial', 'town']], KW Curr: ['racial', 'population', 'residents']\n",
      "weighted: tensor([0.4372])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.4372])\n",
      "===============================================\n",
      "['1975', 'city', 'town']\n",
      "['1909', 'university', 'institution']\n",
      "Got the keywords in 0.9702 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['oklahoma', 'historic', 'dallas'], ['racial', 'oklahoma', 'population'], ['city', 'racial', 'town'], ['university', 'city', 'institution']], KW Curr: ['1975', 'city', 'town']\n",
      "weighted: tensor([0.4309])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.4309])\n",
      "===============================================\n",
      "['1909', 'university', 'institution']\n",
      "['schools', 'school', 'glenwood']\n",
      "Got the keywords in 0.6702 seconds\n",
      "Got the embeddings and comparisons in 0.0003 seconds\n",
      "Coherence Map: [['oklahoma', 'historic', 'dallas'], ['racial', 'oklahoma', 'population'], ['city', 'racial', 'town'], ['university', 'city', 'institution'], ['schools', 'university', 'school']], KW Curr: ['1909', 'university', 'institution']\n",
      "weighted: tensor([0.3806])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3806])\n",
      "===============================================\n",
      "['schools', 'school', 'glenwood']\n",
      "['pontotoc', 'tech', 'technology']\n",
      "Got the keywords in 0.1916 seconds\n",
      "Got the embeddings and comparisons in 0.0002 seconds\n",
      "Coherence Map: [['oklahoma', 'historic', 'dallas'], ['racial', 'oklahoma', 'population'], ['city', 'racial', 'town'], ['university', 'city', 'institution'], ['schools', 'university', 'school'], ['pontotoc', 'schools', 'tech']], KW Curr: ['schools', 'school', 'glenwood']\n",
      "weighted: tensor([0.5368])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.5368])\n",
      "===============================================\n",
      "['pontotoc', 'tech', 'technology']\n",
      "['2006', 'crime', 'guilt']\n",
      "Got the keywords in 0.4300 seconds\n",
      "Got the embeddings and comparisons in 0.0037 seconds\n",
      "Coherence Map: [['oklahoma', 'historic', 'dallas'], ['racial', 'oklahoma', 'population'], ['city', 'racial', 'town'], ['university', 'city', 'institution'], ['schools', 'university', 'school'], ['pontotoc', 'schools', 'tech'], ['crime', 'pontotoc', 'guilt']], KW Curr: ['pontotoc', 'tech', 'technology']\n",
      "weighted: tensor([0.4320])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.4320])\n",
      "===============================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2006', 'crime', 'guilt']\n",
      "['1885', '1889', 'water']\n",
      "Got the keywords in 1.1601 seconds\n",
      "Got the embeddings and comparisons in 0.0011 seconds\n",
      "Coherence Map: [['oklahoma', 'historic', 'dallas'], ['racial', 'oklahoma', 'population'], ['city', 'racial', 'town'], ['university', 'city', 'institution'], ['schools', 'university', 'school'], ['pontotoc', 'schools', 'tech'], ['crime', 'pontotoc', 'guilt'], ['water', 'crime', 'water']], KW Curr: ['2006', 'crime', 'guilt']\n",
      "weighted: tensor([0.4488])\n",
      "Label: 1, Prediction: 0, logit: tensor([0.4488])\n",
      "===============================================\n",
      "['1885', '1889', 'water']\n",
      "['1907', '1903', '1922']\n",
      "Got the keywords in 1.2390 seconds\n",
      "Got the embeddings and comparisons in 0.0000 seconds\n",
      "Coherence Map: [['oklahoma', 'historic', 'dallas'], ['racial', 'oklahoma', 'population'], ['city', 'racial', 'town'], ['university', 'city', 'institution'], ['schools', 'university', 'school'], ['pontotoc', 'schools', 'tech'], ['crime', 'pontotoc', 'guilt'], ['water', 'crime', 'water'], []], KW Curr: ['1885', '1889', 'water']\n",
      "weighted: tensor([0.4816])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.4816])\n",
      "===============================================\n",
      "['1907', '1903', '1922']\n",
      "['winter', 'cold', 'comodoro']\n",
      "Got the keywords in 0.9769 seconds\n",
      "Got the embeddings and comparisons in 0.0000 seconds\n",
      "Coherence Map: [['oklahoma', 'historic', 'dallas'], ['racial', 'oklahoma', 'population'], ['city', 'racial', 'town'], ['university', 'city', 'institution'], ['schools', 'university', 'school'], ['pontotoc', 'schools', 'tech'], ['crime', 'pontotoc', 'guilt'], ['water', 'crime', 'water'], [], []], KW Curr: ['1907', '1903', '1922']\n",
      "weighted: tensor([0.3860])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3860])\n",
      "===============================================\n",
      "['winter', 'cold', 'comodoro']\n",
      "['rivadavia', 'population', '1991']\n",
      "Got the keywords in 0.7998 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['oklahoma', 'historic', 'dallas'], ['racial', 'oklahoma', 'population'], ['city', 'racial', 'town'], ['university', 'city', 'institution'], ['schools', 'university', 'school'], ['pontotoc', 'schools', 'tech'], ['crime', 'pontotoc', 'guilt'], ['water', 'crime', 'water'], [], [], ['rivadavia', 'winter', 'population']], KW Curr: ['winter', 'cold', 'comodoro']\n",
      "weighted: tensor([0.3812])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3812])\n",
      "===============================================\n",
      "['rivadavia', 'population', '1991']\n",
      "['industrial', 'northern', 'downtown']\n",
      "Got the keywords in 0.9470 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['oklahoma', 'historic', 'dallas'], ['racial', 'oklahoma', 'population'], ['city', 'racial', 'town'], ['university', 'city', 'institution'], ['schools', 'university', 'school'], ['pontotoc', 'schools', 'tech'], ['crime', 'pontotoc', 'guilt'], ['water', 'crime', 'water'], [], [], ['rivadavia', 'winter', 'population'], ['industrial', 'rivadavia', 'northern']], KW Curr: ['rivadavia', 'population', '1991']\n",
      "weighted: tensor([0.5200])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.5200])\n",
      "===============================================\n",
      "['industrial', 'northern', 'downtown']\n",
      "['industrial', 'production', 'city']\n",
      "Got the keywords in 0.8847 seconds\n",
      "Got the embeddings and comparisons in 0.0006 seconds\n",
      "Coherence Map: [['oklahoma', 'historic', 'dallas'], ['racial', 'oklahoma', 'population'], ['city', 'racial', 'town'], ['university', 'city', 'institution'], ['schools', 'university', 'school'], ['pontotoc', 'schools', 'tech'], ['crime', 'pontotoc', 'guilt'], ['water', 'crime', 'water'], [], [], ['rivadavia', 'winter', 'population'], ['industrial', 'rivadavia', 'northern'], ['industrial', 'industrial', 'production']], KW Curr: ['industrial', 'northern', 'downtown']\n",
      "weighted: tensor([0.3697])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3697])\n",
      "===============================================\n",
      "['industrial', 'production', 'city']\n",
      "['1907', '1986', '1933']\n",
      "Got the keywords in 0.8687 seconds\n",
      "Got the embeddings and comparisons in 0.0000 seconds\n",
      "Coherence Map: [['oklahoma', 'historic', 'dallas'], ['racial', 'oklahoma', 'population'], ['city', 'racial', 'town'], ['university', 'city', 'institution'], ['schools', 'university', 'school'], ['pontotoc', 'schools', 'tech'], ['crime', 'pontotoc', 'guilt'], ['water', 'crime', 'water'], [], [], ['rivadavia', 'winter', 'population'], ['industrial', 'rivadavia', 'northern'], ['industrial', 'industrial', 'production'], []], KW Curr: ['industrial', 'production', 'city']\n",
      "weighted: tensor([0.3066])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.3066])\n",
      "===============================================\n",
      "['1907', '1986', '1933']\n",
      "['rivadavia', 'create', 'argentina']\n",
      "Got the keywords in 0.8759 seconds\n",
      "Got the embeddings and comparisons in 0.0000 seconds\n",
      "Coherence Map: [[]], KW Curr: ['1907', '1986', '1933']\n",
      "weighted: tensor([0.3688])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3688])\n",
      "===============================================\n",
      "['rivadavia', 'create', 'argentina']\n",
      "['cartography', 'geographic', 'córdova']\n",
      "Got the keywords in 0.6848 seconds\n",
      "Got the embeddings and comparisons in 0.0002 seconds\n",
      "Coherence Map: [[], ['cartography', 'rivadavia', 'geographic']], KW Curr: ['rivadavia', 'create', 'argentina']\n",
      "weighted: tensor([0.3891])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3891])\n",
      "===============================================\n",
      "['cartography', 'geographic', 'córdova']\n",
      "['1926', '1908', '1928']\n",
      "Got the keywords in 0.6903 seconds\n",
      "Got the embeddings and comparisons in 0.0000 seconds\n",
      "Coherence Map: [[], ['cartography', 'rivadavia', 'geographic'], []], KW Curr: ['cartography', 'geographic', 'córdova']\n",
      "weighted: tensor([0.4549])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.4549])\n",
      "===============================================\n",
      "['1926', '1908', '1928']\n",
      "['ships', 'cordova', 'shipyard']\n",
      "Got the keywords in 0.7968 seconds\n",
      "Got the embeddings and comparisons in 0.0000 seconds\n",
      "Coherence Map: [[], ['cartography', 'rivadavia', 'geographic'], [], []], KW Curr: ['1926', '1908', '1928']\n",
      "weighted: tensor([0.3854])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3854])\n",
      "===============================================\n",
      "['ships', 'cordova', 'shipyard']\n",
      "['concrete', '228', 'bca']\n",
      "Got the keywords in 0.8335 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [[], ['cartography', 'rivadavia', 'geographic'], [], [], ['concrete', 'ships', 'bca']], KW Curr: ['ships', 'cordova', 'shipyard']\n",
      "weighted: tensor([0.5134])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.5134])\n",
      "===============================================\n",
      "['concrete', '228', 'bca']\n",
      "['rivadavia', 'capacity', 'generators']\n",
      "Got the keywords in 0.6067 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [[], ['cartography', 'rivadavia', 'geographic'], [], [], ['concrete', 'ships', 'bca'], ['rivadavia', 'concrete', 'capacity']], KW Curr: ['concrete', '228', 'bca']\n",
      "weighted: tensor([0.4604])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.4604])\n",
      "===============================================\n",
      "['rivadavia', 'capacity', 'generators']\n",
      "['deportiva', 'rivadavia', 'sports']\n",
      "Got the keywords in 0.7265 seconds\n",
      "Got the embeddings and comparisons in 0.0006 seconds\n",
      "Coherence Map: [[], ['cartography', 'rivadavia', 'geographic'], [], [], ['concrete', 'ships', 'bca'], ['rivadavia', 'concrete', 'capacity'], ['deportiva', 'rivadavia', 'rivadavia']], KW Curr: ['rivadavia', 'capacity', 'generators']\n",
      "weighted: tensor([0.4088])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.4088])\n",
      "===============================================\n",
      "['deportiva', 'rivadavia', 'sports']\n",
      "['1939', '1941', '1943']\n",
      "Got the keywords in 0.8790 seconds\n",
      "Got the embeddings and comparisons in 0.0000 seconds\n",
      "Coherence Map: [[], ['cartography', 'rivadavia', 'geographic'], [], [], ['concrete', 'ships', 'bca'], ['rivadavia', 'concrete', 'capacity'], ['deportiva', 'rivadavia', 'rivadavia'], []], KW Curr: ['deportiva', 'rivadavia', 'sports']\n",
      "weighted: tensor([0.3422])\n",
      "Label: 1, Prediction: 1, logit: tensor([0.3422])\n",
      "===============================================\n",
      "['1939', '1941', '1943']\n",
      "['1878', '1894', 'nashville']\n",
      "Got the keywords in 0.7222 seconds\n",
      "Got the embeddings and comparisons in 0.0000 seconds\n",
      "Coherence Map: [[]], KW Curr: ['1939', '1941', '1943']\n",
      "weighted: tensor([0.3599])\n",
      "Label: 1, Prediction: 0, logit: tensor([0.3599])\n",
      "===============================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1878', '1894', 'nashville']\n",
      "['hohenwald', '5520', '5479']\n",
      "Got the keywords in 0.5714 seconds\n",
      "Got the embeddings and comparisons in 0.0002 seconds\n",
      "Coherence Map: [[], []], KW Curr: ['1878', '1894', 'nashville']\n",
      "weighted: tensor([0.3154])\n",
      "Label: 0, Prediction: 1, logit: tensor([0.3154])\n",
      "===============================================\n",
      "['hohenwald', '5520', '5479']\n",
      "['population', 'racial', 'households']\n",
      "Got the keywords in 0.4993 seconds\n",
      "Got the embeddings and comparisons in 0.0101 seconds\n",
      "Coherence Map: [['population', 'hohenwald', 'racial']], KW Curr: ['hohenwald', '5520', '5479']\n",
      "weighted: tensor([0.3703])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.3703])\n",
      "===============================================\n",
      "['population', 'racial', 'households']\n",
      "['279574', 'water', 'states']\n",
      "Got the keywords in 0.8482 seconds\n",
      "Got the embeddings and comparisons in 0.0008 seconds\n",
      "Coherence Map: [['population', 'hohenwald', 'racial'], ['water', 'population', 'states']], KW Curr: ['population', 'racial', 'households']\n",
      "weighted: tensor([0.4863])\n",
      "Label: 1, Prediction: 0, logit: tensor([0.4863])\n",
      "===============================================\n",
      "['279574', 'water', 'states']\n",
      "['population', 'households', 'householder']\n",
      "Got the keywords in 0.8571 seconds\n",
      "Got the embeddings and comparisons in 0.0005 seconds\n",
      "Coherence Map: [['population', 'hohenwald', 'racial'], ['water', 'population', 'states'], ['population', 'water', 'households']], KW Curr: ['279574', 'water', 'states']\n",
      "weighted: tensor([0.5557])\n",
      "Label: 0, Prediction: 0, logit: tensor([0.5557])\n",
      "===============================================\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 770;\n",
       "                var nbb_unformatted_code = \"start = 75\\nnum_samples = 50\\nmax_tokens = 256  # want to keep this under 512\\nmax_str_length = 30\\n\\ntrue_labels = text_labels[start : start + num_samples]\\n\\npredictions = coherence_tester(\\n    text_data[start : start + num_samples],\\n    true_labels,\\n    max_tokens=max_tokens,\\n    max_str_length=max_str_length,\\n)\";\n",
       "                var nbb_formatted_code = \"start = 75\\nnum_samples = 50\\nmax_tokens = 256  # want to keep this under 512\\nmax_str_length = 30\\n\\ntrue_labels = text_labels[start : start + num_samples]\\n\\npredictions = coherence_tester(\\n    text_data[start : start + num_samples],\\n    true_labels,\\n    max_tokens=max_tokens,\\n    max_str_length=max_str_length,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start = 75\n",
    "num_samples = 50\n",
    "max_tokens = 256  # want to keep this under 512\n",
    "max_str_length = 30\n",
    "\n",
    "true_labels = text_labels[start : start + num_samples]\n",
    "\n",
    "predictions = coherence_tester(\n",
    "    text_data[start : start + num_samples],\n",
    "    true_labels,\n",
    "    max_tokens=max_tokens,\n",
    "    max_str_length=max_str_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "id": "d3a93727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0]\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 765;\n",
       "                var nbb_unformatted_code = \"print([x[1] for x in predictions])\\nprint(true_labels)\";\n",
       "                var nbb_formatted_code = \"print([x[1] for x in predictions])\\nprint(true_labels)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print([x[1] for x in predictions])\n",
    "print(true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "id": "61e7863f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 766;\n",
       "                var nbb_unformatted_code = \"pred_string = \\\"\\\".join(str([x[1] for x in predictions]))\\ntrue_string = \\\"\\\".join(str(true_labels))\";\n",
       "                var nbb_formatted_code = \"pred_string = \\\"\\\".join(str([x[1] for x in predictions]))\\ntrue_string = \\\"\\\".join(str(true_labels))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_string = \"\".join(str([x[1] for x in predictions]))\n",
    "true_string = \"\".join(str(true_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "id": "19af16ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 767;\n",
       "                var nbb_unformatted_code = \"avg_k = len(true_labels) // (true_labels.count(1) + 1)  # get avg segment size\";\n",
       "                var nbb_formatted_code = \"avg_k = len(true_labels) // (true_labels.count(1) + 1)  # get avg segment size\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "avg_k = len(true_labels) // (true_labels.count(1) + 1)  # get avg segment size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "id": "db43c420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 4\n",
      "wd = 0.3741496598639456\n",
      "pk = 0.3673469387755102\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 768;\n",
       "                var nbb_unformatted_code = \"wd_score = windowdiff(pred_string, true_string, avg_k)\\npk_score = pk(pred_string, true_string, avg_k)\\n\\nprint(f\\\"k = {avg_k}\\\")\\nprint(f\\\"wd = {wd_score}\\\")\\nprint(f\\\"pk = {pk_score}\\\")\";\n",
       "                var nbb_formatted_code = \"wd_score = windowdiff(pred_string, true_string, avg_k)\\npk_score = pk(pred_string, true_string, avg_k)\\n\\nprint(f\\\"k = {avg_k}\\\")\\nprint(f\\\"wd = {wd_score}\\\")\\nprint(f\\\"pk = {pk_score}\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wd_score = windowdiff(pred_string, true_string, avg_k)\n",
    "pk_score = pk(pred_string, true_string, avg_k)\n",
    "\n",
    "print(f\"k = {avg_k}\")\n",
    "print(f\"wd = {wd_score}\")\n",
    "print(f\"pk = {pk_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb16194",
   "metadata": {},
   "source": [
    "## Prediction Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "id": "af208503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 755;\n",
       "                var nbb_unformatted_code = \"pred_thresh = 0.33\";\n",
       "                var nbb_formatted_code = \"pred_thresh = 0.33\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_thresh = 0.33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "id": "e9cebcd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 756;\n",
       "                var nbb_unformatted_code = \"modified_predictions = [\\n    1 if x < pred_thresh else 0 for x in [x[0] for x in predictions]\\n]\\n\\npred_string = \\\"\\\".join(str(modified_predictions))\\ntrue_string = \\\"\\\".join(str(true_labels))\\n\\navg_k = len(true_labels) // (true_labels.count(1) + 1)  # get avg segment size\";\n",
       "                var nbb_formatted_code = \"modified_predictions = [\\n    1 if x < pred_thresh else 0 for x in [x[0] for x in predictions]\\n]\\n\\npred_string = \\\"\\\".join(str(modified_predictions))\\ntrue_string = \\\"\\\".join(str(true_labels))\\n\\navg_k = len(true_labels) // (true_labels.count(1) + 1)  # get avg segment size\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "modified_predictions = [\n",
    "    1 if x < pred_thresh else 0 for x in [x[0] for x in predictions]\n",
    "]\n",
    "\n",
    "pred_string = \"\".join(str(modified_predictions))\n",
    "true_string = \"\".join(str(true_labels))\n",
    "\n",
    "avg_k = len(true_labels) // (true_labels.count(1) + 1)  # get avg segment size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "id": "6a2f65af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0]\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 757;\n",
       "                var nbb_unformatted_code = \"print(pred_string)\\nprint(true_string)\";\n",
       "                var nbb_formatted_code = \"print(pred_string)\\nprint(true_string)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(pred_string)\n",
    "print(true_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "id": "b9dbb752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 4\n",
      "wd = 0.36054421768707484\n",
      "pk = 0.35374149659863946\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 758;\n",
       "                var nbb_unformatted_code = \"wd_score = windowdiff(pred_string, true_string, avg_k)\\npk_score = pk(pred_string, true_string, avg_k)\\n\\nprint(f\\\"k = {avg_k}\\\")\\nprint(f\\\"wd = {wd_score}\\\")\\nprint(f\\\"pk = {pk_score}\\\")\";\n",
       "                var nbb_formatted_code = \"wd_score = windowdiff(pred_string, true_string, avg_k)\\npk_score = pk(pred_string, true_string, avg_k)\\n\\nprint(f\\\"k = {avg_k}\\\")\\nprint(f\\\"wd = {wd_score}\\\")\\nprint(f\\\"pk = {pk_score}\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wd_score = windowdiff(pred_string, true_string, avg_k)\n",
    "pk_score = pk(pred_string, true_string, avg_k)\n",
    "\n",
    "print(f\"k = {avg_k}\")\n",
    "print(f\"wd = {wd_score}\")\n",
    "print(f\"pk = {pk_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34585627",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "462a8434",
   "metadata": {},
   "source": [
    "## KeyBERT Embedding Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "d15e7648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 172;\n",
       "                var nbb_unformatted_code = \"curr = 230\\nprev = curr - 1\";\n",
       "                var nbb_formatted_code = \"curr = 230\\nprev = curr - 1\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "curr = 230\n",
    "prev = curr - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "8c6434ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got the keywords in 0.6567 seconds\n",
      "Got the embeddings and comparisons in 0.0007 seconds\n",
      "['cantonese', 'languages', 'vietnamese', 'communes']\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 205;\n",
       "                var nbb_unformatted_code = \"cohesion = coherence.get_coherence(\\n    [text_data[curr], text_data[prev]], coherence_threshold=0.25\\n)\\nprint([k[0] for k in cohesion])\";\n",
       "                var nbb_formatted_code = \"cohesion = coherence.get_coherence(\\n    [text_data[curr], text_data[prev]], coherence_threshold=0.25\\n)\\nprint([k[0] for k in cohesion])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cohesion = coherence.get_coherence(\n",
    "    [text_data[curr], text_data[prev]], coherence_threshold=0.25\n",
    ")\n",
    "print([k[0] for k in cohesion])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "357c0021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 206;\n",
       "                var nbb_unformatted_code = \"# get the keywords for the current sentences\\nkeywords_current = keywords_lib.get_keywords_with_kb_embeddings(text_data[curr])\\nkeywords_prev = keywords_lib.get_keywords_with_kb_embeddings(text_data[prev])\\n\\n# compute the word comparisons between the previous (with the coherence map)\\n# and the current (possibly the first sentence in a new segment)\\nword_comparisons_with_coherence, weights = compare_coherent_words(\\n    [keywords_prev], keywords_current\\n)\";\n",
       "                var nbb_formatted_code = \"# get the keywords for the current sentences\\nkeywords_current = keywords_lib.get_keywords_with_kb_embeddings(text_data[curr])\\nkeywords_prev = keywords_lib.get_keywords_with_kb_embeddings(text_data[prev])\\n\\n# compute the word comparisons between the previous (with the coherence map)\\n# and the current (possibly the first sentence in a new segment)\\nword_comparisons_with_coherence, weights = compare_coherent_words(\\n    [keywords_prev], keywords_current\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get the keywords for the current sentences\n",
    "keywords_current = keywords_lib.get_keywords_with_kb_embeddings(text_data[curr])\n",
    "keywords_prev = keywords_lib.get_keywords_with_kb_embeddings(text_data[prev])\n",
    "\n",
    "# compute the word comparisons between the previous (with the coherence map)\n",
    "# and the current (possibly the first sentence in a new segment)\n",
    "word_comparisons_with_coherence, weights = compare_coherent_words(\n",
    "    [keywords_prev], keywords_current\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "dd52c9d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('township', 0.2304),\n",
       "  ('communes', 0.1857),\n",
       "  ('hải', 0.1399),\n",
       "  ('wards', 0.1397),\n",
       "  ('đông', 0.1224)],\n",
       " [('cantonese', 0.5038),\n",
       "  ('mandarin', 0.464),\n",
       "  ('languages', 0.3483),\n",
       "  ('language', 0.343),\n",
       "  ('vietnamese', 0.3184)])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 207;\n",
       "                var nbb_unformatted_code = \"[(x[0], x[1]) for x in keywords_current], [(x[0], x[1]) for x in keywords_prev]\";\n",
       "                var nbb_formatted_code = \"[(x[0], x[1]) for x in keywords_current], [(x[0], x[1]) for x in keywords_prev]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "[(x[0], x[1]) for x in keywords_current], [(x[0], x[1]) for x in keywords_prev]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5121f953",
   "metadata": {},
   "source": [
    "# KeyBERT Embedding Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "id": "559ab602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 679;\n",
       "                var nbb_unformatted_code = \"docs = [\\n        \\\"Hi my name is Devarsh\\\",\\n        \\\"Devarsh likes to play Basketball.\\\",\\n    \\\"I love to watch Cricket.\\\",\\n        \\\"I am a strong programmer. And my name is Devarsh\\\",\\n]\";\n",
       "                var nbb_formatted_code = \"docs = [\\n    \\\"Hi my name is Devarsh\\\",\\n    \\\"Devarsh likes to play Basketball.\\\",\\n    \\\"I love to watch Cricket.\\\",\\n    \\\"I am a strong programmer. And my name is Devarsh\\\",\\n]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "docs = [\n",
    "    \"Hi my name is Devarsh\",\n",
    "    \"Devarsh likes to play Basketball.\",\n",
    "    \"I love to watch Cricket.\",\n",
    "    \"I am a strong programmer. And my name is Devarsh\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "id": "00458200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 680;\n",
       "                var nbb_unformatted_code = \"from keybert import KeyBERT\\n\\nkw_model = KeyBERT()\\ndoc_embeddings, word_embeddings = kw_model.extract_embeddings(\\n    docs, min_df=1, stop_words=\\\"english\\\"\\n)\\nkeywords = kw_model.extract_keywords(\\n    docs,\\n    min_df=1,\\n    stop_words=\\\"english\\\",\\n    doc_embeddings=doc_embeddings,\\n    word_embeddings=word_embeddings,\\n)\";\n",
       "                var nbb_formatted_code = \"from keybert import KeyBERT\\n\\nkw_model = KeyBERT()\\ndoc_embeddings, word_embeddings = kw_model.extract_embeddings(\\n    docs, min_df=1, stop_words=\\\"english\\\"\\n)\\nkeywords = kw_model.extract_keywords(\\n    docs,\\n    min_df=1,\\n    stop_words=\\\"english\\\",\\n    doc_embeddings=doc_embeddings,\\n    word_embeddings=word_embeddings,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keybert import KeyBERT\n",
    "\n",
    "kw_model = KeyBERT()\n",
    "doc_embeddings, word_embeddings = kw_model.extract_embeddings(\n",
    "    docs, min_df=1, stop_words=\"english\"\n",
    ")\n",
    "keywords = kw_model.extract_keywords(\n",
    "    docs,\n",
    "    min_df=1,\n",
    "    stop_words=\"english\",\n",
    "    doc_embeddings=doc_embeddings,\n",
    "    word_embeddings=word_embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "id": "7d30bae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 681,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 681;\n",
       "                var nbb_unformatted_code = \"len(doc_embeddings)\";\n",
       "                var nbb_formatted_code = \"len(doc_embeddings)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(doc_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "id": "018ee52e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 682,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 682;\n",
       "                var nbb_unformatted_code = \"len(word_embeddings)\";\n",
       "                var nbb_formatted_code = \"len(word_embeddings)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "id": "80cbdc77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('devarsh', 0.6267), ('hi', 0.5216)],\n",
       " [('devarsh', 0.6549),\n",
       "  ('basketball', 0.5558),\n",
       "  ('play', 0.3787),\n",
       "  ('likes', 0.2284)],\n",
       " [('cricket', 0.7118), ('watch', 0.3656), ('love', 0.307)],\n",
       " [('programmer', 0.5942), ('devarsh', 0.5528), ('strong', 0.3452)]]"
      ]
     },
     "execution_count": 683,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 683;\n",
       "                var nbb_unformatted_code = \"keywords\";\n",
       "                var nbb_formatted_code = \"keywords\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "id": "fd1ac50f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 701;\n",
       "                var nbb_unformatted_code = \"kw_model = KeyBERT()\\nimport torch\\n\\n\\ndef get_keywords_with_embeddings_test(\\n    data,\\n) -> list[tuple[str, float, torch.Tensor]]:\\n    doc_embeddings, word_embeddings = kw_model.extract_embeddings(data)\\n\\n    keywords = kw_model.extract_keywords(\\n        data, doc_embeddings=doc_embeddings, word_embeddings=word_embeddings\\n    )\\n\\n    keywords_with_embeddings = []\\n    count = 0\\n    print(len(word_embeddings))\\n    for i, (kw, we) in enumerate(zip(keywords, word_embeddings)):\\n        for j, words in enumerate(kw):\\n            keywords_with_embeddings.append((words[0], words[1], torch.tensor(we)))\\n            count += 1\\n\\n    return keywords_with_embeddings\";\n",
       "                var nbb_formatted_code = \"kw_model = KeyBERT()\\nimport torch\\n\\n\\ndef get_keywords_with_embeddings_test(\\n    data,\\n) -> list[tuple[str, float, torch.Tensor]]:\\n    doc_embeddings, word_embeddings = kw_model.extract_embeddings(data)\\n\\n    keywords = kw_model.extract_keywords(\\n        data, doc_embeddings=doc_embeddings, word_embeddings=word_embeddings\\n    )\\n\\n    keywords_with_embeddings = []\\n    count = 0\\n    print(len(word_embeddings))\\n    for i, (kw, we) in enumerate(zip(keywords, word_embeddings)):\\n        for j, words in enumerate(kw):\\n            keywords_with_embeddings.append((words[0], words[1], torch.tensor(we)))\\n            count += 1\\n\\n    return keywords_with_embeddings\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kw_model = KeyBERT()\n",
    "import torch\n",
    "\n",
    "\n",
    "def get_keywords_with_embeddings_test(\n",
    "    data,\n",
    ") -> list[tuple[str, float, torch.Tensor]]:\n",
    "    doc_embeddings, word_embeddings = kw_model.extract_embeddings(data)\n",
    "\n",
    "    keywords = kw_model.extract_keywords(\n",
    "        data, doc_embeddings=doc_embeddings, word_embeddings=word_embeddings\n",
    "    )\n",
    "\n",
    "    keywords_with_embeddings = []\n",
    "    count = 0\n",
    "    print(len(word_embeddings))\n",
    "    for i, (kw, we) in enumerate(zip(keywords, word_embeddings)):\n",
    "        for j, words in enumerate(kw):\n",
    "            keywords_with_embeddings.append((words[0], words[1], torch.tensor(we)))\n",
    "            count += 1\n",
    "\n",
    "    return keywords_with_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "id": "d1bbf3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 702;\n",
       "                var nbb_unformatted_code = \"embeddings = get_keywords_with_embeddings_test(docs)\";\n",
       "                var nbb_formatted_code = \"embeddings = get_keywords_with_embeddings_test(docs)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings = get_keywords_with_embeddings_test(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "id": "f1ea7b6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 703,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 703;\n",
       "                var nbb_unformatted_code = \"len(embeddings)\";\n",
       "                var nbb_formatted_code = \"len(embeddings)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9883ef15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
